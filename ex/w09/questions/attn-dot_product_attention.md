You are given a set of vectors
$$
\fh_1 = (1,2,3)^\top,\quad
\fh_2 = (1,2,1)^\top,\quad
\fh_3 = (0,1,-1)^\top
$$ 
and an alignment source vector $\fs=(1,2,1)^\top$. Compute the resulting dot-product attention weights $\alpha_i$ for $i=1,2,3$ and the resulting context vector $\fc$.