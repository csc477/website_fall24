{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSC413 Lab 9: GradCAM and Input Gradients\n",
    "\n",
    "We have seen that convolutional neural networks (CNN) are successful in \n",
    "many computer vision tasks, including classification, object detection\n",
    "and others. However, it is not immediately clear how CNNs work, and how\n",
    "one can explain the predictions made by CNNs.\n",
    "A deeper understanding of how CNNs work can also help us identify reasons\n",
    "why CNNs may fail to produce correct predictions for some samples.\n",
    "\n",
    "A line of work started to visualize and interpret computed features of\n",
    "convolutional neural networks. CAM and Grad-CAM are two influential and\n",
    "fundamental works to find which parts of the input have the most impact\n",
    "on the final output of the models by analyzing the model's extracted\n",
    "feature maps.\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "1. Explain how visualizing the regions of an image that contribute to the\n",
    "   CNN model's prediction can help explain how the model works.\n",
    "2. Implement CAM on a convolutional neural network with GAP + a single fc layer.\n",
    "3. Explain the limitations of CAM, and how Grad-CAM overcomes this limitation.\n",
    "4. Implement Grad-CAM on a convolutional neural network.\n",
    "\n",
    "Acknowledgements:\n",
    "2. We have borrowed some codes from [CAM Official Repo](https://github.com/zhoubolei/CAM).\n",
    "3. We have borrowd texts, figures and formulas from main papers of [CAM](https://arxiv.org/pdf/1512.04150v1.pdf) and [Grad-CAM](https://arxiv.org/pdf/1610.02391.pdf).\n",
    "\n",
    "## Submission\n",
    "\n",
    "If you are working with a partner, start by creating a group on Markus.\n",
    "If you are working alone,\n",
    "click \"Working Alone\".\n",
    "\n",
    "Submit the ipynb file `lab09.ipynb` on Markus \n",
    "**containing all your solutions to the Graded Task**s.\n",
    "Your notebook file must contain your code **and outputs** where applicable,\n",
    "including printed lines and images.\n",
    "Your TA will not run your code for the purpose of grading.\n",
    "\n",
    "For this lab, you should submit the following:\n",
    "\n",
    "- Part 1. Your implementation of the `predict` function (1 point)\n",
    "- Part 1. Your implementation of the `get_resnet_features` function (1 point)\n",
    "- Part 1. Your implementation of the `compute_cam` function (3 point)\n",
    "- Part 1. Your interpretation of the grad cam outputs (2 point)\n",
    "- Part 2. Your implementation of the `compute_gradcam` function (3 point)\n",
    "\n",
    "\n",
    "## Part 1. Class Activation Maps (CAM)\n",
    "\n",
    "CAM and its extension Grad-CAM takes the approach of identifying the regions\n",
    "of the an image that contributes most to the model's prediction.\n",
    "This information can be visualized as a heat map, and provides a way to interpret\n",
    "a model's prediction: did the model predict that the image is that of a \"boat\"\n",
    "because of the shape of the ears, or because of the water in the background?\n",
    "\n",
    "We discussed, during lecture, that convolutional layers preserve the geometry\n",
    "of the image, and that these convolutional layers actually behave as feature/\n",
    "object detectors of various complexity.\n",
    "Since the geometry of the output of a CNN layer corresponds to the geometry of\n",
    "input image, it is straightforward to locate the region of the image that corresponds\n",
    "to a particularly high activation value.\n",
    "This is because the computations that we use in a CNN (convolutions, max pooling,\n",
    "activations) are all geometry preserving (equivariant).\n",
    "\n",
    "However, fully-connected layers are typically used for classification in the final\n",
    "layers of a CNN.  These fully-connected layers are **not** geometry preserving,\n",
    "thus information about the locations of discriminating features are lost \n",
    "when fully-connected layers are used for classification.\n",
    "\n",
    "The idea behind CAM is to avoid using these fully-connected layers for\n",
    "classification, so that we can reconstruct location information in a straightforward\n",
    "way. Instead of fully-connected layers, we use:\n",
    "\n",
    "1. A global average pooling (GAP) layer. This layer will take as input the output of a \n",
    "   CNN layer (e.g., of shape `H x W x C`) and perform an average operation for each\n",
    "   *channel* along the entire activation height/width (producing an output vector\n",
    "   of shape `C`).\n",
    "2. A single linear layer to map this vector (of length `C`) into the output space.\n",
    "\n",
    "Since both the pooling and linear layers have straightforward computation, it is possible\n",
    "to assign credit for a output score for a class back to specific activation values\n",
    "of the CNN output.\n",
    "\n",
    "The framework of the Class Activation Mapping is as below (from https://github.com/zhoubolei/CAM):\n",
    "\n",
    "<img src=\"https://camo.githubusercontent.com/ef86405fc14b4af391c891c93a50ce92c8bfc3f8fae6e7c04f5f7185b21c3eca/687474703a2f2f636e6e6c6f63616c697a6174696f6e2e637361696c2e6d69742e6564752f6672616d65776f726b2e6a7067\" width=\"500px\"/>\n",
    "\n",
    "In this part of the lab, we will implement CAM to produce a heatmap of the\n",
    "contribution to locations in the image to a predicted class.\n",
    "We will use the pre-trained convolutional neural network **ResNet**,\n",
    "chosen because this model's architecture uses global average pooling (GAP).\n",
    "ResNet is trained on the ImageNet data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models, transforms\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Download resnet pretrained weights\n",
    "resnet = models.resnet18(pretrained=True)\n",
    "\n",
    "# CAM can only be used when the models are in \"evaluation phase\".\n",
    "resnet.eval()\n",
    "\n",
    "# Print model architecture\n",
    "# We will use the CNN activations computed after layer 4, and before GAP.\n",
    "resnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ImageNet labels are a bit challenging to read. We will download a list of human-readable\n",
    "labels from here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the imagenet category list\n",
    "with open('imagenet-simple-labels.json') as f:\n",
    "    classes = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remind ourselves of how ResNet works, let's predict what class these images belongs to:\n",
    "\n",
    "<img src=\"https://www.cs.toronto.edu/~lczhang/413/cat.jpg\" width=\"300px\" />\n",
    "<img src=\"https://www.cs.toronto.edu/~lczhang/413/boat.jpg\" width=\"300px\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://www.cs.toronto.edu/~lczhang/413/cat.jpg\n",
    "!wget https://www.cs.toronto.edu/~lczhang/413/boat.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def process_input(image_file):\n",
    "    # open the image\n",
    "    img = Image.open(image_file)\n",
    "\n",
    "    # transform the images by resizing and normalizing\n",
    "    preprocess = transforms.Compose([\n",
    "       transforms.Resize((224,224)),\n",
    "       transforms.ToTensor(),\n",
    "       transforms.Normalize(\n",
    "          mean=[0.485, 0.456, 0.406],\n",
    "          std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "    return preprocess(img).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Graded Task**: Write a function that takes a model and an image file and produces a list of the\n",
    "top 5 predictions with the corresponding probability score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, image_file):\n",
    "    \"\"\"\n",
    "    Return the top 5 class predictions with the corresponding probability score.\n",
    "\n",
    "    Parameters:\n",
    "        `model`      - nn.Module\n",
    "        `image_file` - file path to the image\n",
    "\n",
    "    Returns: a list of 5 (string, int, float) pairs: \n",
    "        the string is the predicted ResNet class name (see classes above),\n",
    "        the int is the predicted ResNet class id,\n",
    "        and the float is the prediction probability. The list should be ordered\n",
    "        so that the highest probabilty score appears first.\n",
    "\n",
    "    Example:\n",
    "        >>> predict(resnet, 'cat.jpg')\n",
    "        [('prison', 743, 0.23517875373363495),\n",
    "         ('shopping cart', 791, 0.07393667101860046),\n",
    "         ('rocking chair', 765, 0.06884343922138214),\n",
    "         ('wheelbarrow', 428, 0.06603048741817474),\n",
    "         ('ring-tailed lemur', 383, 0.0434008426964283)]\n",
    "    \"\"\"\n",
    "    x = process_input(image_file)\n",
    "\n",
    "    result = None # TODO\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please include the output of the below cell in your submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(resnet, 'cat.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can use ResNet to make predictions, we need two additional pieces\n",
    "of information for CAM.\n",
    "\n",
    "First, given an image, we need to be able to compute the\n",
    "**features/activations of the last convolutional layer**. This feature map\n",
    "is the input to the GAP layer. Although this information is computed in a \n",
    "forward pass, we will need to write some code to extract this information.\n",
    "\n",
    "Second, we will need the weights of the final fully-connected layer in ResNet.\n",
    "\n",
    "**Graded Task**: Complete the following function that takes an image file and produces the\n",
    "weights of the finally fully-connected layer in Resnet.\n",
    "You may find the `named_children()` method of resnet helpful, as it produces a\n",
    "sequence of (named) layers. We would like the feature map directly before the\n",
    "global average pooling layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (name, model) in resnet.named_children():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resnet_features(image_file):\n",
    "    \"\"\"\n",
    "    Return the final CNN layer (layer4) feature map in resnet\n",
    "\n",
    "    Parameters:\n",
    "        `image_file` - file path to the image\n",
    "\n",
    "    Returns: PyTorch tensor of shape [1, 512, 7, 7]\n",
    "    \"\"\"\n",
    "\n",
    "    x = process_input(image_file)\n",
    "\n",
    "    result = None # TODO\n",
    "    for (name, model) in resnet.named_children():\n",
    "        # TODO -- update result\n",
    "        if name == 'layer4':\n",
    "            break\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fets = get_resnet_features('cat.jpg')\n",
    "print(fets.shape) # should be [1, 512, 7, 7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Assign the variable `fc_weight` to the\n",
    "weights of the final fully-connected layer in resnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = None # TODO\n",
    "print(weights.shape) # should be [1000, 512]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Graded Task**: Complete the function `compute_cam`, which takes the \n",
    "CNN feature map (from the `get_resnet_features` function), the ImageNet label of \n",
    "interest, and produces a heat map of the features that contribute to the\n",
    "label score according to the following approach. \n",
    "\n",
    "We will use the notation ${\\bf X}$ to denote the CNN feature map (the input to the GAP),\n",
    "with $X_{i,j,c}$ being the activation at location $(i, j)$ and channel $c$.\n",
    "Here, ${\\bf X}$ is a tensor with shape $H \\times W \\times C$, where $H \\times W$ is\n",
    "the height and width of the feature map and $C$ is the number of channels. \n",
    "We will use the vector ${\\bf h}$ to denote the output of the GAP,\n",
    "so that $h_c = \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} X_{i,j,c}$.\n",
    "Finally, we will use ${\\bf W}$ to denote the finally fully connected layer weights,\n",
    "and ${\\bf z}$ to the denote the prediction score,\n",
    "so that ${\\bf z} = {\\bf W}{\\bf h}$.\n",
    "\n",
    "Now, we would like to relate the features $X_{i,j,c}$ to the scores $z_k$, so that\n",
    "we can compute the contribution of the features at position $(i,j)$ to the score\n",
    "$k$.\n",
    "\n",
    "For an output class $k$, we have:\n",
    "\n",
    "$$z_k  = \\sum_{c=1}^{C} w_{k,c} h_c$$\n",
    "\n",
    "Substituing $h_c$ for its definition, we have:\n",
    "\n",
    "$$z_k  = \\sum_{c=1}^{C} w_{k,c} \\frac{1}{HW} \\sum_{i=1}^H \\sum_{j=1}^W X_{i,j,c}$$\n",
    "\n",
    "Rearranging the sums, we have:\n",
    "\n",
    "$$z_k  = \\frac{1}{HW} \\sum_{i=1}^H \\sum_{j=1}^W \\sum_{c=1}^C w_{k,c} X_{i,j,c}$$\n",
    "\n",
    "The inner term $\\sum_c w_{k,c} X_{i,j,c}$ is exactly what we are looking for:\n",
    "this term indicates how much the value in location $(i, j)$ of \n",
    "the feature map ${\\bf X}$ attributes to class $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cam(features, label):\n",
    "    \"\"\"\n",
    "    Computes the contribution of each location in `features` towards \n",
    "    `label` using CAM.\n",
    "\n",
    "    Parameters:\n",
    "        `features`: PyTorch Tensor of shape [1, 512, 7, 7] representing\n",
    "                    final layer feature map in ResNet (e.g., from calling\n",
    "                    `get_resnet_features`)\n",
    "        `label`   : resnet label, integer between 0-999\n",
    "\n",
    "    Returns: PyTorch Tensor of shape [7, 7]\n",
    "    \"\"\"\n",
    "    features = features.squeeze(0) # remove the first dimension\n",
    "    result = None # TODO \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Run the below code, which superimposes the result of the `compute_cam`\n",
    "operation on the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_cam(image_file, label):\n",
    "    # open the image\n",
    "    img = Image.open(image_file)\n",
    "\n",
    "    # compute CAM features\n",
    "    fets = get_resnet_features('cat.jpg')\n",
    "    m = compute_cam(fets, label)\n",
    "\n",
    "    # normalize \"m\"\n",
    "    m = m - m.min()\n",
    "    m = m / m.max()\n",
    "    # convert \"m\" into pixel intensities\n",
    "    m = np.uint8(255 * m.detach().numpy())\n",
    "    # apply a color map\n",
    "    m = cv2.resize(m, img.size)\n",
    "    heatmap = cv2.applyColorMap(m, cv2.COLORMAP_JET)\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.title(\"%s %s\" % (image_file, classes[label]))\n",
    "    plt.imshow((0.3 * heatmap + 0.5 * np.array(img)).astype(np.uint8)) # superimpose heat map on img\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_cam('cat.jpg', 743)\n",
    "visualize_cam('cat.jpg', 383)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Graded Task** Compare the above two outputs, and explain what conclusion you \n",
    "may be able to draw about the contribution of the pixel locations to those\n",
    "two classes. Why do you think the model misclassified the image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: your explanation goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Grad-CAM\n",
    "\n",
    "Although CAM was an important step toward understanding convolutional neural networks,\n",
    "the technique is only applicable to convolutional networks with GAP and \n",
    "a single fully-connected layer.  Recall that it leveraged the following relationship\n",
    "between the output of the GAP layer ${\\bf h}$ and the score for the output class\n",
    "$z_k$:\n",
    "\n",
    "$$z_k  = \\sum_c w_{k,c} h_c$$\n",
    "\n",
    "Where $w_{k,c}$ is the fully connected layer weight that describes the strength of the\n",
    "connection betwee $h_c$ and $z_k$. In other words, $w_{k,c}$ describes the following\n",
    "gradient:\n",
    "\n",
    "$$\\frac{\\partial z_k}{\\partial h_c}$$\n",
    "\n",
    "With this in mind, you may be able to see how CAM may be generalized so that\n",
    "${\\bf z}$ may be a more complex function of ${\\bf h}$---e.g., a MLP or even an RNN!\n",
    "\n",
    "Gradient-weighted Class Activation Mapping (Grad-CAM) is a generalized form of CAM,\n",
    "and can be applied to any convolutional neural network.\n",
    "In Grad-CAM, we use the gradient $\\frac{\\partial z_k}{\\partial h_c}$\n",
    "in place of $w_{k,c}$ when attributing class scores to locations $(i, j)$.\n",
    "In other words, the below term indicates how much the value in location $(i, j)$ of \n",
    "the feature map ${\\bf X}$ attributes to class $k$.\n",
    "\n",
    "$$ReLU(\\sum_c \\frac{\\partial z_k}{\\partial h_c} X_{i,j,c})$$\n",
    "\n",
    "The addition of the ReLU activation only allows positive contributions to be visualized.\n",
    "\n",
    "Sidenote: To generalize this result even further, we can replace $z_k$ with any target we would like!\n",
    "Grad-CAM has been used on neural networks that performs image caption\n",
    "generation: a model with a CNN *encoder* and an RNN *decoder*. \n",
    "We can use use the gradients of any\n",
    "target concept (say \"dog\" in a classification network or a sequence of words\n",
    "in a captioning network) flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. Taking a look at [this video](https://www.youtube.com/watch?v=COjUB9Izk6E) helps you to understand the power of Grad-CAM.\n",
    "\n",
    "Let's explore GradCAM with the VGG network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg19 = models.vgg19(pretrained=True)\n",
    "vgg19.eval()\n",
    "vgg19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(vgg19, 'cat.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task** Just like with CAM, we will need to extract the feature map obtained from the last \n",
    "convolutional layer. This step is actually very straightforward with VGG since `vgg19` splits the\n",
    "network into a `features` network and a `classifier` network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vgg_features(image_file):\n",
    "    \"\"\"\n",
    "    Return the output of `vgg19.features` network for the image\n",
    "\n",
    "    Parameters:\n",
    "        `image_file` - file path to the image\n",
    "\n",
    "    Returns: PyTorch tensor of shape [1, 512, 7, 7]\n",
    "    \"\"\"\n",
    "\n",
    "    x = process_input(image_file)\n",
    "    result = None # TODO\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_vgg_features('cat.jpg').shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Read the forward method of the VGG model here. https://github.com/pytorch/vision/blob/main/torchvision/models/vgg.py\n",
    "What other steps are remaining in the forward pass?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Explain the remaining steps here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Graded Task**: Complete the function `compute_gradcam`, which takes an\n",
    "image file path, the ImageNet label of \n",
    "interest, and produces a heat map of the features that contribute to the\n",
    "label score according to the GradCAM approach described at the beginning of\n",
    "Part 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradcam(image_file, label):\n",
    "    \"\"\"\n",
    "    Computes the contribution of each location in `features` towards \n",
    "    `label` using GradCAM.\n",
    "\n",
    "    Parameters:\n",
    "        `image_file` - file path to the image\n",
    "        `label`   : resnet label, integer between 0-999\n",
    "\n",
    "    Returns: PyTorch Tensor of shape [7, 7]\n",
    "    \"\"\"\n",
    "    # obtain the image input features\n",
    "    x = process_input(image_file)\n",
    "\n",
    "    # obtain the output of the features network in the CNN\n",
    "    fets = vgg19.features(x)\n",
    "\n",
    "    # tell PyTorch to compute the gradients with respect\n",
    "    # to \"fets\"\n",
    "    fets.retain_grad()\n",
    "\n",
    "    # TODO: compute the rest of the vgg19 forward pass from `fets`\n",
    "    out = None # should be the output of the classifier network\n",
    "\n",
    "    z_k = out.squeeze(0)[label] # identify the target output class\n",
    "    z_k.backward()              # backpropagation to compute gradients\n",
    "\n",
    "    features_grad = fets.grad   # identify the gradient of z_k with respect to fets\n",
    "\n",
    "    # account for the pooling operation, so that \"pooled_grad\"\n",
    "    # aligns with the notation used\n",
    "    n, c, h, w = features_grad.shape\n",
    "    features_grad = torch.reshape(features_grad, (c, h*w))\n",
    "    pooled_grad = features_grad.sum(dim=1)\n",
    "\n",
    "    # rearrange \"fets\" so that \"X\" aligns with the notation\n",
    "    # used above\n",
    "    X = fets.squeeze(0).permute((1, 2, 0))\n",
    "\n",
    "    # TODO: Compute the heatmap using the gradcam\n",
    "    m = None\n",
    "    m = F.relu(m) # apply the ReLU operation\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Run the below code, which superimposes the result of the `compute_gradcam`\n",
    "operation on the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_gradcam(image_file, label):\n",
    "    # open the image\n",
    "    img = Image.open(image_file)\n",
    "\n",
    "    # compute CAM features\n",
    "    m = compute_gradcam(image_file, label)\n",
    "\n",
    "    # normalize \"m\"\n",
    "    m = m - m.min()\n",
    "    m = m / m.max()\n",
    "    # convert \"m\" into pixel intensities\n",
    "    m = np.uint8(255 * m.detach().numpy())\n",
    "    # apply a color map\n",
    "    m = cv2.resize(m, img.size)\n",
    "    heatmap = cv2.applyColorMap(m, cv2.COLORMAP_JET)\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.title(\"%s %s\" % (image_file, classes[label]))\n",
    "    plt.imshow((0.3 * heatmap + 0.5 * np.array(img)).astype(np.uint8)) # superimpose heat map on img\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_cam('cat.jpg', 743)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_cam('cat.jpg', 383)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_cam('boat.jpg', 536)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just For Fun\n",
    "\n",
    "As you might have seen in the [video](https://www.youtube.com/watch?v=COjUB9Izk6E),\n",
    "Grad-CAM can be applied to text-generating models. For example, in image-captioning\n",
    "tasks, a text is generated describing the given image. Some methods first feed the\n",
    "image to a convolutional neural network to extract features, and then feed the extracted\n",
    "features to an RNN, to generate the text. Neuraltalk2 was one of the earliest models\n",
    "using this approach. Similar to the classification task, it is enough to compute the\n",
    "gradient of the score (what is the score in an image-captioning task?) with respect\n",
    "to the last convolutional layer.\n",
    "\n",
    "If you are interested in how neuraltalk2 functions you can check [this project](https://cs.stanford.edu/people/karpathy/deepimagesent/). Moreover, if you are looking for more hands-on experience, [this repo](https://github.com/ruotianluo/ImageCaptioning.pytorch) has implemented many image-captioning methods, and you can easily apply Grad-CAM on them (especially show and tell).\n",
    "\n",
    "Hint: There is a file which re-implements the forward pipeline of ResNet101, where you can store the features."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
