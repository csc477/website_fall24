{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92adc930",
   "metadata": {},
   "source": [
    "# CSC413 Lab 8: Text Classification using RNNs\n",
    "\n",
    "**Sentiment Analysis** is the problem of identifying the writer's sentiment given a piece of text.\n",
    "Sentiment Analysis can be applied to movie reviews, feedback of other forms, emails, tweets, \n",
    "course evaluations, and much more.\n",
    "\n",
    "In this lab, we will build an RNN to classify positive vs negative tweets\n",
    "We use the Sentiment140 data set, which contains tweets with either a positive\n",
    "or negative emoticon. Our goal is to determine whether which type of\n",
    "emoticon the tweet (with the emoticon removed) contained. The dataset was actually collected by\n",
    "a group of students, much like you, who are doing their first machine learning projects.\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "- Use PyTorch to train an RNN model\n",
    "- Apply and analyze the components of an RNN model\n",
    "- Explain how batching is done on sequence data, where the training data in a batch may have different lengths\n",
    "- Use pre-trained word embeddings as part of a transfer learning strategy for text classification\n",
    "- Understand the bias that exists in word embeddings and language models.\n",
    "\n",
    "Acknowledgements:\n",
    "\n",
    "- Data is sampled from http://help.sentiment140.com/for-students\n",
    "\n",
    "\n",
    "Please work in groups of 1-2 during the lab.\n",
    "\n",
    "## Submission\n",
    "\n",
    "If you are working with a partner, start by creating a group on Markus. If you are working alone,\n",
    "click \"Working Alone\".\n",
    "\n",
    "Submit the ipynb file `lab10.ipynb` on Markus \n",
    "**containing all your solutions to the Graded Task**s.\n",
    "Your notebook file must contain your code **and outputs** where applicable,\n",
    "including printed lines and images.\n",
    "Your TA will not run your code for the purpose of grading.\n",
    "\n",
    "For this lab, you should submit the following:\n",
    "\n",
    "- Part 1. Your output showing several positive tweets. (1 point)\n",
    "- Part 2. Your explanation of the shapes of `wordemb`. (1 point)\n",
    "- Part 2. Your explanation of the shapes of `h` and `out`. (2 points)\n",
    "- Part 2. Your explanation of why computing the mean and max of hidden states across *all* time steps is likely more informative than using the final output state. (1 point)\n",
    "- Part 3. Your demonstration of the model's ability to \"overfit\" on a data set. (1 point)\n",
    "- Part 3. Your output from training the model on the full data set. (1 point)\n",
    "- Part 4. Your explanation of why `MyGloveRNN` requires fewer iteration to obtain \"good\" accuracy. (1 point)\n",
    "- Part 4. Your comparison of `MyGloveRNN` and `MyRNN` in low data settings.. (1 point)\n",
    "- Part 4. Your explanation of where the biases in embeddings come from, and whether our model will have the same sorts of baises.. (1 point)\n",
    "\n",
    "\n",
    "## Part 1. Data\n",
    "\n",
    "Start by running these two lines of code to download the data on to Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ee73c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download tutorial data files.\n",
    "!wget https://www.cs.toronto.edu/~lczhang/413/sample_tweets.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bbf7d0",
   "metadata": {},
   "source": [
    "As always, we start by understanding what our data looks like. Notice that the\n",
    "test set has been set aside for us. Both the training and test set files follow\n",
    "the same format. Each line in the csv file contains the tweet text,\n",
    "the string label \"4\" (positive) or \"0\" (negative), and some additional information about the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f26b361",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "datafile = \"sample_tweets.csv\"\n",
    "\n",
    "# Training/Validation set\n",
    "data = csv.reader(open(datafile))\n",
    "for i, line in enumerate(data):\n",
    "    print(line)\n",
    "    if i > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0e88cf",
   "metadata": {},
   "source": [
    "**Task**: How many positive and negative tweets are in this file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321e515f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "print(Counter(x[0] for x in csv.reader(open(datafile))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cba0a6",
   "metadata": {},
   "source": [
    "**Graded Task**: We have printed several negative tweets above. \n",
    "Print 10 positive tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe6e4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Please make sure to include both your code and the\n",
    "# printed output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11bb18d",
   "metadata": {},
   "source": [
    "We will now split the dataset into training, validation, and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284b58bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data; convert labels into integers\n",
    "data = [(review, int(label=='4'))  # label 1 = positive, 0 = negative\n",
    "        for label, _, _, _, _, review in csv.reader(open(datafile))]\n",
    "\n",
    "# shuffle the data, since the file stores all negative tweets first\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(data)\n",
    "\n",
    "train_data = data[:50000] \n",
    "val_data = data[50000:60000] \n",
    "test_data = data[60000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c50ecd3",
   "metadata": {},
   "source": [
    "In order to be able to use neural networks to make predictions about these tweets,\n",
    "we need to begin by convert these tweets into sequences of numbers, each representing\n",
    "a words. This is akin to a one-hot encoding: each word will be converted into an\n",
    "a number representing the unique *index* of that word.\n",
    "\n",
    "Although we could do this conversion by writing our own python code,\n",
    "torch has a package called **torchtext** that has utilities useful for text classification\n",
    "and generation tasks. \n",
    "In particular, the `Vocab` class and `build_vocab_from_iterator` will be useful for us\n",
    "for building the mapping from words to indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc4a246",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import Vocab, build_vocab_from_iterator\n",
    "\n",
    "# we will *tokenize* each word by using a tokenzier from \n",
    "# https://pytorch.org/text/stable/data_utils.html#get-tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "train_data_words = [tokenizer(x) for x, t in train_data]\n",
    "\n",
    "# build the vocabulary object. the parameters to this function\n",
    "# is described below\n",
    "vocab = build_vocab_from_iterator(train_data_words,\n",
    "                                  specials=['<bos>', '<eos>', '<unk>', '<pad>'],\n",
    "                                  min_freq=10)\n",
    "\n",
    "# set the index of a word not in the vocabulary\n",
    "vocab.set_default_index(2) # this is the index of the `<unk>` keyword"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9e7049",
   "metadata": {},
   "source": [
    "Now, `vocab` is an object of class `Vocab` (see more here [https://pytorch.org/text/stable/vocab.html](https://pytorch.org/text/stable/vocab.html) )\n",
    "that provides functionalities for converting words into their indices.\n",
    "In addition to words appearing in the training set, ther are four special tokens that \n",
    "we use, akin to placeholder words:\n",
    "\n",
    "- `<bos>`, to indicate the beginning of a sequence.\n",
    "- `<eos>`, to indicate the end of a sequence.\n",
    "- `<unk>`, to indicate a word that is *not* in the vocabulary. This includes\n",
    "  words that appear too infrequently to be included in the vocabulary, and any\n",
    "  other words in the validation/test sets that are not see in training.\n",
    "- `<pad>`, used for padding shorter sequences in a batch: since each tweet\n",
    "  may have different length, the shorter tweets in each batch will be padded with\n",
    "  the `<pad>` token so that each sequence (tweet) in a batch has the same length.\n",
    "\n",
    "The `min_freq` parameter identifies the minimum number of times a word must appear in the\n",
    "training set in order to be included in the vocabulary.\n",
    "\n",
    "Here you can see the `vocab` object in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c1e5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the number of words in the vocabulary\n",
    "print(len(vocab))\n",
    "\n",
    "# Convert a tweet into a sequence of word indices.\n",
    "tweet = 'The movie Pneumonoultramicroscopicsilicovolcanoconiosis is a good movie, it is very funny'\n",
    "tokens = tokenizer(f'<bos> {tweet} <eos>')\n",
    "print(tokens)\n",
    "indices = vocab.forward(tokens)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b0f46a",
   "metadata": {},
   "source": [
    "**Task**: What is the index of the `<pad>` token?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36eb8d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: write code to identify the index of the `<pad>` token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cddbe3",
   "metadata": {},
   "source": [
    "Now let's apply this transformation to the entire set of training, validation, and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25f875f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_indices(data, vocab):\n",
    "    \"\"\"Convert data of form [(tweet, label)...] where tweet is a string\n",
    "    into an equivalent list, but where the tweets represented as a list\n",
    "    of word indices.\n",
    "    \"\"\"\n",
    "    return [(vocab.forward(tokenizer(f'<bos> {text} <eos>')), label)\n",
    "            for (text, label) in data]\n",
    "\n",
    "train_data_indices = convert_indices(train_data, vocab)\n",
    "val_data_indices = convert_indices(val_data, vocab)\n",
    "test_data_indices = convert_indices(test_data, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015d6106",
   "metadata": {},
   "source": [
    "We have seen that PyTorch's `DataLoader` provides an easy way to form minibatches \n",
    "when we worked with image data. However, text and sequence data is more challenging to\n",
    "work with since the sequences may not be the same length.\n",
    "\n",
    "Although we can (and will!) continue to use `DataLoader` for our text data, we need to\n",
    "provide a function that merges sequences of various lengths into two PyTorch tensors \n",
    "correspondingg to the inputs and targets for that batch.\n",
    "\n",
    "\n",
    "**Task**: Following the instructions below, complete the `collate_batch` function,\n",
    "which creates the input and target tensors\n",
    "for a batch of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17443eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_batch(batch):\n",
    "    \"\"\"\n",
    "    Returns the input and target tensors for a batch of data\n",
    "\n",
    "    Parameters:\n",
    "        `batch` - An iterable data structure of tuples (indices, label),\n",
    "                  where `indices` is a sequence of word indices, and \n",
    "                  `label` is either 1 or 0.\n",
    "\n",
    "    Returns: a tuple `(X, t)`, where \n",
    "        - `X` is a PyTorch tensor of shape (batch_size, sequence_length)\n",
    "        - `t` is a PyTorch tensor of shape (batch_size)\n",
    "    where `sequence_length` is the length of the longest sequence in the batch\n",
    "    \"\"\"\n",
    "\n",
    "    text_list = []  # collect each sample's sequence of word indices\n",
    "    label_list = [] # collect each sample's target labels\n",
    "    for (text_indices, label) in batch:\n",
    "        text_list.append(torch.tensor(text_indices))\n",
    "        # TODO: what do we need to do with `label`?\n",
    "\n",
    "    X = pad_sequence(text_list, padding_value=3).transpose(0, 1)\n",
    "    t = None # TODO\n",
    "    return X, t\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_data_indices, batch_size=10, shuffle=True,\n",
    "                              collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1882817e",
   "metadata": {},
   "source": [
    "With the above code in mind, we should be able to extract batches from `train_dataloader`.\n",
    "Notice that `X.shape` is different in each batch.\n",
    "You should also see that the index `3` is used to pad shorter sequences in in a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec441b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (X, t) in enumerate(train_dataloader):\n",
    "    print(X.shape, t.shape)\n",
    "    if i >= 10:\n",
    "        break\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61020bd",
   "metadata": {},
   "source": [
    "**Task**: Why does each sequence begin with the token `0`, and end with the token `1` (ignoring\n",
    "the paddings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c4cf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your explanation goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d19ff33",
   "metadata": {},
   "source": [
    "## Part 2. Model\n",
    "\n",
    "We will use a recurrent neural network model to classify positive vs negative\n",
    "sentiments. Our RNN model will have three components that are typical in a\n",
    "sequence classification model:\n",
    "\n",
    "- An *embedding layer*, which will map each word index (akin to a one-hot embedding)\n",
    "  into a low-dimensional vector. This layer as having the same functionality as the\n",
    "  weights $W^{(word)}$ from lab 2.\n",
    "- A *recurrent layer*, which performs the recurrent neural network computation.\n",
    "  The input to this layer is the low-dimensional embedding vectors\n",
    "  for each word in the sequence.\n",
    "- A *fully connected layer*, which computes the final binary classification using\n",
    "  features computed from the recurrent layer. In our case, we concatenate the\n",
    "  *max* and *mean* of the hidden units across the time steps (i.e. across each word).\n",
    "\n",
    "Let's define the model that we will use, and then explore it step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f28bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MyRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, hidden_size, num_classes):\n",
    "        super(MyRNN, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb_size = emb_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_classes = num_classes\n",
    "        self.emb = nn.Embedding(vocab_size, emb_size)\n",
    "        self.rnn = nn.RNN(emb_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Look up the embedding\n",
    "        wordemb = self.emb(X)\n",
    "        # Forward propagate the RNN\n",
    "        h, out = self.rnn(wordemb)\n",
    "        # combine the hidden features computed from *each* time step of\n",
    "        # the RNN. we do this by \n",
    "        features = torch.cat([torch.amax(h, dim=1),\n",
    "                              torch.mean(h, dim=1)], axis=-1)\n",
    "        # Compute the final prediction\n",
    "        z = self.fc(features)\n",
    "        return z\n",
    "\n",
    "model = MyRNN(len(vocab), 128, 64, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e3ee11",
   "metadata": {},
   "source": [
    "To explore exactly what this model is doing, let's grab one batch of data from\n",
    "the data loader we created. We will observe, step-by-step, what computation will be\n",
    "performed on the input `X` to obtain the final prediction. We do this by \n",
    "emulating the `forward` method of the `MyRNN` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcc1d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, t = next(iter(train_dataloader))\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8055b4",
   "metadata": {},
   "source": [
    "**Graded Task**: Run the code below to check the shape of `wordemb`.\n",
    "What shape does this tensor have?  Explain what each dimension in this shape means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d3c22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordemb = model.emb(X)\n",
    "\n",
    "print(wordemb.shape)\n",
    "\n",
    "# TODO: Include your explanation here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1baeb9",
   "metadata": {},
   "source": [
    "**Graded Task**: Run the code below, which computes the RNN forward pass,\n",
    "with `wordemb` as input.\n",
    "What shape do the tensors `h` and `out` have?  Explain what these tensors correspond to.\n",
    "(See the RNN reference [https://pytorch.org/docs/stable/generated/torch.nn.RNN.html](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html) on the PyTorch documentation page.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f17f7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "h, out = model.rnn(wordemb)\n",
    "\n",
    "print(h.shape)\n",
    "print(out.shape)\n",
    "\n",
    "# The tensors `h` and `out` are related. To see the relation,\n",
    "# choose an index in the batch and compare the following two\n",
    "# vectors in `h` and `out`.\n",
    "index = 2 # choose an index to iterate through the batch\n",
    "print(h[index, -1, :])\n",
    "print(out[0, index, :])\n",
    "\n",
    "# TODO: Include your explanation here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41671215",
   "metadata": {},
   "source": [
    "**Graded Task**: There is a step in the MyRNN forward pass\n",
    "that combines the features from *each* time step of the RNN by \n",
    "computing:\n",
    "\n",
    "1. the *maximum* value of each position in the hidden vector.\n",
    "2. the *mean* value of each position in the hidden vector.\n",
    "3. concatenating the resulting two vectors.\n",
    "\n",
    "(Note that in the demo below, we are working with a minibatch. Thus,\n",
    "each of `out1`, `out2`, and `features` below are *matrices* containing\n",
    "the vectors from each minibatch)\n",
    "\n",
    "This method typically performs better than, say, taking the hidden\n",
    "state at the last time step (the value `out` from above). Explain,\n",
    "intuitively, why you might expect this performance to be the case for a\n",
    "sentiment analysis task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d12985b",
   "metadata": {},
   "outputs": [],
   "source": [
    "out1 = torch.amax(h, dim=1)\n",
    "out2 = torch.mean(h, dim=1)\n",
    "features = torch.cat([out1, out2], axis=-1)\n",
    "\n",
    "# Compare, for a single input in the batch, the connection between\n",
    "# `h`, `out1`, `out2` and `features`:\n",
    "print(h[index, :, :])\n",
    "print(out1[index, :])\n",
    "print(out2[index, :])\n",
    "print(features[index, :])\n",
    "\n",
    "# TODO: Include your explanation here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a065eb1",
   "metadata": {},
   "source": [
    "**Task**: Finally, the model uses the `features` tensor to compute\n",
    "the prediction for each element in the batch. Run the code below to\n",
    "complete this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76aa317",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.fc(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddaff1b",
   "metadata": {},
   "source": [
    "There is one more thing we need to do before training the model, which is\n",
    "to write a function to estimate the accuracy of the model. This is done for \n",
    "you below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc11ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model, dataset, max=1000):\n",
    "    \"\"\"\n",
    "    Estimate the accuracy of `model` over the `dataset`.\n",
    "    We will take the **most probable class**\n",
    "    as the class predicted by the model.\n",
    "\n",
    "    Parameters:\n",
    "        `model`   - An object of class nn.Module\n",
    "        `dataset` - A dataset of the same type as `train_data`.\n",
    "        `max`     - The max number of samples to use to estimate \n",
    "                    model accuracy\n",
    "\n",
    "    Returns: a floating-point value between 0 and 1.\n",
    "    \"\"\"\n",
    "\n",
    "    correct, total = 0, 0\n",
    "    dataloader = DataLoader(dataset,\n",
    "                            batch_size=1,  # use batch size 1 to prevent padding\n",
    "                            collate_fn=collate_batch)\n",
    "    for i, (x, t) in enumerate(dataloader):\n",
    "        z = model(x)\n",
    "        y = torch.argmax(z, axis=1)\n",
    "        correct += int(torch.sum(t == y))\n",
    "        total   += 1\n",
    "        if i >= max:\n",
    "            break\n",
    "    return correct / total\n",
    "\n",
    "accuracy(model, train_data_indices) # should be close to half"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb10bd60",
   "metadata": {},
   "source": [
    "## Part 3. Training\n",
    "\n",
    "In this section, we will train the `MyRNN` model to classify tweets.\n",
    "As the models that we are building begin to increase in complexity, it is important\n",
    "to use good debugging techniques. In this section, we will introduce the technique of\n",
    "checking whether the model and training code is able to overfit on a small training set.\n",
    "This is a way to check for bugs in the implementation.\n",
    "\n",
    "**Task**: Complete the training code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8501ba96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_model(model,                # an instance of MLPModel\n",
    "                train_data,           # training data\n",
    "                val_data,             # validation data\n",
    "                learning_rate=0.001,\n",
    "                batch_size=100,\n",
    "                num_epochs=10,\n",
    "                plot_every=50,        # how often (in # iterations) to track metrics\n",
    "                plot=True):           # whether to plot the training curve\n",
    "    train_loader = torch.utils.data.DataLoader(train_data,\n",
    "                                               batch_size=batch_size,\n",
    "                                               collate_fn=collate_batch,\n",
    "                                               shuffle=True) # reshuffle minibatches every epoch\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # these lists will be used to track the training progress\n",
    "    # and to plot the training curve\n",
    "    iters, train_loss, train_acc, val_acc = [], [], [], []\n",
    "    iter_count = 0 # count the number of iterations that has passed\n",
    "\n",
    "    try:\n",
    "        for e in range(num_epochs):\n",
    "            for i, (texts, labels) in enumerate(train_loader):\n",
    "                z = None # TODO\n",
    "\n",
    "                loss = None # TODO\n",
    "\n",
    "                loss.backward() # propagate the gradients\n",
    "                optimizer.step() # update the parameters\n",
    "                optimizer.zero_grad() # clean up accumualted gradients\n",
    "\n",
    "                iter_count += 1\n",
    "                if iter_count % plot_every == 0:\n",
    "                    iters.append(iter_count)\n",
    "                    ta = accuracy(model, train_data)\n",
    "                    va = accuracy(model, val_data)\n",
    "                    train_loss.append(float(loss))\n",
    "                    train_acc.append(ta)\n",
    "                    val_acc.append(va)\n",
    "                    print(iter_count, \"Loss:\", float(loss), \"Train Acc:\", ta, \"Val Acc:\", va)\n",
    "    finally:\n",
    "        # This try/finally block is to display the training curve\n",
    "        # even if training is interrupted\n",
    "        if plot:\n",
    "            plt.figure()\n",
    "            plt.plot(iters[:len(train_loss)], train_loss)\n",
    "            plt.title(\"Loss over iterations\")\n",
    "            plt.xlabel(\"Iterations\")\n",
    "            plt.ylabel(\"Loss\")\n",
    "\n",
    "            plt.figure()\n",
    "            plt.plot(iters[:len(train_acc)], train_acc)\n",
    "            plt.plot(iters[:len(val_acc)], val_acc)\n",
    "            plt.title(\"Accuracy over iterations\")\n",
    "            plt.xlabel(\"Iterations\")\n",
    "            plt.ylabel(\"Loss\")\n",
    "            plt.legend([\"Train\", \"Validation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a1bf35",
   "metadata": {},
   "source": [
    "**Graded Task**: As a way to check the model and training code, \n",
    "check if your model can obtain a 100\\% training accuracy relatively quickly\n",
    "(e.g. within less than a minute of training time), when training on only the\n",
    "first 20 element of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0258a7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyRNN(vocab_size=len(vocab),\n",
    "              emb_size=300,\n",
    "              hidden_size=64,\n",
    "              num_classes=2)\n",
    "# TODO: Include your code and output "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523f7b3b",
   "metadata": {},
   "source": [
    "**Task**: Will this model that you trained above have a high accuracy over\n",
    "the validation set? Explain why or why not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd423d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your explanation goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99026f0c",
   "metadata": {},
   "source": [
    "**Graded Task**: Train your model on the full data set. What validation accuracy\n",
    "can you achieve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4343c761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Include your code here. Try a few hyperparameter choices until you\n",
    "# are satisfied that your model performance is reasonable (i.e. no obviously\n",
    "# poor hyperparameter choices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e172c4",
   "metadata": {},
   "source": [
    "Instead of a (vanilla) RNN model, PyTorch also makes available \n",
    "`nn.LSTM` and `nn.GRU` units. They can be used in place of `nn.RNN` without \n",
    "further changes to the `MyRNN` code.\n",
    "\n",
    "In general, gated units like LSTM's are much more frequently used than vanilla RNNs,\n",
    "although transformers are much more popular now as well.\n",
    "\n",
    "## Part 4. Pretrained Embeddings\n",
    "\n",
    "As we saw in the previous lab on images, **transfer learning** is a useful technique\n",
    "in practical machine learning, especially in low-data settings:\n",
    "instead of training an entire neural network from scratch, we use (part of) a\n",
    "model that is pretrained on large amounts of similar data. We use the intermediate\n",
    "state of this pretrained model as features to our model---i.e. we use the pretrained\n",
    "models to compute *features*.\n",
    "\n",
    "Just like with images, using a pretrained model is an important strategy for working\n",
    "with text. Large language models is an excellent demonstration of how generalizable\n",
    "pretrained features can be.\n",
    "\n",
    "In this part of the lab, we will use a slightly older idea of using pretrained *word embeddings*.\n",
    "In particular, instead of training our own `nn.Embedding` layer, we will use\n",
    "GloVe embeddings (2014) [https://nlp.stanford.edu/projects/glove/](https://nlp.stanford.edu/projects/glove/)\n",
    "trained on a large data set containing all of Wikipedia and other webpages.\n",
    "\n",
    "Nowadays, large language model (LLMs), including those with APIs provided by various organizations,\n",
    "can also be used to map words/sentences into embeddings.\n",
    "However, the basic idea of using pretrained models in low-data settings remains similar.\n",
    "We will also identify some bias issues with pretrained word embeddings.\n",
    "There is evidence that these types of bias issues \n",
    "continues to persist in LLMs as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41308fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import GloVe\n",
    "\n",
    "glove = torchtext.vocab.GloVe(name=\"6B\", dim=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b4dfb9",
   "metadata": {},
   "source": [
    "**Task**: Run the below code to print the GloVe word embedding for the word \"cat\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b09e544",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(glove['cat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6b9cc9",
   "metadata": {},
   "source": [
    "Unfortunately, it is not straightforward to add the `<pad>`, `<unk>`, `<bos>` and `<eos>`\n",
    "tokens. So we will do without them.\n",
    "\n",
    "**Task**: Run the below code to look up GloVe word indices for the training, validation, and \n",
    "test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d30894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_indices_glove(data, default=len(glove)-1):\n",
    "    result = []\n",
    "    for text, label in data:\n",
    "        words = tokenizer(text) # for simplicity, we wont use <bos> and <eos>\n",
    "        indices = []\n",
    "        for w in words:\n",
    "            if w in glove.stoi:\n",
    "                indices.append(glove.stoi[w])\n",
    "            else:\n",
    "                # this is a bit of a hack, but we will repurpose *last* word\n",
    "                # (least common word) appearing in the GloVe vocabluary as our\n",
    "                # '<pad>' token\n",
    "                indices.append(default)\n",
    "        result.append((indices, label),)\n",
    "    return result\n",
    "\n",
    "train_data_glove = convert_indices_glove(train_data)\n",
    "val_data_glove = convert_indices_glove(val_data)\n",
    "test_data_glove = convert_indices_glove(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ff08a2",
   "metadata": {},
   "source": [
    "Now, we will modify the `MyRNN` to use the pretrained GloVe vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96172f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGloveRNN(nn.Module):\n",
    "    def __init__(self,  hidden_size, num_classes):\n",
    "        super(MyGloveRNN, self).__init__()\n",
    "        self.vocab_size, self.emb_size = glove.vectors.shape\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_classes = num_classes\n",
    "        self.emb = nn.Embedding.from_pretrained(glove.vectors)\n",
    "        self.emb.requires_grad=False # do *not* update the glove embeddings\n",
    "        self.rnn = nn.RNN(self.emb_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Look up the embedding\n",
    "        wordemb = self.emb(X)\n",
    "        # Forward propagate the RNN\n",
    "        h, out = self.rnn(wordemb)\n",
    "        # combine the hidden features computed from *each* time step of\n",
    "        # the RNN. we do this by \n",
    "        features = torch.cat([torch.amax(h, dim=1),\n",
    "                              torch.mean(h, dim=1)], axis=-1)\n",
    "        # Compute the final prediction\n",
    "        z = self.fc(features)\n",
    "        return z\n",
    "\n",
    "    def parameters(self):\n",
    "        # do not return the parameters of self.emb \n",
    "        # so the optimizer will not update the parameters of self.emb\n",
    "        return (p for p in super(MyGloveRNN, self).parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "model = MyGloveRNN(64, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d88381",
   "metadata": {},
   "source": [
    "**Task** Train this model. Use comparable hyperparameters so that you can compare\n",
    "your result against `MyRNN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d34cc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train your model here, and include the output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc698235",
   "metadata": {},
   "source": [
    "**Graded Task**: You might notice that a *very* smaller number of \n",
    "iterations will be required to train this model to a reasonable\n",
    "performance (e.g. >70% validation accuracy). Why might this be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daa9159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Include your explanation here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20240dc",
   "metadata": {},
   "source": [
    "**Graded Task**: Train both MyGloveRNN and MyRNN models using the corresponding\n",
    "embeddings (pretrained vs. not), **but only with the first 200 data points in the\n",
    "training set**. How do the validation accuracies compare between these two models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b9072e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Training code for MyGloveRNN.\n",
    "# Include outputs and training curves in your submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c9aea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Training code for MyRNN\n",
    "# Include outputs and training curves in your submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3042bed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare the validation accuaries here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfc0d6d",
   "metadata": {},
   "source": [
    "Machine learning models have an air of \"fairness\" about them, since models\n",
    "make decisions without human intervention. However, models can and do learn\n",
    "whatever bias is present in the training data.\n",
    "GloVe vectors seems innocuous enough: they are just representations of\n",
    "words in some embedding space. Even so, we will show that the structure\n",
    "of the GloVe vectors encodes the everyday biases present in the texts\n",
    "that they are trained on.\n",
    "\n",
    "We start with an example analogy to demonstrate the power of GloVe embeddings\n",
    "that allows us to complete analogies by applying arithmetic operations\n",
    "to the word vectors.\n",
    "\n",
    "$$doctor - man + woman \\approx ??$$\n",
    "\n",
    "To find the answers to the above analogy, we will compute the following vector,\n",
    "and then find the word whose vector representation is *closest* to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416955a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = glove['doctor'] - glove['man'] + glove['woman']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2402283",
   "metadata": {},
   "source": [
    "**Task**: Run the code below to find the closets word. You should see the word\n",
    "\"nurse\" fairly high up in that list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fb9d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_closest_words(vec, n=5):\n",
    "    dists = torch.norm(glove.vectors - vec, dim=1)     # compute distances to all words\n",
    "    lst = sorted(enumerate(dists.numpy()), key=lambda x: x[1]) # sort by distance\n",
    "    for idx, difference in lst[1:n+1]: \t\t\t\t\t       # take the top n\n",
    "        print(glove.itos[idx], difference)\n",
    "\n",
    "print_closest_words(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db42a63",
   "metadata": {},
   "source": [
    "**Task**:  To compare, use a similar method to find the answer to this analogy:\n",
    "$$doctor - woman + man \\approx ??$$\n",
    "\n",
    "In other words, we go the opposite direction in the \"gender\" axis to check\n",
    "if similarly concerning analogies exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72d150e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_closest_words(glove['doctor'] - glove['woman'] + glove['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c58e2d",
   "metadata": {},
   "source": [
    "**Task**: Compare the following two outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da371f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_closest_words(glove['programmer'] - glove['man'] + glove['woman'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa86b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_closest_words(glove['programmer'] - glove['woman'] + glove['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf4124d",
   "metadata": {},
   "source": [
    "**Task**: Compare the following two outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d06e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_closest_words(glove['professor'] - glove['man'] + glove['woman'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37c91e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_closest_words(glove['professor'] - glove['woman'] + glove['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3206f0e3",
   "metadata": {},
   "source": [
    "**Task**: Compare the following two outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c803ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_closest_words(glove['engineer'] - glove['man'] + glove['woman'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba13f55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_closest_words(glove['engineer'] - glove['woman'] + glove['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907fd276",
   "metadata": {},
   "source": [
    "**Graded Task**: Explain where the bias in these embeddings come from.\n",
    "Would you expect our word embeddings (trained on tweets) to be similarly\n",
    "problematic? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bc61d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your explanation goes here"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
