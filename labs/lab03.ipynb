{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSC413 Lab 3: Multi-Layer Perceptrons with MedMNIST\n",
    "\n",
    "MedMNIST's PneumoniaMNIST data set. We will now transition fully to \n",
    "using PyTorch for our labs going forward. \n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "1. Build and train an MLP using PyTorch\n",
    "2. Define the following performance metrics for evaluating machine learning models: true positive, true negative, false positive, false negative, precision, recall, ROC curve, and AUC.\n",
    "3. Interpret the confusion matrix.\n",
    "4. Explain the advantages of the AUC metrics over accuracy metrics.\n",
    "5. Perform grid search to find hyperparameters.\n",
    "\n",
    "Acknowledgements:\n",
    "\n",
    "- The MedMNIST data is from https://medmnist.com/\n",
    "- This assignment is written by Mahdi Haghifam, Sonya Allin, Lisa Zhang, Mike Pawliuk and Rutwa Engineer\n",
    "\n",
    "Please work in groups of 1-2 during the lab.\n",
    "\n",
    "## Submission\n",
    "\n",
    "If you are working with a partner, start by creating a group on Markus. If you are working alone,\n",
    "click \"Working Alone\".\n",
    "\n",
    "Submit the ipynb file `lab03.ipynb` on Markus \n",
    "**containing all your solutions to the Graded Task**s.\n",
    "Your notebook file must contain your code **and outputs** where applicable,\n",
    "including printed lines and images.\n",
    "Your TA will not run your code for the purpose of grading.\n",
    "\n",
    "For this lab, you should submit the following:\n",
    "\n",
    "- Part 2. Your expression that computes the number of trainable parameters in the MLPModel (1 point)\n",
    "- Part 2. Your implementation of `accuracy`. (1 point)\n",
    "- Part 2. Your implementation of `train_model`. (2 points)\n",
    "- Part 3. Your implementation of `precision` and `recall`. (2 points)\n",
    "- Part 3. Your interpretation of the confusion matrix for `m_once` (1 point)\n",
    "- Part 4. Your completion of the grid search, along with the output (2 point)\n",
    "- Part 4. Your description of why a model with high AUC may still perform poorly for some groups (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Colab Setup\n",
    "\n",
    "We will be using the `medmnist` data set, which is available as a Python package.\n",
    "Recall that on Google Colab, we use \"!\" to run shell commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install medmnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Data\n",
    "\n",
    "We will use the MedMNIST data set, which is described here: [https://medmnist.com/](https://medmnist.com/). \n",
    "We will use the PneumoniaMNIST images, which are greyscale chest X-ray images that has been resized to 28x28.\n",
    "The task is to predict, given one of these X-ray images, whether the patient has pneumonia or not---a binary\n",
    "classification task.\n",
    "We chose this dataset both because it is lightweight, and because it allows us to discuss the sensitive nature of\n",
    "biomedical images. \n",
    "\n",
    "Let's begin by printing some information about the PneumoniaMNIST  data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import medmnist\n",
    "from medmnist import PneumoniaMNIST\n",
    "\n",
    "medmnist.INFO['pneumoniamnist']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: The dataset providers already split the data into training, validation, and test sets.\n",
    "How many samples are there in the training, validation, and test sets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visually inspect the first element of the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_imgs = PneumoniaMNIST(split='train', download=True)\n",
    "\n",
    "for img, target in train_data_imgs:\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    print(np.array(img)) # img is a numpy array of shape 28x28 , with integer values between 0-255\n",
    "    print(target)        # the target\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Based on the code above, what is the type of the data structure `train_data`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: The code below plots 5 images from each class: normal and pneumonia.\n",
    "Do you notice qualitative differences between these two sets of images?\n",
    "It is always important to qualitatively assess your data prior to training, so that you\n",
    "can develop intuition as to what features may or may not be important for your model.\n",
    "Understanding your data also helps to estimate how challenging the classification problem\n",
    "may be and identify incorrect implementations (e.g., a surprisingly high model accuracy could\n",
    "indicate issues with training set leakage into the test set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normal images\n",
    "plt.figure()\n",
    "n = 0\n",
    "for img, target in train_data_imgs:\n",
    "    if int(target) == 0:\n",
    "      plt.subplot(1, 5, n+1)\n",
    "      plt.title(\"normal\")\n",
    "      plt.imshow(img, cmap='gray')\n",
    "      n += 1\n",
    "    if n >= 5:\n",
    "      break\n",
    "# pneumonia images\n",
    "plt.figure()\n",
    "n = 0\n",
    "for img, target in train_data_imgs:\n",
    "    if int(target) == 1:\n",
    "      plt.subplot(1, 5, n+1)\n",
    "      plt.title(\"pneumonia\")\n",
    "      plt.imshow(img, cmap='gray')\n",
    "      n += 1\n",
    "    if n >= 5:\n",
    "      break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write your explanation here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch makes it easy to apply pre-processing transformations to the data, for example to normalize\n",
    "the data prior to using for training. We will use the standard preprocessing functions to\n",
    "*transform the images into tensors* for PyTorch to be able to use. This transformation also\n",
    "changes the values to be floating-point numbers between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms # contains a collection of transformations\n",
    "\n",
    "train_data = PneumoniaMNIST(split='train', download=True, transform=transforms.ToTensor())\n",
    "val_data = PneumoniaMNIST(split='val', download=True, transform=transforms.ToTensor())\n",
    "test_data = PneumoniaMNIST(split='test', download=True, transform=transforms.ToTensor())\n",
    "\n",
    "for img, target in train_data:\n",
    "    print(img)    # img is a PyTorch tensor fo shape 1x28x28\n",
    "    print(target) # the target\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: How many X-ray images are in the training set *with* pneumonia? What about *without* pneumonia?\n",
    "What about the validation/test sets?\n",
    "What does your answer say about the data balance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write code to find the answer here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Model and Training\n",
    "\n",
    "We will build our own PyTorch model, which will be a subclass of `nn.Module`.\n",
    "This subclass provides the important methods that we used in the training loop\n",
    "in lab 1, including the methods that allow us to compute the forward pass\n",
    "by calling the model object, and other methods used under the hood to compute\n",
    "the backward pass.\n",
    "\n",
    "Our model will be a three-layer MLP with the following architecture:\n",
    "ACTUALTODO---the model architecture may change!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class MLPModel(nn.Module):\n",
    "    \"\"\"A three-layer MLP model for binary classification\"\"\"\n",
    "    def __init__(self, input_dim=28*28, num_hidden=100):\n",
    "        super(MLPModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, num_hidden)\n",
    "        self.fc2 = nn.Linear(num_hidden, num_hidden)\n",
    "        self.fc3 = nn.Linear(num_hidden, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Graded Task**: How many trainable parameters are in this model?\n",
    "Express your answer in terms of `input_dim` and `num_hidden`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute the number of trainable parameters in MLPModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to assess model performance, we will begin by\n",
    "implementing the `accuracy` function, which computes the accuracy\n",
    "of the model across a dataset.\n",
    "\n",
    "**Graded Task**: Complete the `accuracy` function. Keep in mind\n",
    "that this function will be slightly different from the `accuracy`\n",
    "function in lab 1, since we are working on a binary classification\n",
    "problem and prediction here is a single logit value (rather than\n",
    "a vector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model, dataset):\n",
    "    \"\"\"\n",
    "    Compute the accuracy of `model` over the `dataset`.\n",
    "    We will take the **most probable class**\n",
    "    as the class predicted by the model.\n",
    "\n",
    "    Parameters:\n",
    "        `model` - A PyTorch MLPModel\n",
    "        `dataset` - A data structure that acts like a list of 2-tuples of\n",
    "                  the form (x, t), where `x` is a PyTorch tensor of shape\n",
    "                  [1, 28, 28] representing an MedMNIST image,\n",
    "                  and `t` is the corresponding binary target label\n",
    "\n",
    "    Returns: a floating-point value between 0 and 1.\n",
    "    \"\"\"\n",
    "\n",
    "    correct, total = 0, 0\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=100)\n",
    "    for img, t in loader:\n",
    "        X = img.reshape(-1, 784)\n",
    "        z = model(X)\n",
    "\n",
    "        y = None # TODO: pred should be a [N, 1] tensor with binary \n",
    "                    # predictions, (0 or 1 in each entry)\n",
    "\n",
    "        correct += int(torch.sum(t == y))\n",
    "        total   += t.shape[0]\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we are working with binary classification, we will be using a\n",
    "different implementation of the cross-entropy loss function, implemented\n",
    "via PyTorch in a class called `BCEWithLogitsLoss` (short for\n",
    "Binary Cross Entropy with Logits loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loss function takes a predicted logit (pre-softmax activation)\n",
    "and the ground-truth label. \n",
    "The use of pre-softmax logits rather than prediction probabilities is\n",
    "due to numerical stability reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(criterion(torch.tensor([2.5]),  # predicted\n",
    "                torch.tensor([1.])))  # actual\n",
    "\n",
    "print(criterion(torch.tensor([-2.5]), # predicted\n",
    "                torch.tensor([1.])))  # actual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Explain why the second printed \n",
    "value above is *larger* than the first. In other words,\n",
    "why does it make sense that we think of the second prediction\n",
    "(logit of z=-2.5) as \"worse\" than the first (logit of z=2.5)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your explanation goes here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Graded Task**: Complete the following code to be used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,                # an instance of MLPModel\n",
    "                train_data,           # training data\n",
    "                val_data,             # validation data\n",
    "                learning_rate=0.1,\n",
    "                batch_size=100,\n",
    "                num_epochs=10,\n",
    "                plot_every=50,        # how often (in # iterations) to track metrics\n",
    "                plot=True):           # whether to plot the training curve\n",
    "    train_loader = torch.utils.data.DataLoader(train_data,\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=True) # reshuffle minibatches every epoch\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # these lists will be used to track the training progress\n",
    "    # and to plot the training curve\n",
    "    iters, train_loss, train_acc, val_acc = [], [], [], []\n",
    "    iter_count = 0 # count the number of iterations that has passed\n",
    "\n",
    "    try:\n",
    "        for e in range(num_epochs):\n",
    "            for i, (images, labels) in enumerate(train_loader):\n",
    "                z = None # TODO\n",
    "  \n",
    "                loss = None # TODO\n",
    "  \n",
    "                loss.backward() # propagate the gradients\n",
    "                optimizer.step() # update the parameters\n",
    "                optimizer.zero_grad() # clean up accumualted gradients\n",
    "  \n",
    "                iter_count += 1\n",
    "                if iter_count % plot_every == 0:\n",
    "                    iters.append(iter_count)\n",
    "                    ta = accuracy(model, train_data)\n",
    "                    va = accuracy(model, val_data)\n",
    "                    train_loss.append(float(loss))\n",
    "                    train_acc.append(ta)\n",
    "                    val_acc.append(va)\n",
    "                    print(iter_count, \"Loss:\", float(loss), \"Train Acc:\", ta, \"Val Acc:\", va)\n",
    "    finally:\n",
    "        # This try/finally block is to display the training curve\n",
    "        # even if training is interrupted\n",
    "        if plot:\n",
    "            plt.figure()\n",
    "            plt.plot(iters[:len(train_loss)], train_loss)\n",
    "            plt.title(\"Loss over iterations\")\n",
    "            plt.xlabel(\"Iterations\")\n",
    "            plt.ylabel(\"Loss\")\n",
    "\n",
    "            plt.figure()\n",
    "            plt.plot(iters[:len(train_acc)], train_acc)\n",
    "            plt.plot(iters[:len(val_acc)], val_acc)\n",
    "            plt.title(\"Accuracy over iterations\")\n",
    "            plt.xlabel(\"Iterations\")\n",
    "            plt.ylabel(\"Accuracy\")\n",
    "            plt.legend([\"Train\", \"Validation\"])\n",
    "\n",
    "# Please include the output of this cell for grading\n",
    "model = MLPModel()\n",
    "train_model(model, train_data, val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Suppose that a model has a validation accuracy of 74% for this \n",
    "binary classification task. Why would this model be considered a very bad model?\n",
    "Your answer should illustrate why accuracy may not be an excellent tool to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write your explanation here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. Performance Metrics\n",
    "\n",
    "We often use accuracy as a go-to metric when evaluating the performance of\n",
    "a classification model. However, the accuracy measure weighs all errors equally.\n",
    "A deeper look into the types of errors made can provide a more complete picture\n",
    "of model performance, especially when there is data imbalance and---when applying\n",
    "models in real situations---when some errors may be associated with more serious\n",
    "impacts to users than others. \n",
    "\n",
    "To start our explorations, we'll look at the decisions we made well, i.e. the:\n",
    "\n",
    "- True Positives (TP), or positive outcomes that were correctly predicted as positive.\n",
    "- True Negatives (TN), or negative outcomes that were correctly predicted as negative.\n",
    "\n",
    "Then we will look at our mistakes, i.e. the:\n",
    "\n",
    "- False Positives (FP, or Type I errors), or negative outcomes that were predicted as positive. In our case, this occurs when our model predicts that a person has heart disease, but they do not.\n",
    "- False Negatives (FN, or Type II errors), or positive outcomes that were predicted as negative. In our case, this occurs when our model predicts that a person does not have heart disease, but they do.\n",
    "\n",
    "We can then use the metrics above to calculate:\n",
    "\n",
    "- Precision (or True Positive Rate, or Positive Predicive Value): $\\frac{TP}{TP + FP}$. The answers the question: out of all the examples that we predicted as positive, how many are really positive?\n",
    "- Recall (or Sensitivity): $\\frac{TP}{TP + FN}$. The answers the question: out of all the positive examples in the data set, how many did we predict as positive?\n",
    "- False Positive Rate (or Negative Predicive Value): $\\frac{TN}{TN + FN}$. The answers the question: out of all the examples that we predicted as negative, how many are really negative?\n",
    "\n",
    "\n",
    "**Graded Task**: Complete the functions `precision` and `recall`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(model, dataset):\n",
    "    \"\"\"\n",
    "    Compute the precision of `model` over the `dataset`.  We will take the\n",
    "    **most probable class** as the class predicted by the model.\n",
    "\n",
    "    Parameters:\n",
    "        `model` - A PyTorch MLPModel\n",
    "        `dataset` - A data structure that acts like a list of 2-tuples of\n",
    "                  the form (x, t), where `x` is a PyTorch tensor of shape\n",
    "                  [1, 28, 28] representing an MedMNIST image,\n",
    "                  and `t` is the corresponding binary target label\n",
    "\n",
    "    Returns: a floating-point value between 0 and 1.\n",
    "    \"\"\"\n",
    "    true_pos, total_pred_pos = 0, 0\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=100)\n",
    "    for img, t in loader:\n",
    "        X = img.reshape(-1, 784)\n",
    "        z = model(X)\n",
    "\n",
    "        y = None # TODO: pred should be a [N, 1] tensor with binary \n",
    "                    # predictions, (0 or 1 in each entry)\n",
    "\n",
    "        # TODO: update total_pred_pos and true_pos\n",
    "    return true_pos / total_pred_pos\n",
    "\n",
    "\n",
    "def recall(model, dataset):\n",
    "    \"\"\"\n",
    "    Compute the recall (or sensitivity) of `model` over the `dataset`.  We will\n",
    "    take the **most probable class** as the class predicted by the model.\n",
    "\n",
    "    Parameters:\n",
    "        `model` - A PyTorch MLPModel\n",
    "        `dataset` - A data structure that acts like a list of 2-tuples of\n",
    "                  the form (x, t), where `x` is a PyTorch tensor of shape\n",
    "                  [1, 28, 28] representing an MedMNIST image,\n",
    "                  and `t` is the corresponding binary target label\n",
    "\n",
    "    Returns: a floating-point value between 0 and 1.\n",
    "    \"\"\"\n",
    "    true_pos, total_actual_pos = 0, 0 # track the true and false positive\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=100)\n",
    "    for img, t in loader:\n",
    "        X = img.reshape(-1, 784)\n",
    "        z = model(X)\n",
    "\n",
    "        y = None # TODO: pred should be a [N, 1] tensor with binary \n",
    "                    # predictions, (0 or 1 in each entry)\n",
    "\n",
    "        # TODO: update total_pos and true_pos\n",
    "    return true_pos / total_actual_pos\n",
    "\n",
    "print(\"Precision(Training)\", precision(model, train_data))\n",
    "print(\"Recall(Training)\", recall(model, train_data))\n",
    "print(\"Precision(Validation)\", precision(model, val_data))\n",
    "print(\"Recall(Validation)\", recall(model, val_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **confusion matrix** is a table that shows the number of TP, TN, FP, and FN.  A confusion matrix can be a valuable tool in understanding \n",
    "why a model makes the mistake that it makes.\n",
    "\n",
    "**Task** Run the code below to display the confusion matrix for your model\n",
    "for the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "def get_prediction(model, data, sample=1000):\n",
    "    loader = torch.utils.data.DataLoader(data, batch_size=sample, shuffle=True)\n",
    "    for X, t in loader:\n",
    "        z = model(X.view(-1, 784))\n",
    "        y = torch.sigmoid(z)\n",
    "        break\n",
    "    y = y.detach().numpy()\n",
    "    t = t.detach().numpy()\n",
    "    return y, t\n",
    "\n",
    "y, t = get_prediction(model, val_data)\n",
    "y = y > 0.5\n",
    "cm = confusion_matrix(t, y)\n",
    "cmp = ConfusionMatrixDisplay(cm, display_labels=[\"0\", \"1\"])\n",
    "cmp.plot()\n",
    "plt.title(\"Confusion Matrix (Val Data)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: The code below trains a MLPModel for a very few number \n",
    "of iterations. You should see that this model achieves a 74% accuracy.\n",
    "Display the confusion matrix for this model by running the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_once = MLPModel()\n",
    "train_model(m_once, train_data, val_data, learning_rate=0.5, batch_size=500, num_epochs=1)\n",
    "print(\"Training Accuracy:\", accuracy(m_once, train_data))\n",
    "print(\"Validation Accuracy:\", accuracy(m_once, val_data))\n",
    "\n",
    "y, t = get_prediction(m_once, val_data)\n",
    "y = y > 0.5\n",
    "ConfusionMatrixDisplay(confusion_matrix(t, y), display_labels=[\"0\", \"1\"]).plot()\n",
    "plt.title(\"Confusion Matrix (Val Data)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Graded Task**: What does the confusion matrix tell you about how\n",
    "the `m_once` model is achieving 74% accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your explanation goes here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have been choosing a threshold of 0.5 for turning our continuous predicted\n",
    "probabilities into a discrete prediction. However, this can be an\n",
    "arbitrary choice. \n",
    "\n",
    "**Task**: Explain why, in practical application, it may be reasonable to use a different \n",
    "threshold value. In what situation might you want the threshold to be set\n",
    "very high in order to make a positive prediction? What about a negative prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your explanation goes here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **receiver operating characteristic curve** (or ROC) shows how the True Positive Rate and False Positive Rate vary based on our choice of the decision making threshold used to binarize predictions.  By default, this threshold is 0.5, but it can be changed to any value between 0 and 1. Different thresholds will result in different TP and FP rates, all of which are illustrated on our graph. we can calculate the area underneath this curve in order to get a sense as to how our classifiers might work across a wide range of different thresholds. This calcution of area can also be used as a metric of our model's \"goodness\", and it is called AUC (or \"Area Under Curve\").\n",
    "\n",
    "The AUC metric is particularly useful for machine learning practitioners\n",
    "because it does *not* depend on the choice of the threshold value used\n",
    "for making discrete predicions. The metric is also resistant to \n",
    "measurement.\n",
    "\n",
    "**Task**: Is it better for the AUC to be larger or smaller? Explain why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your explanation goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below plots the ROC curve for a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, RocCurveDisplay, auc\n",
    "\n",
    "y, t = get_prediction(model, val_data)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(t, y)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "rocp = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc)\n",
    "rocp.plot()\n",
    "plt.title(\"Validation ROC Curve\")\n",
    "\n",
    "\n",
    "y, t = get_prediction(model, train_data)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(t, y)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "rocp = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc)\n",
    "rocp.plot()\n",
    "plt.title(\"Training ROC Curve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a function you can use to estimate the auc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_auc(model, data):\n",
    "    y, t = get_prediction(model, data)\n",
    "    fpr, tpr, thresholds = roc_curve(t, y)\n",
    "    return auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4. Hyperparameter Tuning via Grid Search\n",
    "\n",
    "As we mentioned in lab 1, hyperparameter choices matter significantly,\n",
    "and these hyperparameter choices interact with one another. \n",
    "Practitioners use a strategy called **grid search** to try\n",
    "all variations of hyperparameters from a set of hyperparameters.\n",
    "\n",
    "One very important hyperparameter is the number of hidden units in our MLPModel.\n",
    "This setting affects the number of parameters (weights/biases) used in our model.\n",
    "\n",
    "The use of ReLU vs sigmoid activation function is another hyperparameter that we\n",
    "will explore. \n",
    "\n",
    "Finally, optimization parameters like the batch size and the learning rate can\n",
    "also significantly affect the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPModelSigmoid(nn.Module):\n",
    "    \"\"\"A three-layer MLP model for binary classification\"\"\"\n",
    "    def __init__(self, input_dim=28*28, num_hidden=100):\n",
    "        super(MLPModelSigmoid, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, num_hidden)\n",
    "        self.fc2 = nn.Linear(num_hidden, num_hidden)\n",
    "        self.fc3 = nn.Linear(num_hidden, 1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.sig(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sig(out)\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Graded Task**: Complete the code below, which performs grid search over the following hyperparameter\n",
    "values of the:\n",
    "\n",
    "- hidden size\n",
    "- activation function (ReLu vs sigmoid activation)\n",
    "- batch size\n",
    "- learning rate\n",
    "\n",
    "Do so by creating a new model and train it with the appropriate\n",
    "settings, then assessing the final training/validation accuracy,\n",
    "precision, recall, and AUC score. You may use to use\n",
    "the flag `plot=False` when calling `train_model`. You might\n",
    "also set `plot_every` to a large value and visualize the\n",
    "training curve as a separate step for hyperparameter values\n",
    "that you're interested in.\n",
    "\n",
    "Please include all your output in your submission. \n",
    "\n",
    "(There is one more graded task below that you can complete while the \n",
    "hyperparameter tuning is running.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch = {}\n",
    "for num_hidden in [25, 100, 250]:\n",
    "    for act in [\"relu\", \"sigmoid\"]:\n",
    "        for bs in [10, 100, 500]:\n",
    "            for lr in [0.01, 0.1]:\n",
    "                # Adjust num_epoch based on the batch size, so that we \n",
    "                # train for the same number of iterations irrespective\n",
    "                # of batch size\n",
    "                ne = int(20 * (bs/100))\n",
    "\n",
    "                modelname = f\"num_hidden: {num_hidden}, activation: {act}, batch_size: {bs}, learning_rate: {lr}\"\n",
    "                print(f\"========={modelname}\")\n",
    "\n",
    "                # TODO: create and train the model with the appropriate settings\n",
    "\n",
    "                # Update and display metrics. This part is done for you.\n",
    "                metrics = {\n",
    "                    \"acc_train\": accuracy(m, train_data),\n",
    "                    \"acc_val\": accuracy(m, val_data),\n",
    "                    \"precision_train\": precision(m, train_data),\n",
    "                    \"precision_val\": precision(m, val_data),\n",
    "                    \"recall_train\": recall(m, train_data),\n",
    "                    \"recall_val\": recall(m, val_data),\n",
    "                    \"auc_train\": get_auc(m, train_data),\n",
    "                    \"auc_val\": get_auc(m, val_data),\n",
    "                }\n",
    "                gridsearch[modelname] = metrics\n",
    "                print(f'Accuracy (train):{metrics[\"acc_train\"]} (val):{metrics[\"acc_val\"]}')\n",
    "                print(f'Precision (train):{metrics[\"precision_train\"]} (val):{metrics[\"precision_val\"]}')\n",
    "                print(f'Recall (train):{metrics[\"recall_train\"]} (val):{metrics[\"recall_val\"]}')\n",
    "                print(f'AUC (train):{metrics[\"auc_train\"]} (val):{metrics[\"auc_val\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please include the below output in your submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gridsearch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Which hyperparameter choice is the \"best\"? You should base this answer on the \n",
    "validation AUC. Use the other metrics as a guide to understand the kinds of predictions and mistakes\n",
    "that your model is likely make. \n",
    "Train a final model with those hyperparameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Report the test accuracy and AUC for this model, and plot the confusion matrix over the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Graded Task**: Explain why a model with high AUC may still\n",
    "produce consistently poor predictions for a subset of the population.\n",
    "You might find this article interesting: [Gender imbalance in medical imaging datasets produces biased classifiers for computer-aided diagnosis](https://www.pnas.org/doi/10.1073/pnas.1919012117); in particular, \n",
    "Figure 1 shows how test AUC differs male/female patients depending on the training set used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
