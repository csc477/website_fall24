[
  {
    "objectID": "course-grading.html",
    "href": "course-grading.html",
    "title": "Grading Scheme",
    "section": "",
    "text": "Assignment 1\n15%\n\n\nAssignment 2\n15%\n\n\nPanel Discussion\n10%\n\n\nProject Proposal\n10%\n\n\nProject Presentation\n25%\n\n\nFinal Project Report\n25%\n\n\n\n\nMarking rubric for panel discussion\nEvery week we will reserve 20-30 mins for a panel discussion based on the assigned reading for that day (4-5 papers). This discussion will include three types of roles: panel members, audience members, and a moderator. Each panel discussion will include 4 panel members, 1 moderator, and audience members. Panel members are responsible for answering questions, the audience is responsible for asking questions, and the moderator is responsible for steering the discussion and having backup questions if the audience is not asking any.\n\nPanel member evaluation\n\nAnswering questions from the moderator and the audience correctly / well (6 pts)\nEngaging with points of other panelists (1 pts)\nKeeping answers brief / allowing other people time to speak (2 pts)\nPre-submitting 1-2 questions on Quercus about the assigned papers of the day, the Thursday before lecture (1 pts)\n\n\n\nAudience member evaluation\n\nPre-submitting 1-2 questions on Quercus about the assigned papers of the day, the Thursday before lecture (10 pts)\n\n\n\nModerator evaluation\n\nSteering the discussion in terms of groups / themes of questions (2 pts)\nEnsuring there is time for every panel member to speak (4 pts)\nEngaging the audience / ensuring the audience has enough time to ask questions (3 pts)\nPre-submitting 1-2 questions on Quercus about the assigned papers of the day, the Thursday before lecture (1 pts)\n\n\n\n\nMarking rubric for the project proposal\n\nIntroduction (1 pts), which states the proposed problem being solved and any applications / implications.\nFigure or diagram (1 pts), showing an overview of your proposed solution, i.e. shows the overall idea in a way that is easily understandable without even reading the rest of the report.\nRelated work (1 pts) and bibliography. Highlight how your method is different from other approaches. Present other approaches in the proper light without diminishing their contributions.\nMethodology (2 pts). Describe your proposed methodology as well as any assumptions it relies on. Explain prerequisite concepts clearly and succinctly.\nEvaluation (2 pts). What experiments are you planning to do and why? What are the questions you want to answer through these experiments?\nTimeline (1 pts). What are the milestones required to complete your project and by when do you plan to complete them?\nAnticipated risks and mitigation plan (2 pts). What issues might arise with your proposed project and timeline and how will you address these issues if they occur?\n\n\n\nMarking rubric for the project presentation\n\nQuality of presentation\n\nSlide design (2 pts)\nDelivery of presentation (3 pts)\nRespecting time constraints (2 pts)\nResponse to questions (3 pts)\n\n\n\nTechnical content\n\nMotivation and definition of the problem (2 pts)\nPutting prior work into context (3 pts)\nMethodology explanation (3 pts)\nDiscussion of experiments (5 pts)\nDiscussion of limitations (2 pts)\n\n\n\n\nMarking rubric for the final project report\n\nAbstract (2 pts) that summarizes the main idea of the project and your contributions.\nIntroduction (3 pts) that states the problem being solved and any applications / implications.\nFigure or diagram (2 pts) that shows the overall idea in a way that is easily understandable.\nRelated work (2 pts) and bibliography. Highlight how your method is different from other approaches. Present other approaches in the proper light without diminishing their contributions.\nMethodology (7 pts). Describe your method in detail as well as any assumptions it relies on. Explain prerequisite concepts clearly and succinctly. Include algorithm descriptions, figures, and equations as you wish.\nEvaluation (8 pts). Include any figures or tables that illustrate your experimental results. Do not forget to include error bars if applicable. Analyze your findings, and comment on their statistical significance. In your evaluation please take into account Joelle Pineau’s ML reproducibility checklist.\nLimitations (2 pts). Describe some settings in which your approach performs poorly, and list a few ideas for how to adddress them. Describe opportunities for future work, as well as open problems.\nConclusions (1 pts). A summary of your contributions and results."
  },
  {
    "objectID": "lecs/w01/lec01.html#todays-agenda",
    "href": "lecs/w01/lec01.html#todays-agenda",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\nIntroduction\nAdministrivia\n\nOffice hours\nTutorials\nAssignment descriptions\nPrerequisites\n\nTopics covered by the course\nSensors and Actuators\nQuiz about background and interests"
  },
  {
    "objectID": "lecs/w01/lec01.html#your-tas",
    "href": "lecs/w01/lec01.html#your-tas",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Your TAs",
    "text": "Your TAs\n\n\nYewon Lee\nMSc student\nComputer Science, UofT\ncsc477-tas@cs.toronto.edu\n\nYasasa Abeysirigoonawardena\nMSc student\nComputer Science, UofT\n\ncsc477-tas@cs.toronto.edu\n\nRadian Gondokaryono\nPhD student\nComputer Science, UofT\n\ncsc477-tas@cs.toronto.edu"
  },
  {
    "objectID": "lecs/w01/lec01.html#my-lab-robot-vision-and-learning-rvl",
    "href": "lecs/w01/lec01.html#my-lab-robot-vision-and-learning-rvl",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "My lab: Robot Vision and Learning (RVL)",
    "text": "My lab: Robot Vision and Learning (RVL)\n\n\n\n\n\n\n\n\n\n\n\n\n\nMission: create algorithms that enable robots to learn to act intelligently in outdoor environments and alongside humans"
  },
  {
    "objectID": "lecs/w01/lec01.html#how-i-became-interested-in-robotics",
    "href": "lecs/w01/lec01.html#how-i-became-interested-in-robotics",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "How I became interested in robotics",
    "text": "How I became interested in robotics\n\n\n\n\n\n\nMars Exploration Rover\n\n\n\n\n \n\n\n\n\n\nRoboCup, small-sized league"
  },
  {
    "objectID": "lecs/w01/lec01.html#how-i-became-interested-in-robotics-1",
    "href": "lecs/w01/lec01.html#how-i-became-interested-in-robotics-1",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "How I became interested in robotics",
    "text": "How I became interested in robotics"
  },
  {
    "objectID": "lecs/w01/lec01.html#how-i-became-interested-in-robotics-2",
    "href": "lecs/w01/lec01.html#how-i-became-interested-in-robotics-2",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "How I became interested in robotics",
    "text": "How I became interested in robotics"
  },
  {
    "objectID": "lecs/w01/lec01.html#today-you-have",
    "href": "lecs/w01/lec01.html#today-you-have",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Today you have",
    "text": "Today you have"
  },
  {
    "objectID": "lecs/w01/lec01.html#factory-automation",
    "href": "lecs/w01/lec01.html#factory-automation",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Factory Automation",
    "text": "Factory Automation\n\n\n\n\n\nAutonomous warehouse robots at Amazon\n\n\n\n\n\n\nAutonomous arms at Tesla"
  },
  {
    "objectID": "lecs/w01/lec01.html#pipe-inspection",
    "href": "lecs/w01/lec01.html#pipe-inspection",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Pipe Inspection",
    "text": "Pipe Inspection\n\nManually-controlled inspection robots"
  },
  {
    "objectID": "lecs/w01/lec01.html#nuclear-disaster-cleanup",
    "href": "lecs/w01/lec01.html#nuclear-disaster-cleanup",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Nuclear Disaster Cleanup",
    "text": "Nuclear Disaster Cleanup\n\nRemote-controlled cleaning robot at Fukushima Daiichi, 2011"
  },
  {
    "objectID": "lecs/w01/lec01.html#nuclear-disaster-cleanup-1",
    "href": "lecs/w01/lec01.html#nuclear-disaster-cleanup-1",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Nuclear Disaster Cleanup",
    "text": "Nuclear Disaster Cleanup\n\nRemote-controlled cleaning robot at Fukushima Daiichi, 2011"
  },
  {
    "objectID": "lecs/w01/lec01.html#nuclear-disaster-cleanup-2",
    "href": "lecs/w01/lec01.html#nuclear-disaster-cleanup-2",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Nuclear Disaster Cleanup",
    "text": "Nuclear Disaster Cleanup\n\nRemote-controlled cleaning robot at Chernobyl, 1986"
  },
  {
    "objectID": "lecs/w01/lec01.html#aerial-package-delivery",
    "href": "lecs/w01/lec01.html#aerial-package-delivery",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Aerial Package Delivery",
    "text": "Aerial Package Delivery"
  },
  {
    "objectID": "lecs/w01/lec01.html#aerial-first-aid-delivery",
    "href": "lecs/w01/lec01.html#aerial-first-aid-delivery",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Aerial First-Aid Delivery",
    "text": "Aerial First-Aid Delivery"
  },
  {
    "objectID": "lecs/w01/lec01.html#smart-wheelchairs",
    "href": "lecs/w01/lec01.html#smart-wheelchairs",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Smart Wheelchairs",
    "text": "Smart Wheelchairs"
  },
  {
    "objectID": "lecs/w01/lec01.html#robot-surgery",
    "href": "lecs/w01/lec01.html#robot-surgery",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Robot Surgery",
    "text": "Robot Surgery\ndaVinci robot-assisted surgery"
  },
  {
    "objectID": "lecs/w01/lec01.html#precision-agriculture",
    "href": "lecs/w01/lec01.html#precision-agriculture",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Precision Agriculture",
    "text": "Precision Agriculture\n\nfarmbot.io"
  },
  {
    "objectID": "lecs/w01/lec01.html#self-driving-trucks",
    "href": "lecs/w01/lec01.html#self-driving-trucks",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Self-driving Trucks",
    "text": "Self-driving Trucks"
  },
  {
    "objectID": "lecs/w01/lec01.html#mining-operations",
    "href": "lecs/w01/lec01.html#mining-operations",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Mining Operations",
    "text": "Mining Operations"
  },
  {
    "objectID": "lecs/w01/lec01.html#oil-spill-containment",
    "href": "lecs/w01/lec01.html#oil-spill-containment",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Oil Spill Containment",
    "text": "Oil Spill Containment\n\nBP Deepwater Horizon Spill, Gulf of Mexico, 2010"
  },
  {
    "objectID": "lecs/w01/lec01.html#autonomy-vs.-remote-control",
    "href": "lecs/w01/lec01.html#autonomy-vs.-remote-control",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Autonomy vs. Remote Control",
    "text": "Autonomy vs. Remote Control\n\nQ: When is full or partial autonomy necessary?\nQ: When is remote control preferred?"
  },
  {
    "objectID": "lecs/w01/lec01.html#todays-agenda-1",
    "href": "lecs/w01/lec01.html#todays-agenda-1",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\n\nIntroduction\n\n\n\nAdministrivia\n\nOffice hours\nTutorials\nAssignment descriptions\nPrerequisites\n\nTopics covered by the course\nSensors and Actuators\nQuiz about background and interests"
  },
  {
    "objectID": "lecs/w01/lec01.html#prerequisites",
    "href": "lecs/w01/lec01.html#prerequisites",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Prerequisites",
    "text": "Prerequisites\n\n\nSoftware Engineering\n\nLoops, conditionals, classes, modularity\nLists, hash maps/dictionaries, trees\nThreads, callbacks, remote procedure calls, serialization\n\nLinear Algebra\n\nMatrix multiplication and inversion, determinant\nSolving systems of equations, Gaussian elimination\nMatrix decompositions: Cholesky, QR\nLeast squares\n\nBasic Probability Theory\n\nMultivariate distributions, especially Gaussians\nConditional probability, Bayes’ rule\nMaximum likelihood estimation"
  },
  {
    "objectID": "lecs/w01/lec01.html#prerequisites-1",
    "href": "lecs/w01/lec01.html#prerequisites-1",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Prerequisites",
    "text": "Prerequisites\nCurrently\nRequired: CSC209H5; STA256H5; MAT223H5/MAT240H5; MAT232H5; CSC376\nRecommended: MAT224H5; CSC384H5; CSC311H5;"
  },
  {
    "objectID": "lecs/w01/lec01.html#assignments",
    "href": "lecs/w01/lec01.html#assignments",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "4 Assignments",
    "text": "4 Assignments\n\n~80% coding and the rest theory\nStarter code will be provided\nBonus questions will be provided\nAccepted languages: Python, C++\nYou’re going to learn ROS (Robot Operating System) and use the Gazebo simulator\nYou’re also going to learn numpy and scipy\nAbout 2 weeks to work on each"
  },
  {
    "objectID": "lecs/w01/lec01.html#ros-gazebo-simulation",
    "href": "lecs/w01/lec01.html#ros-gazebo-simulation",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "ROS + Gazebo simulation",
    "text": "ROS + Gazebo simulation"
  },
  {
    "objectID": "lecs/w01/lec01.html#quizzes",
    "href": "lecs/w01/lec01.html#quizzes",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "7 Quizzes",
    "text": "7 Quizzes\n\n5-10 mins to complete them\nNot cumulative in terms of material. They cover only one lecture\nMeant to check whether you have understood basic concepts"
  },
  {
    "objectID": "lecs/w01/lec01.html#evaluation",
    "href": "lecs/w01/lec01.html#evaluation",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Evaluation",
    "text": "Evaluation\nCSC477\n\n4 assignments, 15% each = 60%\n7 quizzes, 2% each = 14%\n1 final exam = 26%\n\n\nCSC2630\n\n3 assignments, 15% each = 45%\n7 quizzes, 2% each = 14%\n1 final project = 41%"
  },
  {
    "objectID": "lecs/w01/lec01.html#recommended-textbooks-optional",
    "href": "lecs/w01/lec01.html#recommended-textbooks-optional",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Recommended Textbooks (optional)",
    "text": "Recommended Textbooks (optional)"
  },
  {
    "objectID": "lecs/w01/lec01.html#recommended-online-courses-optional",
    "href": "lecs/w01/lec01.html#recommended-online-courses-optional",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Recommended Online Courses (optional)",
    "text": "Recommended Online Courses (optional)\n\nMaterial is related to 477 but not identical\nI will post links on Quercus to specific lectures that are relevant\n\n\f\n\nhttps://www.udacity.com/course/artificial-intelligence-for-robotics--cs373\nhttps://www.edx.org/course/autonomous-mobile-robots-ethx-amrx-1\nhttps://underactuated.mit.edu/ (more advanced, little overlap with 477)"
  },
  {
    "objectID": "lecs/w01/lec01.html#office-hours-zoom",
    "href": "lecs/w01/lec01.html#office-hours-zoom",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Office Hours (Zoom)",
    "text": "Office Hours (Zoom)\n\nFlorian: Thursdays 3-4pm\nYewon: Tuesdays 11-12pm\nYasasa: Fridays 11-12pm\n\n\f\n\nOffice hours will begin next week"
  },
  {
    "objectID": "lecs/w01/lec01.html#online-communication",
    "href": "lecs/w01/lec01.html#online-communication",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Online communication",
    "text": "Online communication\n\nUse Quercus\nPlease check your course-related email frequently\nEmail us at csc477-instructor@cs.toronto.edu and csc477-tas@cs.toronto.edu\nAnonymous feedback about anything course-related: https://www.surveymonkey.com/r/H8QH65F"
  },
  {
    "objectID": "lecs/w01/lec01.html#todays-agenda-2",
    "href": "lecs/w01/lec01.html#todays-agenda-2",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\n\nIntroduction\nAdministrivia\n\nOffice hours\nTutorials\nAssignment descriptions\nPrerequisites\n\n\n\n\nTopics covered by the course\nSensors and Actuators\nQuiz about background and interests"
  },
  {
    "objectID": "lecs/w01/lec01.html#main-topics-to-be-covered",
    "href": "lecs/w01/lec01.html#main-topics-to-be-covered",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Main topics to be covered",
    "text": "Main topics to be covered"
  },
  {
    "objectID": "lecs/w01/lec01.html#covered-qquad-qquad-qquad-not-covered",
    "href": "lecs/w01/lec01.html#covered-qquad-qquad-qquad-not-covered",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Covered \\(\\qquad \\qquad \\qquad\\) Not Covered",
    "text": "Covered \\(\\qquad \\qquad \\qquad\\) Not Covered"
  },
  {
    "objectID": "lecs/w01/lec01.html#main-topics-to-be-covered-1",
    "href": "lecs/w01/lec01.html#main-topics-to-be-covered-1",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Main topics to be covered",
    "text": "Main topics to be covered\n\n\n\n\n\nMain question: what is the next state given the curent state and controls?"
  },
  {
    "objectID": "lecs/w01/lec01.html#main-topics-to-be-covered-2",
    "href": "lecs/w01/lec01.html#main-topics-to-be-covered-2",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Main topics to be covered",
    "text": "Main topics to be covered\n\n\n\n\n\nMain question: what are the controls that will take the system from state A to B?"
  },
  {
    "objectID": "lecs/w01/lec01.html#not-covered-in-csc477-but-related-learning-for-control",
    "href": "lecs/w01/lec01.html#not-covered-in-csc477-but-related-learning-for-control",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Not covered in CSC477, but related: learning for control",
    "text": "Not covered in CSC477, but related: learning for control"
  },
  {
    "objectID": "lecs/w01/lec01.html#not-covered-in-csc477-but-related-learning-for-control-1",
    "href": "lecs/w01/lec01.html#not-covered-in-csc477-but-related-learning-for-control-1",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Not covered in CSC477, but related: learning for control",
    "text": "Not covered in CSC477, but related: learning for control"
  },
  {
    "objectID": "lecs/w01/lec01.html#main-topics-to-be-covered-3",
    "href": "lecs/w01/lec01.html#main-topics-to-be-covered-3",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Main topics to be covered",
    "text": "Main topics to be covered"
  },
  {
    "objectID": "lecs/w01/lec01.html#main-topics-to-be-covered-4",
    "href": "lecs/w01/lec01.html#main-topics-to-be-covered-4",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Main topics to be covered",
    "text": "Main topics to be covered\n\nKnown: robot’s position and orientation\nWant to estimate: a map of the environment from laser measurements\n\n\n\n\n\n\n\n\nOccupancy grid mapping"
  },
  {
    "objectID": "lecs/w01/lec01.html#main-topics-to-be-covered-5",
    "href": "lecs/w01/lec01.html#main-topics-to-be-covered-5",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Main topics to be covered",
    "text": "Main topics to be covered"
  },
  {
    "objectID": "lecs/w01/lec01.html#orb-slam-video",
    "href": "lecs/w01/lec01.html#orb-slam-video",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "orb slam video",
    "text": "orb slam video\nhttps://www.youtube.com/watch?v=_9VcvGybsDA"
  },
  {
    "objectID": "lecs/w01/lec01.html#lecture-topics-qquad-qquad-tutorials",
    "href": "lecs/w01/lec01.html#lecture-topics-qquad-qquad-tutorials",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Lecture topics \\(\\qquad \\qquad\\) Tutorials",
    "text": "Lecture topics \\(\\qquad \\qquad\\) Tutorials\n\n\n\n\n\n\nIntro to the Robot Operating System (ROS)\nRefresher on linear algebra and least squares\nRefresher on basic probability and continuous distributions\nHow to align 3D pointclouds. Demo of the PCL library\nHow to implement a Kalman Filter\nHow to implement a Particle Filter\nHow to approximate functions"
  },
  {
    "objectID": "lecs/w01/lec01.html#assignments-1",
    "href": "lecs/w01/lec01.html#assignments-1",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Assignments",
    "text": "Assignments\n\n\n\n\n\n\n\nA1: Designing a feedback controller for wall-following"
  },
  {
    "objectID": "lecs/w01/lec01.html#assignments-2",
    "href": "lecs/w01/lec01.html#assignments-2",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Assignments",
    "text": "Assignments\n\n\n\n\n\n\n\n\nA1: Designing a feedback controller for wall-following\n\nA2: Implementing path-planning and feedback control algorithms"
  },
  {
    "objectID": "lecs/w01/lec01.html#assignments-3",
    "href": "lecs/w01/lec01.html#assignments-3",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Assignments",
    "text": "Assignments\n\n\n\n\n\n   \n\n\nA1: Designing a feedback controller for wall-following\n\nA2: Implementing path-planning and feedback control algorithms\n\nA3: Occupancy grid mapping with known robot location\nA4: Localization in a known map using particle filters"
  },
  {
    "objectID": "lecs/w01/lec01.html#todays-agenda-3",
    "href": "lecs/w01/lec01.html#todays-agenda-3",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\n\nIntroduction\nAdministrivia\n\nOffice hours\nTutorials\nAssignment descriptions\nPrerequisites\n\nTopics covered by the course\n\n\n\nSensors and Actuators\nQuiz about background and interests"
  },
  {
    "objectID": "lecs/w01/lec01.html#sensors-and-actuators",
    "href": "lecs/w01/lec01.html#sensors-and-actuators",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Sensors and Actuators",
    "text": "Sensors and Actuators\n\nSensors:\n\nCharacteristics and types\nMeasurement noise\nRequired bandwidth\n\n\n\f\n\nActuators:\n\nTypes of motors\nPulse-Width Modulation"
  },
  {
    "objectID": "lecs/w01/lec01.html#sensors",
    "href": "lecs/w01/lec01.html#sensors",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Sensors",
    "text": "Sensors\n\nDevices that can sense and measure physical properties of the environment.\nKey phenomenon is transduction (conversion of energy from one form to another). E.g.:\n\nImaging sensors: light to pixel voltages\nDepth sensors: mechanical pressure to voltage\n\nMeasurements are noisy, and difficult to interpret"
  },
  {
    "objectID": "lecs/w01/lec01.html#sensors-general-characteristics",
    "href": "lecs/w01/lec01.html#sensors-general-characteristics",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Sensors: general characteristics",
    "text": "Sensors: general characteristics\n\nSensitivity: (change of output) ÷ (change of input)\nLinearity: constancy of (output ÷ input)\nMeasurement range: [min, max] or {min, max}\nResponse time: time required for input change to cause output change\nAccuracy: difference between measurement and actual\nRepeatability/Drift: difference between repeated measures\nResolution: smallest observable increment\nBandwidth: required rate of data transfer\nSNR: signal-to-noise ratio"
  },
  {
    "objectID": "lecs/w01/lec01.html#sensors-vision",
    "href": "lecs/w01/lec01.html#sensors-vision",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Sensors: vision",
    "text": "Sensors: vision\n\n\n\n\n\nCCD image sensor\n\n\n\nCCD (charge-coupled device) imaging sensors:\n\nCapacitor array accumulates electric charge proportional to light intensity.\nEach capacitor’s charge is transferred to its neighbor.\nLast capacitor’s charge gets amplified and output as voltage.\n(+) High-quality, low-noise images\n(-) Higher power consumption\n(-) Slow readout\n(-) Specialized fabrication\n\nvoltage → analog-to-digital converter → pixel value in {0, 255}\nCMOS (complementary metal-oxide semi-conductor) imaging sensors:\n\nOne amplifier per pixel\n(+) Low power\n(+) Fast readout\n(+) Easier to fabricate\n(-) Poor low-light sensitivity\n(-) Higher noise"
  },
  {
    "objectID": "lecs/w01/lec01.html#global-vs.-rolling-shutter",
    "href": "lecs/w01/lec01.html#global-vs.-rolling-shutter",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Global vs. Rolling Shutter",
    "text": "Global vs. Rolling Shutter\nShutter = mechanism that allows light to hit the imaging sensor\nShutter “speed” = Exposure time = time duration in which the sensor is exposed to light\n\n\n\n\n\n\n\nRolling shutter"
  },
  {
    "objectID": "lecs/w01/lec01.html#reading-rgb-images-from-a-camera",
    "href": "lecs/w01/lec01.html#reading-rgb-images-from-a-camera",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Reading RGB images from a camera",
    "text": "Reading RGB images from a camera\n\n\n\nEach pixel contains an intensity value from 0…255\n\n\n\n\n\n\n\n\n\n\n\n\n600 x 1000 pixels\n\n\n\n\n\n\n\n600 x 1000 pixels\n\n\n\n\n\n\n\n600 x 1000 pixels"
  },
  {
    "objectID": "lecs/w01/lec01.html#reading-rgb-images-from-a-camera-1",
    "href": "lecs/w01/lec01.html#reading-rgb-images-from-a-camera-1",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Reading RGB images from a camera",
    "text": "Reading RGB images from a camera\n\n\n\nEach pixel contains an intensity value from 0…255\n\n\n\n\n\n\\(\\to\\) A matrix of 600 x 1000 x 3 = ~ 1.8 million numbers\n\n\n\n\n\n\n\n\n\n600 x 1000 pixels\n\n\n\n\n\n\n\n600 x 1000 pixels\n\n\n\n\n\n\n\n600 x 1000 pixels"
  },
  {
    "objectID": "lecs/w01/lec01.html#computerrobot-vision",
    "href": "lecs/w01/lec01.html#computerrobot-vision",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Computer/robot vision",
    "text": "Computer/robot vision\n\n\n\n\n\nStructured numbers\n\n\n\n\n\n\nI’m seeing a parrot\nI’m seeing a toy bicycle\nThe parrot is riding the bicycle\nThe bicycle is on top of a desk\nIs this physically plausible?\nWhere is the parrot in 3D w.r.t. the camera?\nWhere will the parrot go next?\nWhat is the speed of the parrot?\n\nConclusions/Inference/Deduction/Estimation"
  },
  {
    "objectID": "lecs/w01/lec01.html#camera-lenses",
    "href": "lecs/w01/lec01.html#camera-lenses",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Camera lenses",
    "text": "Camera lenses\n\n\n\nLens determines:\n\nimage distortion\nfocus\nsharpness or blur\n\nLens characteristics:\n\nfocal length\naperture\ndepth-of-field"
  },
  {
    "objectID": "lecs/w01/lec01.html#pinhole-camera-model",
    "href": "lecs/w01/lec01.html#pinhole-camera-model",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Pinhole Camera Model",
    "text": "Pinhole Camera Model\n\n\nWe know approximately how a 3D point (X,Y,Z) projects to pixel (x,y)\nWe call this the pinhole projection model"
  },
  {
    "objectID": "lecs/w01/lec01.html#perspective-projection-xy-𝜋xyz",
    "href": "lecs/w01/lec01.html#perspective-projection-xy-𝜋xyz",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "(1) Perspective projection [x,y] = 𝜋(X,Y,Z)",
    "text": "(1) Perspective projection [x,y] = 𝜋(X,Y,Z)\n\n\n\nhttp://www.cim.mcgill.ca/%7Elanger/558.html\n\nBy similar triangles: x/f = X/Z\nSo, x = f * X/Z and similarly y = f * Y/Z\nProblem: we just lost depth (Z) information by doing this projection, i.e. depth is now uncertain."
  },
  {
    "objectID": "lecs/w01/lec01.html#lens-distortion",
    "href": "lecs/w01/lec01.html#lens-distortion",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "(2) Lens distortion",
    "text": "(2) Lens distortion"
  },
  {
    "objectID": "lecs/w01/lec01.html#estimating-parameters-of-lens-distortion",
    "href": "lecs/w01/lec01.html#estimating-parameters-of-lens-distortion",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "(2) Estimating parameters of lens distortion",
    "text": "(2) Estimating parameters of lens distortion"
  },
  {
    "objectID": "lecs/w01/lec01.html#non-pinhole-cameras-thin-lens-model",
    "href": "lecs/w01/lec01.html#non-pinhole-cameras-thin-lens-model",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Non-pinhole cameras: thin lens model",
    "text": "Non-pinhole cameras: thin lens model\n\n\n\n\n\n\nUnlike the pinhole camera, this is able to model blur.\n\n\n\n\n\n\n\nhttp://www.cim.mcgill.ca/%7Elanger/558.html"
  },
  {
    "objectID": "lecs/w01/lec01.html#beyond-the-visible-spectrum-infrared-cameras",
    "href": "lecs/w01/lec01.html#beyond-the-visible-spectrum-infrared-cameras",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Beyond the visible spectrum: infrared cameras",
    "text": "Beyond the visible spectrum: infrared cameras"
  },
  {
    "objectID": "lecs/w01/lec01.html#beyond-the-visible-spectrum-infrared-cameras-1",
    "href": "lecs/w01/lec01.html#beyond-the-visible-spectrum-infrared-cameras-1",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Beyond the visible spectrum: infrared cameras",
    "text": "Beyond the visible spectrum: infrared cameras\n\nDrawback: Doesn’t work underwater"
  },
  {
    "objectID": "lecs/w01/lec01.html#beyond-the-visible-spectrum-infrared-cameras-2",
    "href": "lecs/w01/lec01.html#beyond-the-visible-spectrum-infrared-cameras-2",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Beyond the visible spectrum: infrared cameras",
    "text": "Beyond the visible spectrum: infrared cameras"
  },
  {
    "objectID": "lecs/w01/lec01.html#beyond-the-visible-spectrum-rgbd-cameras",
    "href": "lecs/w01/lec01.html#beyond-the-visible-spectrum-rgbd-cameras",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Beyond the visible spectrum: RGBD cameras",
    "text": "Beyond the visible spectrum: RGBD cameras\n\n\n\n\nMain ideas:\n\nActive sensing\nProjector emits infrared light in the scene\nInfrared sensor reads the infrared light\nDeformation of the expected pattern allows computation of the depth"
  },
  {
    "objectID": "lecs/w01/lec01.html#beyond-the-visible-spectrum-rgbd-cameras-1",
    "href": "lecs/w01/lec01.html#beyond-the-visible-spectrum-rgbd-cameras-1",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Beyond the visible spectrum: RGBD cameras",
    "text": "Beyond the visible spectrum: RGBD cameras\n\n\nDrawbacks:\n\nDoes not work well outdoors, sunlight saturates its measurements\nMaximum range is [0.5, 8] meters\n\nAdvantages:\n\nReal-time depth estimation at 30Hz\nCheap"
  },
  {
    "objectID": "lecs/w01/lec01.html#beyond-the-visible-spectrum-rgbd-cameras-2",
    "href": "lecs/w01/lec01.html#beyond-the-visible-spectrum-rgbd-cameras-2",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Beyond the visible spectrum: RGBD cameras",
    "text": "Beyond the visible spectrum: RGBD cameras\n\n\nEnabled a wave of research, applications, and video games, based on real-time skeleton tracking"
  },
  {
    "objectID": "lecs/w01/lec01.html#beyond-the-visible-spectrum-rgbd-cameras-3",
    "href": "lecs/w01/lec01.html#beyond-the-visible-spectrum-rgbd-cameras-3",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Beyond the visible spectrum: RGBD cameras",
    "text": "Beyond the visible spectrum: RGBD cameras\n\n\nDespite their drawbacks RGBD sensors have been extensively used in robotics."
  },
  {
    "objectID": "lecs/w01/lec01.html#d-lidar-light-detection-and-ranging",
    "href": "lecs/w01/lec01.html#d-lidar-light-detection-and-ranging",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "3D LIDAR (Light detection and ranging)",
    "text": "3D LIDAR (Light detection and ranging)\n\n\nProduces a pointcloud of 3D points and intensities\n\n(x,y,z) in the laser’s frame of reference\nIntensity is related to the material of the object that reflects the light\n\nWorks based on time-of-flight for each beam to return back to the scanner\n\nNot very robust to adverse weather conditions: rain, snow, smoke, fog etc.\nUsed in most self-driving cars today for obstacle detection. Range &lt; 100m.\n\n\n\n\nUsually around 1million points in a single pointcloud"
  },
  {
    "objectID": "lecs/w01/lec01.html#d-lidar-light-detection-and-ranging-1",
    "href": "lecs/w01/lec01.html#d-lidar-light-detection-and-ranging-1",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "2D LIDAR (Light detection and ranging)",
    "text": "2D LIDAR (Light detection and ranging)\n\n\nProduces a scan of 2D points and intensities\n\n(x,y) in the laser’s frame of reference\nIntensity is related to the material of the object that reflects the light\n\nCertain surfaces are problematic for LIDAR: e.g. glass\n\n\nLots of moving parts: motors quickly rotate the laser beam and once complete (angle bound reached) a scan is returned. I.e. points are not strictly speaking time-synchronized, even though we usually treat them as such.\n\n\n\n\nUsually around 1024 points in a single scan."
  },
  {
    "objectID": "lecs/w01/lec01.html#inertial-sensors",
    "href": "lecs/w01/lec01.html#inertial-sensors",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Inertial Sensors",
    "text": "Inertial Sensors\n\nGyroscopes, Accelerometers, Magnetometers\nInertial Measurement Unit (IMU)\nPerhaps the most important sensor for 3D navigation, along with the GPS\nWithout IMUs, plane autopilots would be much harder, if not impossible, to build"
  },
  {
    "objectID": "lecs/w01/lec01.html#gyroscopes",
    "href": "lecs/w01/lec01.html#gyroscopes",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Gyroscopes",
    "text": "Gyroscopes\n\nMeasure angular velocity in the body frame\nOften affected by noise and bias\n\n\\[\nw_\\text{measured}(t) = w_\\text{true}(t) + b_g(t) + n_g(t)\n\\]\n\nWe integrate it to get 3D orientation (Euler angles, quaternions rotation matrices), but there is drift due to noise and bias"
  },
  {
    "objectID": "lecs/w01/lec01.html#accelerometers",
    "href": "lecs/w01/lec01.html#accelerometers",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Accelerometers",
    "text": "Accelerometers\n\nMeasure linear acceleration relative to freefall (measured in g)\nA free-falling accelerometer in a vacuum would measure zero g\nAn accelerometer resting on the surface of the earth would measure 1g\nAlso affected by bias and noise.\nDouble integration to get position is very noisy. Errors grow quadratically with time."
  },
  {
    "objectID": "lecs/w01/lec01.html#magnetometers",
    "href": "lecs/w01/lec01.html#magnetometers",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Magnetometers",
    "text": "Magnetometers\n\n\nDrawbacks:\n\nNeeds careful calibration\nNeeds to be placed away from moving metal parts, motors\n\nAdvantages:\n\nCan be used as a compass for absolute heading"
  },
  {
    "objectID": "lecs/w01/lec01.html#inertial-measurement-unit",
    "href": "lecs/w01/lec01.html#inertial-measurement-unit",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Inertial Measurement Unit",
    "text": "Inertial Measurement Unit\n\nCombines measurements from accelerometer, gyroscope, and magnetometer to output an estimate of orientation with reduced drift.\nDoes not typically provide a position estimate, due to double integration.\nRuns at 100-1000Hz\nExpect yaw drift of 5-10 deg/hour on most modern low-end IMUs"
  },
  {
    "objectID": "lecs/w01/lec01.html#global-positioning-system-satellites",
    "href": "lecs/w01/lec01.html#global-positioning-system-satellites",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Global Positioning System: Satellites",
    "text": "Global Positioning System: Satellites\n\nEach GPS satellite periodically transmits:\n[Coarse/Acquisition code] A 1023-bit pseudorandom binary sequence (PRN code), which repeats every 1 ms, unique for each satellite (no correlation with other satellites).\n[Navigation frame] A 1500-bit packet that contains\n\nGPS date, time, satellite health\nDetailed orbital data for the satellite, accurate for the next ~4hrs\nPRN codes and status of all satellites in the network\nTakes 12.5mins to transmit\n\n[Precision code] A 6.2-terabit code for military use.\nCarrier frequencies are 1575.42 MHz (L1) and 1227.60 MHz (L2)"
  },
  {
    "objectID": "lecs/w01/lec01.html#global-positioning-system-receivers",
    "href": "lecs/w01/lec01.html#global-positioning-system-receivers",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Global Positioning System: Receivers",
    "text": "Global Positioning System: Receivers\n\nEach (civilian) GPS receiver:\n\nKnows the PRN codes for each satellite in advance\nCorrelates received PRN signal with database PRN signal → time shift → noisy distance to satellite\nIf 4 or more satellite PRN codes are received, it does trilateration to compute latitude and longitude"
  },
  {
    "objectID": "lecs/w01/lec01.html#global-positioning-system-receivers-and-dilution-of-precision",
    "href": "lecs/w01/lec01.html#global-positioning-system-receivers-and-dilution-of-precision",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Global Positioning System: Receivers and Dilution of Precision",
    "text": "Global Positioning System: Receivers and Dilution of Precision"
  },
  {
    "objectID": "lecs/w01/lec01.html#hall-effect-sensor",
    "href": "lecs/w01/lec01.html#hall-effect-sensor",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Hall Effect Sensor",
    "text": "Hall Effect Sensor\n\n\n\nVaries its voltage in response to a magnetic field\nUsed as a proximity switch, to measure a full rotation of a wheel for example\nUsed to measure rate of rotation of wheels"
  },
  {
    "objectID": "lecs/w01/lec01.html#rotary-encoder",
    "href": "lecs/w01/lec01.html#rotary-encoder",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Rotary Encoder",
    "text": "Rotary Encoder\n\nContains an analog to digital converter for encoding the angle of a shaft/motor/axle\nUsually outputs the discretized absolute angle of the shaft/motor/axle\n\n\n\nUseful in order to know where different shafts are relative to each to each other."
  },
  {
    "objectID": "lecs/w01/lec01.html#example-flippers-on-the-aqua-robot",
    "href": "lecs/w01/lec01.html#example-flippers-on-the-aqua-robot",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Example: flippers on the Aqua robot",
    "text": "Example: flippers on the Aqua robot"
  },
  {
    "objectID": "lecs/w01/lec01.html#actuators",
    "href": "lecs/w01/lec01.html#actuators",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Actuators",
    "text": "Actuators\n\n\n\n\n\n\nDC (direct current) motor\n\n\n\n\n\n\n\nServo motor\n\n\n\n\n\n\n\nStepper motor\n\n\n\n\n\n\nThey turn continuously at high RPM (revolutions per minute) when voltage is applied. Used in quadrotors and planes, model cars etc.\n\n\nUsually includes: DC motor, gears, control circuit, position feedback\nPrecise control without free rotation (e.g. robot arms, boat rudders) Limited turning range: 180 degrees\n\n\nPositioning feedback and no positioning errors.\nRotates by a predefined step angle.\nRequires external control circuit.\nPrecise control without free rotation.\nConstant holding torque without powering the motor (good for robot arms or weight-carrying systems)."
  },
  {
    "objectID": "lecs/w01/lec01.html#pulse-width-modulation",
    "href": "lecs/w01/lec01.html#pulse-width-modulation",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Pulse Width Modulation",
    "text": "Pulse Width Modulation\n\n\nUsed for creating analog/continuous behavior when voltage applied is discrete.\nMain idea: turn on and off the motor fast enough so average voltage is the desired target.\nUsed in dimming LEDs, controlling the speed of DC motors, controlling the position of servo motors."
  },
  {
    "objectID": "lecs/w01/lec01.html#todays-agenda-4",
    "href": "lecs/w01/lec01.html#todays-agenda-4",
    "title": "CSC477 / CSC2630 Introduction to Mobile Robotics",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\n\nIntroduction\nAdministrivia\n\nOffice hours\nTutorials\nAssignment descriptions\nPrerequisites\n\nTopics covered by the course\nSensors and Actuators\n\n\n\nQuiz about background and interests"
  },
  {
    "objectID": "computing-troubleshooting.html",
    "href": "computing-troubleshooting.html",
    "title": "Computing troubleshooting",
    "section": "",
    "text": "If you’re having difficulty launching an RStudio session from your reserved container, go to status.oit.duke.edu and scroll down to Teaching and Learning Tools. Under this heading you’ll find an entry called Container Manager (CMGR Coursework Containers).\n\nIf the status shows something other than Operational, this means there is a known incident with the containers. Check back later to see if it’s been resolved. If there’s a deadline coming up soon, post on the course forum to let us know that there’s an issue. We can look into how quickly it might get resolved and decide on what to do about the deadline accordingly. Note: We don’t anticipate this to happen regularly, the systems are Operational a huge majority of the time!\nIf the status shows Operational, this means the system is expected to be working. Check your internet connection, if need be, restart your computer to ensure a fresh new connection. If your issue persists, post on the course forum with details on what you’ve tried and the errors you see (including verbatim errors and/or screenshots).\nEither way you can also fill out the form here, which will notify our the R TA for the department as well as our undergraduate coordinator. They’ll be able to help diagnose the issue."
  },
  {
    "objectID": "project-description.html",
    "href": "project-description.html",
    "title": "Final Project Guidelines",
    "section": "",
    "text": "The project in this course is an opportunity to develop deep learning application in an area of your own choosing. It also provides the chance to complete a deep learning project that is much closer to a real-world application area, for example in medicine, finance, robotics, commerce, biology, chemistry, physics (or other sciences), social media, or other fields.\nWhile this project has some structure, you will be required to deal with the ambiguity and significant decision making that make up the life of a deep learning practitioner."
  },
  {
    "objectID": "project-description.html#project-proposal",
    "href": "project-description.html#project-proposal",
    "title": "Final Project Guidelines",
    "section": "Project Proposal",
    "text": "Project Proposal\n\nLogistics\nProjects must be done in groups of 3-4. Please form groups on Markus by March 17, 10pm. Exceptions to this rule can be made only in rare cases provided there is good reason to do so. Email the instructors if this applies to you. If you do not know anyone in class feel free to post a message on Piazza. We will also set aside some time during the tutorial for students who are looking for collaborators to find each other and discuss forming a group.\nA 1-2 page project proposal is due March 21, 10pm. You will also be asked to summarize the data set that you are using for this proposal.\nEach team will submit a github repository page that describes the deep learning model built in the project. The repository should also contain the code that you wrote.\n\n\nProject Requirements\nBy default, your project must either take a sequence (of variable length) as an input, or produce a sequence as an output, or both. If you have a project proposal that does not involve sequences, please contact the instructors.\nYour model should thus involve an RNN or a Transformer component. Students who want to use methods that we have not covered in the course (e.g. diffusion models, neural ODEs) are free to do so, as long as they confirm their methodology with the instructors before they submit this project proposal. There is also flexibility for students to pursue an open research problem. If any groups want to attempt this, they need to discuss this with one of the instructors before the prject proposal deadline.\nHere are some examples of possible projects:\n\nUsing an RNN (or transformer) to classify sequences (e.g. whether a restaurant review is positive or negative)\nUsing a generative RNN to produce sequences (e.g. South Park TV scripts)\nUsing a Siamese network to determine whether two StackOverflow questions are duplicates\nPredict the next item in a sequence (e.g. Stock market)\nPredict the outcome of a patient based on some sequential factors\nPredict the dynamics of objects under contact and collision (e.g. robotics and graphics)\nGenerate molecules, or predict properties of molecules\n\nBefore choosing a project, consider whether there is data available for you. Since the project deadline is about a month away, consider tailoring your project ideas to what data is available to you.\nYou are encouraged to use transfer learning and data augmentation ideas in your project.\nYou can use deep learning packages (e.g. pytorch, huggingface). However, you should be able to explain the steps involved in the forward pass computation of your model.\n\n\nProject Proposal\nA 1-2 page project proposal is due March 21, 10pm. Please use 12-point font and standard margins. You will also be asked to summarize the data set that you are using for this proposal.\nThe proposal should:\n\nClearly describe the task that your model will perform. (2pt)\n\n2/2 for clearly describing the task using standard deep learning terminology\n1.5/2 for describing the task in a way that is understandable to the grader, but that uses non-standard terminology\n1/2 for describing the task generally (e.g. “sequence classification” without stating the exact classes)\n0/2 for a proposal that does not align with the project requirements\n\nClearly describe the model that you intend to use (2pt)\n\n2/2 for clearly describing the model using standard deep learning terminology; the grader can picture exactly how the model could be used.\n1.5/2 for describing the task in a way that is understandable to the grader, but that uses non-standard terminology\n1/2 for describing the models generally (e.g. sequence-to-sequence model, without describing which ones)\n0/2 for a model that does not align with the project requirements\n\nOutline the data set that you intend to use, and provide some statistics about the amount/type of data that is available (4pt)\n\n1 point for convincing the grader that you are able to acquire the data that you need (with the appropriate license/permission for educational use)\n1 point for convincing the grader that the type and amount of data is sufficient (e.g. via summary statistics, examples data set)\n2 points for convincing the grader that you have explored the data, and considered information about your data relevant to your model (like in A1 Q1)\n\nDiscuss any ethical implications of your model—how might the use (or misuse) of this model help or hurt people? (2pt)\n\n2/2 For a thoughtful discussion that considers the ethical implications across many groups of people (that different groups may be impacted differently).\n1/2 For a discussion that is generic, or considers the ethical implications for only one group of people.\n\nDescribe how work will be divided amongst the team members. We recommend pair-coding for parts of the project, but consider the work that it might take to load/format your data, write a first model, “overfit” to a single data point, etc… (2pt)\n\n2/2 The description provides enough detail so that if a team member is replaced, they know exactly what their responsibilities will be.\n1/2 There is clearly an attempt to describe the division of tasks, but the communication is unclear and/or only the tasks listed above are assigned.\n0/2 Only vague assertions are made (e.g. “we will divide the work equally”, “everyone will work on everything”, or “we will determine who will work on what as the project progresses).\n\nProper formatting (2pt)\n\n2/2 Proposal is 1-2 pages. The proposal is formatted so that readers can find specific information quickly (e.g. via the use of paragraphs and topic sentences)\n1/2 Proposal is slightly over the length limit. There was clearly an attempt to format the proposal, but information is still scattered in various places.\n0/2 Proposal runs extremely long. It is difficult to understand the structure of the proposal."
  },
  {
    "objectID": "project-description.html#final-project",
    "href": "project-description.html#final-project",
    "title": "Final Project Guidelines",
    "section": "Final Project",
    "text": "Final Project\n\nSubmission\nPlease submit a file called github.txt containing a link to the github repository. If your repository will be private, please email the instructors by April 7, 10pm so that TAs and instructors can be added—even if you use tokens.\n\n\nRepository Content\nThe repository should contain:\n\nThe code you used to pre-process the data, but not the data itself. It is generally a bad idea to include data in your github repository, since git is great for lots of small files, but a poor choice for sharing large files. Moreover, most groups are using data collected by other people. While you should share the source of your data, you should generally not share a copy of the data.\nThe code you used to train your model. You may opt to share model weights, or not.\nA README file with the following component:\n\n\nIntroduction that states the deep learning model that you are building\nModel:\n\nA figure/diagram of the model architecture that demonstrates understanding of the steps involved in computing the forward pass\nCount the number of parameters in the model, and a description of where the parameters come from\nExamples of how the model performs on two actual examples from the test set: one successful and one unsuccessful\n\nData:\n\nDescribe the source of your data\nProvide summary statistics of your data to help interpret your results (similar to in the proposal)\nDescribe how you transformed the data (e.g. any data augmentation techniques)\nIf appropriate to your project, describe how the train/validation/test set was split. (Note that splitting the training/validation/test set is not always straightforward!)\n\nTraining:\n\nThe training curve of your final model\nA description how you tuned hyper-parameters\n\nResults:\n\nDescribe the quantitative measure that you are using to evaluate your result\nDescribe the quantitative and qualitative results\nA justification that your implemented method performed reasonably, given the difficulty of the problem—or a hypothesis for why it doesn’t (this is extremely important)\n\nEthical Consideration:\n\nDescription of a use of the system that could give rise to ethical issues. Are there limitations of your model? Your training data?\n\nAuthors\n\nA description of how the work was split—i.e. who did what in this project.\n\n\n\n\nMarking Scheme\nHere is the marking scheme that we will use. Note that you model must be able to make reasonable predictions for your project to receive a passing project grade. In particular, without a reasonable model, you won’t be able to earn credit for Model Examples, Training Curve, Hyperparameter Tuning, Qualitative/Quantitative Results, etc.\nREADME/Writeup (70 points)\n\nIntroduction (4 points): What deep learning model are you building? We are looking for a clear and concise description that uses standard deep learning terminology. Clearly describe the type of task that you are solving, and what your input/outputs are.\nModel Figure (4 points): A figure/diagram of the model architecture that demonstrates understanding of the steps involved in computing the forward pass. We are looking to see if you understand the steps involved in the model computation (i.e. are you treating the model as a black box or do you understand what it’s doing?)\nModel Parameters(4 points): Count the number of parameters in the model, and a description of where the parameters come from. Again, we are looking to see if you understand what the model is doing, and what parameters are being tuned.\nModel Examples (4 points): Examples of how the model performs on two actual examples from the test set: one successful and one unsuccessful.\nData Source (1 point): Describe the source of your data.\nData Summary (4 points): Provide summary statistics of your data to help interpret your results, similar to in the proposal. Please review the feedback provided in the proposal for some guidance on what information is helpful for interpreting your model behaviour.\nData Transformation (3 points): Describe how you transformed the data, i.e. the steps you took to turn the data from what you downloaded, to something that a neural network can use as input. We are looking for a concise description that has just enough information for another person to replicate your process.\nData Split (2 points): If appropriate to your project, describe how the train/validation/test set was split. Note that splitting strategy is not always straightforward, so we are looking to see a split that can be justified.\nTraining Curve (4 points): The training curve of your final model. We are looking for a curve that shows both training and validation performance (if applicable). Your training curve should look reasonable for the problem that you are solving.\nHyperparamter Tuning (4 points): A description how you tuned hyper-parameters. We are looking for hyperparameter choices that makes sense.\nQuantitative Measures (2 points): A description and justification of the quantitative measure that you are using to evaluate your results. For some problems this will be straightforward. For others, please justify the measure that you chose.\nQuantitative and Qualitative Results (8 points): Describe the quantitative and qualitative results. You may choose to use a table or figure to aid in your description. We are looking for both a clear presentation, and a result that makes sense given your data summary. (As an extreme example, you should not have a result that performs worse than a model that, say, predicts the most common class.)\nJustification of Results (20 points): A justification that your implemented method performed reasonably, given the difficulty of the problem—or a hypothesis for why it doesn’t. This is extremely important. We are looking for an interpretation of the result. You may want to refer to your data summary and hyperparameter choices to make your argument.\nEthical Consideration (4 points): Description of a use of the system that could give rise to ethical issues. Are there limitations of your model? Your training data? Please review the feedback provided in the proposal for some guidance on how to think deeply about these issues.\nAuthors (2 points): A description of how the work was split—i.e. who did what in this project. If there are significant issues with the way that work is split, we may follow up with individual teams, and not award equal points to all team members.\n\nCode/Documentation (20 points) We are looking for whether TAs can generally understand what your code does, how it is organized, and the steps that needs to be taken to replicate your model and results. Your code must be in working order (otherwise the TA will not be able to replicate your results)\nAdvanced Concept (10 points). Your project involves at least one of the following:\n\nData Augmentation applied in a way that makes sense for your domain\nTransformer\nGenerative Model, Sequence-to-Sequence Architecture (e.g. that uses teacher-forcing)"
  },
  {
    "objectID": "exams/exam-3.html",
    "href": "exams/exam-3.html",
    "title": "Exam 2",
    "section": "",
    "text": "Exam 3 is released on Friday, April 15 at 9 am ET and must be completed by Mon, April 18 at 11:59 pm.\nThe exam will consist of two parts:\n\nPart 1 - Conceptual: Multiple choice questions on Sakai test and quizzes.\n\nGo here to complete Part 1 of the exam.\nThis portion is comprised of 10 multiple choice / fill in the blank questions may only be submitted one time, so start it when you can set aside ~30 minutes to work on it. You will likely be done quicker but it’s best to set aside ample time.\n\nPart 2 - Applied: Data analysis in RStudio and submitted in Gradescope, like a usual lab and homework.\n\nGo to the GitHub organization for the course and find the exam-3- repo to complete Part 2 of your exam.\nAdd your answers to the exam-3.qmd file in your repo.\nYou can work on this portion at your own pace and come back to it however many times you like until the deadline. I recommend setting aside ~2 hours. You will likely be done quicker, but it’s best to set aside ample time.\n\n\nBoth portions must be completed and submitted by Mon, April 18 at 11:59 pm. Late work will only be accepted in the case of of extenuating circumstances and notification from your academic dean.\n🍀 Good luck! 🍀\n\n\n\nBy taking this exam, you pledge to uphold the Duke Community Standard:\n\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors; and\nI will act if the Standard is compromised.\n\n\n\n\n\nThis is an individual assignment. Everything in your repository and on Sakai is for your eyes only.\nYou may not collaborate or communicate anything about this exam to anyone except the instructor and the head TA (Rick Presman). For example, you may not communicate with other students, other TAs, or post/solicit help on the internet, email or via any other method of communication.\nThe exam is open-book, open-note, so you may use any materials from class as you take the exam.\nOnly head TA and professor office hours will be held during the exam period.\nYou may not email the TAs questions about the exam.\nIf you have questions, direct message the professor on Slack or email or request an appointment to meet on Zoom.\n\n\n\n\n\nPart 1 - Conceptual: The answers to the multiple choice/short answer questions should be submitted under Test & Quizzes in Sakai.\nPart 2 - Applied: The PDF document for the data analysis portion must be uploaded to Gradescope.\n\nYou must submit a PDF to Gradescope that corresponds to the .qmd file on your GitHub repository in order to receive credit for this portion of the exam.\nYou must upload a PDF, not HTML. Any non-PDF submissions will not be graded.\nMark the pages associated with each question. If any answer for a question spans multiple pages, mark all associated pages.\nMake sure to associate the “Workflow & formatting” section with the first page.\nFailure to mark the pages in Gradescope will result in point deductions. Any pages that are not marked will not be graded.\nMake sure that your uploaded PDF document matches your .qmd and the PDF in your GitHub repository exactly. Points will be deducted for documents that are not reproducible."
  },
  {
    "objectID": "exams/exam-3.html#overview",
    "href": "exams/exam-3.html#overview",
    "title": "Exam 2",
    "section": "",
    "text": "Exam 3 is released on Friday, April 15 at 9 am ET and must be completed by Mon, April 18 at 11:59 pm.\nThe exam will consist of two parts:\n\nPart 1 - Conceptual: Multiple choice questions on Sakai test and quizzes.\n\nGo here to complete Part 1 of the exam.\nThis portion is comprised of 10 multiple choice / fill in the blank questions may only be submitted one time, so start it when you can set aside ~30 minutes to work on it. You will likely be done quicker but it’s best to set aside ample time.\n\nPart 2 - Applied: Data analysis in RStudio and submitted in Gradescope, like a usual lab and homework.\n\nGo to the GitHub organization for the course and find the exam-3- repo to complete Part 2 of your exam.\nAdd your answers to the exam-3.qmd file in your repo.\nYou can work on this portion at your own pace and come back to it however many times you like until the deadline. I recommend setting aside ~2 hours. You will likely be done quicker, but it’s best to set aside ample time.\n\n\nBoth portions must be completed and submitted by Mon, April 18 at 11:59 pm. Late work will only be accepted in the case of of extenuating circumstances and notification from your academic dean.\n🍀 Good luck! 🍀"
  },
  {
    "objectID": "exams/exam-3.html#academic-integrity",
    "href": "exams/exam-3.html#academic-integrity",
    "title": "Exam 2",
    "section": "",
    "text": "By taking this exam, you pledge to uphold the Duke Community Standard:\n\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors; and\nI will act if the Standard is compromised."
  },
  {
    "objectID": "exams/exam-3.html#rules-notes",
    "href": "exams/exam-3.html#rules-notes",
    "title": "Exam 2",
    "section": "",
    "text": "This is an individual assignment. Everything in your repository and on Sakai is for your eyes only.\nYou may not collaborate or communicate anything about this exam to anyone except the instructor and the head TA (Rick Presman). For example, you may not communicate with other students, other TAs, or post/solicit help on the internet, email or via any other method of communication.\nThe exam is open-book, open-note, so you may use any materials from class as you take the exam.\nOnly head TA and professor office hours will be held during the exam period.\nYou may not email the TAs questions about the exam.\nIf you have questions, direct message the professor on Slack or email or request an appointment to meet on Zoom."
  },
  {
    "objectID": "exams/exam-3.html#submission",
    "href": "exams/exam-3.html#submission",
    "title": "Exam 2",
    "section": "",
    "text": "Part 1 - Conceptual: The answers to the multiple choice/short answer questions should be submitted under Test & Quizzes in Sakai.\nPart 2 - Applied: The PDF document for the data analysis portion must be uploaded to Gradescope.\n\nYou must submit a PDF to Gradescope that corresponds to the .qmd file on your GitHub repository in order to receive credit for this portion of the exam.\nYou must upload a PDF, not HTML. Any non-PDF submissions will not be graded.\nMark the pages associated with each question. If any answer for a question spans multiple pages, mark all associated pages.\nMake sure to associate the “Workflow & formatting” section with the first page.\nFailure to mark the pages in Gradescope will result in point deductions. Any pages that are not marked will not be graded.\nMake sure that your uploaded PDF document matches your .qmd and the PDF in your GitHub repository exactly. Points will be deducted for documents that are not reproducible."
  },
  {
    "objectID": "course-support.html",
    "href": "course-support.html",
    "title": "Course support",
    "section": "",
    "text": "Most of you will need help at some point and we want to make sure you can identify when that is without getting too frustrated and feel comfortable seeking help."
  },
  {
    "objectID": "course-support.html#lectures-and-labs",
    "href": "course-support.html#lectures-and-labs",
    "title": "Course support",
    "section": "Lectures and labs",
    "text": "Lectures and labs\nIf you have a question during lecture or lab, feel free to ask it! There are likely other students with the same question, so by asking you will create a learning opportunity for everyone."
  },
  {
    "objectID": "course-support.html#office-hours",
    "href": "course-support.html#office-hours",
    "title": "Course support",
    "section": "Office hours",
    "text": "Office hours\nThe teaching team is here to help you be successful in the course. You are encouraged to attend office hours during the times posted on the home page to ask questions about the course content and assignments. A lot of questions are most effectively answered in-person, so office hours are a valuable resource. I encourage each and every one of you to take advantage of this resource! Make a pledge to stop by office hours at least once during the first three weeks of class. If you truly have no questions to ask, just stop by and say hi and introduce yourself. You can find a list of everyone’s office hours here."
  },
  {
    "objectID": "course-support.html#discussion-forum",
    "href": "course-support.html#discussion-forum",
    "title": "Course support",
    "section": "Discussion forum",
    "text": "Discussion forum\nHave a question that can’t wait for office hours? Prefer to write out your question in detail rather than asking in person? The online discussion forum is the best venue for these! We will use Conversations as the online discussion forum. There is a chance another student has already asked a similar question, so please check the other posts on Ed Discussion before adding a new question. If you know the answer to a question that is posted, I encourage you to respond!"
  },
  {
    "objectID": "course-support.html#email",
    "href": "course-support.html#email",
    "title": "Course support",
    "section": "Email",
    "text": "Email\nPlease refrain from emailing any course content questions (those should go Conversations), and only use email for questions about personal matters that may not be appropriate for the public course forum (e.g., illness, accommodations, etc.). For such matters, you may email Dr. Mine Çetinkaya-Rundel at mc301@duke.edu.\nIf there is a question that’s not appropriate for the public forum, you are welcome to email me directly. If you email me, please include “STA 210” in the subject line. Barring extenuating circumstances, I will respond to STA 210 emails within 48 hours Monday - Friday. Response time may be slower for emails sent Friday evening - Sunday."
  },
  {
    "objectID": "course-support.html#academic-support",
    "href": "course-support.html#academic-support",
    "title": "Course support",
    "section": "Academic support",
    "text": "Academic support\nThere are times you may need help with the class that is beyond what can be provided by the teaching team. In those instances, I encourage you to visit the Academic Resource Center. The Academic Resource Center (ARC) offers free services to all students during their undergraduate careers at Duke. Services include Learning Consultations, Peer Tutoring and Study Groups, ADHD/LD Coaching, Outreach Workshops, and more. Because learning is a process unique to every individual, they work with each student to discover and develop their own academic strategy for success at Duke. ARC services are available free to any Duke undergraduate student, in any year, studying in any discipline. Contact the ARC to schedule an appointment. Undergraduates in any year, studying any discipline can benefit! Contact theARC@duke.edu, 919-684-5917."
  },
  {
    "objectID": "course-support.html#mental-health-and-wellness",
    "href": "course-support.html#mental-health-and-wellness",
    "title": "Course support",
    "section": "Mental health and wellness",
    "text": "Mental health and wellness\nStudent mental health and wellness is of primary importance at Duke, and the university offers resources to support students in managing daily stress and self-care. Duke offers several resources for students to seek assistance on coursework and to nurture daily habits that support overall well-being, some of which are listed below:\n\nThe Academic Resource Center: (919) 684-5917, theARC@duke.edu, or arc.duke.edu,\nDuWell: (919) 681-8421, duwell@studentaffairs.duke.edu, or studentaffairs.duke.edu/duwell\n\nIf your mental health concerns and/or stressful events negatively affect your daily emotional state, academic performance, or ability to participate in your daily activities, many resources are available to help you through difficult times. Duke encourages all students to access these resources.\n\nDukeReach. Provides comprehensive outreach services to identify and support students in managing all aspects of well-being. If you have concerns about a student’s behavior or health visit the website for resources and assistance. studentaffairs.duke.edu/dukereach\nCounseling and Psychological Services (CAPS). CAPS helps Duke Students enhance strengths and develop abilities to successfully live, grow and learn in their personal and academic lives. CAPS recognizes that we are living in unprecedented times and that the changes, challenges and stressors brought on by the COVID-19 pandemic have impacted everyone, often in ways that are tax our well-being. CAPS offers many services to Duke undergraduate students, including brief individual and group counseling, couples counseling and more. CAPS staff also provides outreach to student groups, particularly programs supportive of at-risk populations, on a wide range of issues impacting them in various aspects of campus life. CAPS provides services to students via Telehealth. To initiate services, you can contact their front desk at 919-660-1000. studentaffairs.duke.edu/caps\nBlue Devils Care. A convenient and cost-effective way for Duke students to receive 24/7 mental health support through TalkNow and scheduled counseling. bluedevilscare.duke.edu\nTwo-Click Support. Duke Student Government and DukeReach partnership that connects students to help in just two clicks. bit.ly/TwoClickSupport\nWellTrack. Sign up for WellTrack at app.welltrack.com."
  },
  {
    "objectID": "course-support.html#technology-accommodations",
    "href": "course-support.html#technology-accommodations",
    "title": "Course support",
    "section": "Technology accommodations",
    "text": "Technology accommodations\nStudents with demonstrated high financial need who have limited access to computers may request assistance in the form of loaner laptops. For technology assistance requests, please go here. Please note that supplies are limited.\nNote that we will be using Duke’s computational resources in this course. These resources are freely available to you. As long as your computer can connect to the internet and open a browser window, you can perform the necessary computing for this course. All software we use is open-source and/or freely available."
  },
  {
    "objectID": "course-support.html#course-materials-costs",
    "href": "course-support.html#course-materials-costs",
    "title": "Course support",
    "section": "Course materials costs",
    "text": "Course materials costs\nThere are no costs associated with this course. All readings will come from freely available, open resources (open-source textbooks, journal articles, etc.)."
  },
  {
    "objectID": "course-support.html#assistance-with-zoom-or-sakai",
    "href": "course-support.html#assistance-with-zoom-or-sakai",
    "title": "Course support",
    "section": "Assistance with Zoom or Sakai",
    "text": "Assistance with Zoom or Sakai\nFor technical help with Sakai or Zoom, contact the Duke OIT Service Desk at oit.duke.edu/help. You can also access the self-service help documentation for Zoom here and for Sakai here.\nNote that we will be making minimal use of Sakai in this course (primarily for announcements and grade book). All assignment submission and discussion will take place on GitHub instead.\nZoom will be used for online office hours as well as as a backup option should we need to hold the course online instead of in person."
  },
  {
    "objectID": "course-faq.html",
    "href": "course-faq.html",
    "title": "FAQ",
    "section": "",
    "text": "Go to your Files tab, check the box next to the file you want to download, then click on the blue gear icon on the Files tab to reveal the drop down menu, and select Export… If you have selected multiple files to export, RStudio will zip them up into a single zip file for you. If you’ve selected just a single file, it will only download that. The downloaded file will go to wherever files you download off the internet goes on your computer (usually your Downloads folder)."
  },
  {
    "objectID": "course-faq.html#how-do-i-export-my-assignment-pdf-from-rstudio-to-upload-to-gradescope",
    "href": "course-faq.html#how-do-i-export-my-assignment-pdf-from-rstudio-to-upload-to-gradescope",
    "title": "FAQ",
    "section": "",
    "text": "Go to your Files tab, check the box next to the file you want to download, then click on the blue gear icon on the Files tab to reveal the drop down menu, and select Export… If you have selected multiple files to export, RStudio will zip them up into a single zip file for you. If you’ve selected just a single file, it will only download that. The downloaded file will go to wherever files you download off the internet goes on your computer (usually your Downloads folder)."
  },
  {
    "objectID": "course-faq.html#how-can-i-submit-my-assignment-to-gradescope",
    "href": "course-faq.html#how-can-i-submit-my-assignment-to-gradescope",
    "title": "FAQ",
    "section": "How can I submit my assignment to Gradescope?",
    "text": "How can I submit my assignment to Gradescope?\nThe instructions for submitting your assignment to Gradescope can be found here. In a nutshell, you’ll upload your PDF and them mark the page(s) where each question can be found. It’s OK if a question spans multiple pages, just mark them all. It’s also OK if a page includes multiple questions."
  },
  {
    "objectID": "course-faq.html#can-i-use-a-local-install-of-r-and-rstudio-instead-of-using-the-rstudio-containers",
    "href": "course-faq.html#can-i-use-a-local-install-of-r-and-rstudio-instead-of-using-the-rstudio-containers",
    "title": "FAQ",
    "section": "Can I use a local install of R and RStudio instead of using the RStudio containers?",
    "text": "Can I use a local install of R and RStudio instead of using the RStudio containers?\nThe short answer is, I’d rather you didn’t, to save yourself some headache. But, the long answer is, sure! But you will need to install a specific versions of R and RStudio for everything to work as expected. You will also need to install the R packages we’re using as well as have Git installed on your computer. These are not extremely challenging things to get right, but they are not trivial either, particularly on certain operating systems. Myself and the TAs are always happy to provide help with any computational questions when you’re working in the containers we have provided for you. If you’re working on your local setup, we can’t guarantee being able to resolve your issues, though we’re happy to try.\nIf you want to take this path, here is what you need to do:\n\nDownload and install R 4.1.2: https://cran.r-project.org/\nDownload and install a daily build of RStudio: https://dailies.rstudio.com/\nInstall Quarto CLI: https://quarto.org/docs/getting-started/installation.html\nInstall Git: https://happygitwithr.com/install-git.html\nInstall any necessary packages with install.packages(\"___\")\n\nAnd I’d like to reiterate again that successful installation of these software is not a learning goal of this course. So if any of this seems tedious or intimidating in any way, just use the computing environment we have set up for you. More on that here."
  },
  {
    "objectID": "supplemental/model-selection-criteria.html",
    "href": "supplemental/model-selection-criteria.html",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr. Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\nThis document discusses some of the mathematical details of Akaike’s Information Criterion (AIC) and Schwarz’s Bayesian Information Criterion (BIC). We assume the reader knowledge of the matrix form for multiple linear regression.Please see Matrix Notation for Multiple Linear Regression for a review."
  },
  {
    "objectID": "supplemental/model-selection-criteria.html#maximum-likelihood-estimation-of-boldsymbolbeta-and-sigma",
    "href": "supplemental/model-selection-criteria.html#maximum-likelihood-estimation-of-boldsymbolbeta-and-sigma",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "Maximum Likelihood Estimation of \\(\\boldsymbol{\\beta}\\) and \\(\\sigma\\)",
    "text": "Maximum Likelihood Estimation of \\(\\boldsymbol{\\beta}\\) and \\(\\sigma\\)\nTo understand the formulas for AIC and BIC, we will first briefly explain the likelihood function and maximum likelihood estimates for regression.\nLet \\(\\mathbf{Y}\\) be \\(n \\times 1\\) matrix of responses, \\(\\mathbf{X}\\), the \\(n \\times (p+1)\\) matrix of predictors, and \\(\\boldsymbol{\\beta}\\), \\((p+1) \\times 1\\) matrix of coefficients. If the multiple linear regression model is correct then,\n\\[\n\\mathbf{Y} \\sim N(\\mathbf{X}\\boldsymbol{\\beta}, \\sigma^2)\n\\tag{1}\\]\nWhen we do linear regression, our goal is to estimate the unknown parameters \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) from Equation 1. In Matrix Notation for Multiple Linear Regression, we showed a way to estimate these parameters using matrix alegbra. Another approach for estimating \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) is using maximum likelihood estimation.\nA likelihood function is used to summarise the evidence from the data in support of each possible value of a model parameter. Using Equation 1, we will write the likelihood function for linear regression as\n\\[\nL(\\mathbf{X}, \\mathbf{Y}|\\boldsymbol{\\beta}, \\sigma^2) = \\prod\\limits_{i=1}^n (2\\pi \\sigma^2)^{-\\frac{1}{2}} \\exp\\bigg\\{-\\frac{1}{2\\sigma^2}\\sum\\limits_{i=1}^n(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta})^T(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta})\\bigg\\}\n\\tag{2}\\]\nwhere \\(Y_i\\) is the \\(i^{th}\\) response and \\(\\mathbf{X}_i\\) is the vector of predictors for the \\(i^{th}\\) observation. One approach estimating \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) is to find the values of those parameters that maximize the likelihood in Equation 2, i.e. maximum likelhood estimation. To make the calculations more manageable, instead of maximizing the likelihood function, we will instead maximize its logarithm, i.e. the log-likelihood function. The values of the parameters that maximize the log-likelihood function are those that maximize the likelihood function. The log-likelihood function we will maximize is\n\\[\n\\begin{aligned}\n\\log L(\\mathbf{X}, \\mathbf{Y}|\\boldsymbol{\\beta}, \\sigma^2) &= \\sum\\limits_{i=1}^n -\\frac{1}{2}\\log(2\\pi\\sigma^2) -\\frac{1}{2\\sigma^2}\\sum\\limits_{i=1}^n(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta})^T(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta}) \\\\\n&= -\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})^T(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})\\\\\n\\end{aligned}\n\\tag{3}\\]\n\nThe maximum likelihood estimate of \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) are \\[\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} \\hspace{10mm} \\hat{\\sigma}^2 = \\frac{1}{n}(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})^T(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta}) = \\frac{1}{n}RSS\n\\tag{4}\\]\nwhere \\(RSS\\) is the residual sum of squares. Note that the maximum likelihood estimate is not exactly equal to the estimate of \\(\\sigma^2\\) we typically use \\(\\frac{RSS}{n-p-1}\\). This is because the maximum likelihood estimate of \\(\\sigma^2\\) in Equation 4 is a biased estimator of \\(\\sigma^2\\). When \\(n\\) is much larger than the number of predictors \\(p\\), then the differences in these two estimates are trivial."
  },
  {
    "objectID": "supplemental/model-selection-criteria.html#aic",
    "href": "supplemental/model-selection-criteria.html#aic",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "AIC",
    "text": "AIC\nAkaike’s Information Criterion (AIC) is\n\\[\nAIC = -2 \\log L + 2(p+1)\n\\tag{5}\\]\nwhere \\(\\log L\\) is the log-likelihood. This is the general form of AIC that can be applied to a variety of models, but for now, let’s focus on AIC for mutliple linear regression.\n\\[\n\\begin{aligned}\nAIC &= -2 \\log L + 2(p+1) \\\\\n&= -2\\bigg[-\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})^T(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})\\bigg] + 2(p+1) \\\\\n&= n\\log\\big(2\\pi\\frac{RSS}{n}\\big) + \\frac{1}{RSS/n}RSS \\\\\n&= n\\log(2\\pi) + n\\log(RSS) - n\\log(n) + 2(p+1)\n\\end{aligned}\n\\tag{6}\\]"
  },
  {
    "objectID": "supplemental/model-selection-criteria.html#bic",
    "href": "supplemental/model-selection-criteria.html#bic",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "BIC",
    "text": "BIC\n[To be added.]"
  },
  {
    "objectID": "supplemental/slr-derivations.html",
    "href": "supplemental/slr-derivations.html",
    "title": "Deriving the Least-Squares Estimates for Simple Linear Regression",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr. Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\n\n\nThis document contains the mathematical details for deriving the least-squares estimates for slope (\\(\\beta_1\\)) and intercept (\\(\\beta_0\\)). We obtain the estimates, \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\) by finding the values that minimize the sum of squared residuals, as shown in Equation 1.\n\\[\nSSR = \\sum\\limits_{i=1}^{n}[y_i - \\hat{y}_i]^2 = [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)]^2 = [y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i]^2\n\\tag{1}\\]\nRecall that we can find the values of \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\) that minimize /eq-ssr by taking the partial derivatives of Equation 1 and setting them to 0. Thus, the values of \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\) that minimize the respective partial derivative also minimize the sum of squared residuals. The partial derivatives are shown in Equation 2.\n\\[\n\\begin{aligned}\n\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_1} &= -2 \\sum\\limits_{i=1}^{n}x_i(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)  \\\\\n\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_0} &= -2 \\sum\\limits_{i=1}^{n}(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)\n\\end{aligned}\n\\tag{2}\\]\nThe derivation of deriving \\(\\hat{\\beta}_0\\) is shown in Equation 3.\n\\[\n\\begin{aligned}\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_0} &= -2 \\sum\\limits_{i=1}^{n}(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0 \\\\&\\Rightarrow -\\sum\\limits_{i=1}^{n}(y_i + \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i) = 0 \\\\&\\Rightarrow - \\sum\\limits_{i=1}^{n}y_i + n\\hat{\\beta}_0 + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i = 0 \\\\&\\Rightarrow n\\hat{\\beta}_0  = \\sum\\limits_{i=1}^{n}y_i - \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i \\\\&\\Rightarrow \\hat{\\beta}_0  = \\frac{1}{n}\\Big(\\sum\\limits_{i=1}^{n}y_i - \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i\\Big)\\\\&\\Rightarrow \\hat{\\beta}_0  = \\bar{y} - \\hat{\\beta}_1 \\bar{x} \\\\\\end{aligned}\n\\tag{3}\\]\nThe derivation of \\(\\hat{\\beta}_1\\) using the \\(\\hat{\\beta}_0\\) we just derived is shown in Equation 4.\n\\[\n\\begin{aligned}&\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_1} = -2 \\sum\\limits_{i=1}^{n}x_i(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0  \\\\&\\Rightarrow -\\sum\\limits_{i=1}^{n}x_iy_i + \\hat{\\beta}_0\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = 0 \\\\\\text{(Fill in }\\hat{\\beta}_0\\text{)}&\\Rightarrow -\\sum\\limits_{i=1}^{n}x_iy_i + (\\bar{y} - \\hat{\\beta}_1\\bar{x})\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = 0 \\\\&\\Rightarrow  (\\bar{y} - \\hat{\\beta}_1\\bar{x})\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = \\sum\\limits_{i=1}^{n}x_iy_i \\\\&\\Rightarrow \\bar{y}\\sum\\limits_{i=1}^{n}x_i - \\hat{\\beta}_1\\bar{x}\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = \\sum\\limits_{i=1}^{n}x_iy_i \\\\&\\Rightarrow n\\bar{y}\\bar{x} - \\hat{\\beta}_1n\\bar{x}^2 + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = \\sum\\limits_{i=1}^{n}x_iy_i \\\\&\\Rightarrow \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 - \\hat{\\beta}_1n\\bar{x}^2  = \\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x} \\\\&\\Rightarrow \\hat{\\beta}_1\\Big(\\sum\\limits_{i=1}^{n}x_i^2 -n\\bar{x}^2\\Big)  = \\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x} \\\\ &\\hat{\\beta}_1 = \\frac{\\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x}}{\\sum\\limits_{i=1}^{n}x_i^2 -n\\bar{x}^2}\\end{aligned}\n\\tag{4}\\]\nTo write \\(\\hat{\\beta}_1\\) in a form that’s more recognizable, we will use the following:\n\\[\n\\sum x_iy_i - n\\bar{y}\\bar{x} = \\sum(x - \\bar{x})(y - \\bar{y}) = (n-1)\\text{Cov}(x,y)\n\\tag{5}\\]\n\\[\n\\sum x_i^2 - n\\bar{x}^2 - \\sum(x - \\bar{x})^2 = (n-1)s_x^2\n\\tag{6}\\]\nwhere \\(\\text{Cov}(x,y)\\) is the covariance of \\(x\\) and \\(y\\), and \\(s_x^2\\) is the sample variance of \\(x\\) (\\(s_x\\) is the sample standard deviation).\nThus, applying Equation 5 and Equation 6, we have\n\\[\n\\begin{aligned}\\hat{\\beta}_1 &= \\frac{\\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x}}{\\sum\\limits_{i=1}^{n}x_i^2 -n\\bar{x}^2} \\\\&= \\frac{\\sum\\limits_{i=1}^{n}(x-\\bar{x})(y-\\bar{y})}{\\sum\\limits_{i=1}^{n}(x-\\bar{x})^2}\\\\&= \\frac{(n-1)\\text{Cov}(x,y)}{(n-1)s_x^2}\\\\&= \\frac{\\text{Cov}(x,y)}{s_x^2}\\end{aligned}\n\\tag{7}\\]\nThe correlation between \\(x\\) and \\(y\\) is \\(r = \\frac{\\text{Cov}(x,y)}{s_x s_y}\\). Thus, \\(\\text{Cov}(x,y) = r s_xs_y\\). Plugging this into Equation 7, we have\n\\[\n\\hat{\\beta}_1 = \\frac{\\text{Cov}(x,y)}{s_x^2} = r\\frac{s_ys_x}{s_x^2} = r\\frac{s_y}{s_x}\n\\tag{8}\\]"
  },
  {
    "objectID": "supplemental/mlr-matrix.html",
    "href": "supplemental/mlr-matrix.html",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr. Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\nThis document provides the details for the matrix notation for multiple linear regression. We assume the reader has familiarity with some linear algebra. Please see Chapter 1 of An Introduction to Statistical Learning for a brief review of linear algebra."
  },
  {
    "objectID": "supplemental/mlr-matrix.html#introduction",
    "href": "supplemental/mlr-matrix.html#introduction",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "Introduction",
    "text": "Introduction\nSuppose we have \\(n\\) observations. Let the \\(i^{th}\\) be \\((x_{i1}, \\ldots, x_{ip}, y_i)\\), such that \\(x_{i1}, \\ldots, x_{ip}\\) are the explanatory variables (predictors) and \\(y_i\\) is the response variable. We assume the data can be modeled using the least-squares regression model, such that the mean response for a given combination of explanatory variables follows the form in Equation 1.\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_p x_p\n\\tag{1}\\]\nWe can write the response for the \\(i^{th}\\) observation as shown in Equation 2\n\\[\ny_i = \\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip} + \\epsilon_i\n\\tag{2}\\]\nsuch that \\(\\epsilon_i\\) is the amount \\(y_i\\) deviates from \\(\\mu\\{y|x_{i1}, \\ldots, x_{ip}\\}\\), the mean response for a given combination of explanatory variables. We assume each \\(\\epsilon_i \\sim N(0,\\sigma^2)\\), where \\(\\sigma^2\\) is a constant variance for the distribution of the response \\(y\\) for any combination of explanatory variables \\(x_1, \\ldots, x_p\\)."
  },
  {
    "objectID": "supplemental/mlr-matrix.html#matrix-representation-for-the-regression-model",
    "href": "supplemental/mlr-matrix.html#matrix-representation-for-the-regression-model",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "Matrix Representation for the Regression Model",
    "text": "Matrix Representation for the Regression Model\nWe can represent the Equation 1 and Equation 2 using matrix notation. Let\n\\[\n\\mathbf{Y} = \\begin{bmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\y_n\\end{bmatrix}\n\\hspace{15mm}\n\\mathbf{X} = \\begin{bmatrix}x_{11} & x_{12} & \\dots & x_{1p} \\\\\nx_{21} & x_{22} & \\dots & x_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\nx_{n1} & x_{n2} & \\dots & x_{np} \\end{bmatrix}\n\\hspace{15mm}\n\\boldsymbol{\\beta}= \\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{bmatrix}\n\\hspace{15mm}\n\\boldsymbol{\\epsilon}= \\begin{bmatrix}\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix}\n\\tag{3}\\]\nThus,\n\\[\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{\\epsilon}\\]\nTherefore the estimated response for a given combination of explanatory variables and the associated residuals can be written as\n\\[\n\\hat{\\mathbf{Y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\hspace{10mm} \\mathbf{e} = \\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\n\\tag{4}\\]"
  },
  {
    "objectID": "supplemental/mlr-matrix.html#estimating-the-coefficients",
    "href": "supplemental/mlr-matrix.html#estimating-the-coefficients",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "Estimating the Coefficients",
    "text": "Estimating the Coefficients\nThe least-squares model is the one that minimizes the sum of the squared residuals. Therefore, we want to find the coefficients, \\(\\hat{\\boldsymbol{\\beta}}\\) that minimizes\n\\[\n\\sum\\limits_{i=1}^{n} e_{i}^2 = \\mathbf{e}^T\\mathbf{e} = (\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}})^T(\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}})\n\\tag{5}\\]\nwhere \\(\\mathbf{e}^T\\), the transpose of the matrix \\(\\mathbf{e}\\).\n\\[\n(\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}})^T(\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}) = (\\mathbf{Y}^T\\mathbf{Y} -\n\\mathbf{Y}^T \\mathbf{X}\\hat{\\boldsymbol{\\beta}} - (\\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{Y} +\n\\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{X}\n\\hat{\\boldsymbol{\\beta}})\n\\tag{6}\\]\nNote that \\((\\mathbf{Y^T}\\mathbf{X}\\hat{\\boldsymbol{\\beta}})^T = \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{Y}\\). Since these are both constants (i.e. \\(1\\times 1\\) vectors), \\(\\mathbf{Y^T}\\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{Y}\\). Thus, Equation 7 becomes\n\\[\n\\mathbf{Y}^T\\mathbf{Y} - 2 \\mathbf{X}^T\\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{Y} + \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{X}\n\\hat{\\boldsymbol{\\beta}}\n\\tag{7}\\]\nSince we want to find the \\(\\hat{\\boldsymbol{\\beta}}\\) that minimizes Equation 5, will find the value of \\(\\hat{\\boldsymbol{\\beta}}\\) such that the derivative with respect to \\(\\hat{\\boldsymbol{\\beta}}\\) is equal to 0.\n\\[\n\\begin{aligned}\n\\frac{\\partial \\mathbf{e}^T\\mathbf{e}}{\\partial \\hat{\\boldsymbol{\\beta}}} & = \\frac{\\partial}{\\partial \\hat{\\boldsymbol{\\beta}}}(\\mathbf{Y}^T\\mathbf{Y} - 2 \\mathbf{X}^T\\hat{\\boldsymbol{\\beta}}{}^T\\mathbf{Y} + \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}}) = 0 \\\\\n&\\Rightarrow - 2 \\mathbf{X}^T\\mathbf{Y} + 2 \\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} = 0 \\\\\n& \\Rightarrow 2 \\mathbf{X}^T\\mathbf{Y} = 2 \\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\\n& \\Rightarrow \\mathbf{X}^T\\mathbf{Y} = \\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\\n& \\Rightarrow (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\\n& \\Rightarrow (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} = \\mathbf{I}\\hat{\\boldsymbol{\\beta}}\n\\end{aligned}\n\\tag{8}\\]\nThus, the estimate of the model coefficients is \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\)."
  },
  {
    "objectID": "supplemental/mlr-matrix.html#variance-covariance-matrix-of-the-coefficients",
    "href": "supplemental/mlr-matrix.html#variance-covariance-matrix-of-the-coefficients",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "Variance-covariance matrix of the coefficients",
    "text": "Variance-covariance matrix of the coefficients\nWe will use two properties to derive the form of the variance-covariance matrix of the coefficients:\n\n\\(E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] = \\sigma^2I\\)\n\\(\\hat{\\boldsymbol{\\beta}} = \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\epsilon\\)\n\nFirst, we will show that \\(E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] = \\sigma^2I\\)\n\\[\n\\begin{aligned}\nE[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] &= E \\begin{bmatrix}\\epsilon_1  & \\epsilon_2 & \\dots & \\epsilon_n \\end{bmatrix}\\begin{bmatrix}\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix}  \\\\\n& = E \\begin{bmatrix} \\epsilon_1^2  & \\epsilon_1 \\epsilon_2 & \\dots & \\epsilon_1 \\epsilon_n \\\\\n\\epsilon_2 \\epsilon_1 & \\epsilon_2^2 & \\dots & \\epsilon_2 \\epsilon_n \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\epsilon_n \\epsilon_1 & \\epsilon_n \\epsilon_2 & \\dots & \\epsilon_n^2\n\\end{bmatrix} \\\\\n& = \\begin{bmatrix} E[\\epsilon_1^2]  & E[\\epsilon_1 \\epsilon_2] & \\dots & E[\\epsilon_1 \\epsilon_n] \\\\\nE[\\epsilon_2 \\epsilon_1] & E[\\epsilon_2^2] & \\dots & E[\\epsilon_2 \\epsilon_n] \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nE[\\epsilon_n \\epsilon_1] & E[\\epsilon_n \\epsilon_2] & \\dots & E[\\epsilon_n^2]\n\\end{bmatrix}\n\\end{aligned}\n\\tag{9}\\]\nRecall, the regression assumption that the errors \\(\\epsilon_i's\\) are Normally distributed with mean 0 and variance \\(\\sigma^2\\). Thus, \\(E(\\epsilon_i^2) = Var(\\epsilon_i) = \\sigma^2\\) for all \\(i\\). Additionally, recall the regression assumption that the errors are uncorrelated, i.e. \\(E(\\epsilon_i \\epsilon_j) = Cov(\\epsilon_i, \\epsilon_j) = 0\\) for all \\(i,j\\). Using these assumptions, we can write Equation 9 as\n\\[\nE[\\mathbf{\\epsilon}\\mathbf{\\epsilon}^T]  = \\begin{bmatrix} \\sigma^2  & 0 & \\dots & 0 \\\\\n0 & \\sigma^2  & \\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\dots & \\sigma^2\n\\end{bmatrix} = \\sigma^2 \\mathbf{I}\n\\tag{10}\\]\nwhere \\(\\mathbf{I}\\) is the \\(n \\times n\\) identity matrix.\nNext, we show that \\(\\hat{\\boldsymbol{\\beta}} = \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\epsilon\\).\nRecall that the \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\) and \\(\\mathbf{Y} = \\mathbf{X}\\mathbf{\\beta} + \\mathbf{\\epsilon}\\). Then,\n\\[\n\\begin{aligned}\n\\hat{\\boldsymbol{\\beta}} &= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} \\\\\n&= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T(\\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}) \\\\\n&= \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T \\mathbf{\\epsilon} \\\\\n\\end{aligned}\n\\tag{11}\\]\nUsing these two properties, we derive the form of the variance-covariance matrix for the coefficients. Note that the covariance matrix is \\(E[(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})^T]\\)\n\\[\n\\begin{aligned}\nE[(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})^T] &= E[(\\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon} - \\boldsymbol{\\beta})(\\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon} - \\boldsymbol{\\beta})^T]\\\\\n& = E[(\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}] \\\\\n& = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T]\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n& = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T (\\sigma^2\\mathbf{I})\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n&= \\sigma^2\\mathbf{I}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n& = \\sigma^2\\mathbf{I}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n&  = \\sigma^2(\\mathbf{X}^T\\mathbf{X})^{-1} \\\\\n\\end{aligned}\n\\tag{12}\\]"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Fall 2024 - CSC477/CSC2630: Introduction to Mobile Robotics",
    "section": "",
    "text": "Course Overview\nThis cross-listed course (undergraduate version CSC477H, graduate version CSC2630H) provides an introduction to robotic systems from a computational perspective. A robot is regarded as an intelligent computer that can use sensors and act on the world. We will consider the definitional problems in robotics and look at how they are being solved in practice and by the research community. The emphasis is on algorithms, probabilistic reasoning, optimization, inference mechanisms, and behavior strategies, as opposed to electromechanical systems design. This course aims to help students improve their probabilistic modeling skills and instill the idea that a robot that explicitly accounts for its uncertainty works better than a robot that does not.\n\n\nPrerequisites\nRequired: CSC209H5; STA256H5; MAT223H5/MAT240H5; MAT232H5; CSC376\nRecommended: MAT224H5; CSC384H5; CSC311H5;\n\n\nCourse Delivery Details\n\nLectures: Wednesdays @ 3-5pm ET, MN3190 & Livstreamed on Zoom at MY580\nTutorials: Wednesdays @ 5-6pm ET, DH2020 & Livstreamed on Zoom at MY580\nOffice Hours will be delivered on Zoom. In-person office hours can be arranged by appointment.\nZoom links & Announcements will be posted on Quercus\nDiscussions will take place on Piazza\nAnonymous feedback form for suggested improvements\nCourse Syllabus",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "computing-access.html",
    "href": "computing-access.html",
    "title": "Computing access",
    "section": "",
    "text": "To access computing resources for the introductory data science courses offered by the Duke University Department of Statistical Science, go to the Duke Container Manager website, cmgr.oit.duke.edu/containers.\nIf this is your first time accessing the containers, click on reserve STA210 on the Reservations available menu on the right. You only need to do this once, and when you do, you’ll see this container moved to the My reservations menu on the left.\nNext, click on STA210 under My reservations to access the RStudio instance you’ll use for the course."
  },
  {
    "objectID": "ex/w06/questions/prob-variance-sol.html",
    "href": "ex/w06/questions/prob-variance-sol.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "First, we use the definition of variance and rewrite the left hand side as \\[\n\\Var(aX+bY) = \\E\\li[ (aX+bY)^2 \\ri] - \\E[aX+bY]^2.\n\\] Next, we expand the squares for each of the terms on the right hand side: \\[\\begin{align*}\n\\E\\li[ (aX+bY)^2 \\ri]\n  &= \\E\\li[ a^2X^2 + 2abXY + b^2Y^2 \\ri] \\\\\n  &= a^2\\E\\li[ X^2 \\ri] + 2ab\\E[ XY ] + b^2\\E\\li[ Y^2 \\ri] \\\\\n  &= a^2\\E\\li[ X^2 \\ri] + 2ab\\E[ X]\\,\\E[Y] + b^2\\E\\li[ Y^2 \\ri] ,\\\\\n\\E[aX+bY]^2\n  &= \\li(a\\E[X] + b\\E[Y]\\ri)^2 \\\\\n  &= a^2\\E[X]^2 + 2ab\\E[X]\\,\\E[Y] + b^2\\E[Y]^2.\n\\end{align*}\\] Subtracting the two terms, we get \\[\\begin{align*}\n& \\E\\li[ (aX+bY)^2 \\ri] - \\E[aX+bY]^2 \\\\\n& \\quad = a^2\\E\\li[ X^2 \\ri] + 2ab\\E[ X]\\,\\E[Y] + b^2\\E\\li[ Y^2 \\ri]\n    - a^2\\E[X]^2 - 2ab\\E[X]\\,\\E[Y] - b^2\\E[Y]^2 \\\\\n& \\quad = a^2\\E\\li[ X^2 \\ri] - a^2\\E[X]^2\n   + b^2\\E\\li[ Y^2 \\ri] - b^2\\E[Y]^2 \\\\\n& \\quad = a^2(\\E\\li[ X^2 \\ri] - \\E[X]^2)\n   + b^2\\li(\\E\\li[ Y^2 \\ri] - \\E[Y]^2\\ri) \\\\\n& \\quad = a^2\\cdot\\Var(X) + b^2\\cdot\\Var(Y).\n\\end{align*}\\]"
  },
  {
    "objectID": "ex/w06/questions/ml-variance_bias_decomposition.html",
    "href": "ex/w06/questions/ml-variance_bias_decomposition.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "Let \\(D=\\lbrace (x_i,y_i) | i=1 \\ldots n\\rbrace\\) be a dataset obtained from the true underlying data distribution \\(P\\), i.e. \\(D\\sim P^n\\). And let \\(h_D(\\cdot)\\) be a classifier trained on \\(D\\). Show the variance bias decomposition \\[\n\\underbrace{\\mathbb{E}_{D,x,y} \\li[ (h_D(x) - y)^2 \\ri]}_{\\text{Expected test error}}  \n  = \\underbrace{\\mathbb{E}_{D,x} \\li[ (h_D(x) - \\hat{h}(x))^2 \\ri]}_{\\text{Variance}} +\n  \\underbrace{\\mathbb{E}_{x,y} \\li[ (\\hat{y}(x) - y)^2 \\ri]}_{\\text{Noise}}  +  \\underbrace{\\mathbb{E}_{x} \\li[ (\\hat{h}(x) - \\hat{y}(x))^2 \\ri]}_{\\text{Bias}^2}\n\\] where \\(\\hat{h}(x) = \\mathbb{E}_{D \\sim P^n}[h_D(x)]\\) is the expected regressor over possible training sets, given the learning algorithm \\(\\mathcal{A}\\) and \\(\\hat{y}(x) = \\mathbb{E}_{y|x}[y]\\) is the expected label given \\(x\\). As mentioned in the lecture, labels might not be deterministic given x. To carry out the proof, proceed in the following steps:\n\nShow that the following identity holds \\[\\begin{align}\n\\E_{D,x,y}\\li[\\li[h_{D}(x) - y\\ri]^{2}\\ri]\n= \\E_{D, x}\\li[(\\hh_{D}(x) - \\hh(x))^{2}\\ri] + \\E_{x, y} \\li[\\li(\\hh(x) - y\\ri)^{2}\\ri].\n   \\end{align}\\]\nNext, show \\[\\begin{align}\nE_{x, y} \\li[ \\li(\\hh(x) - y \\ri)^{2}\\ri]\n=E_{x, y} \\li[\\li(\\hy(x) - y\\ri)^{2}\\ri] + E_{x} \\li[\\li(\\hh(x) - \\hy(x)\\ri)^{2}\\ri]\n\\end{align}\\] which completes the proof by substituting (2) into (1)."
  },
  {
    "objectID": "ex/w06/exercises06-ensembling.html",
    "href": "ex/w06/exercises06-ensembling.html",
    "title": "Ensembling",
    "section": "",
    "text": "Tre training code below (based on a tutorial from here) trains a simple MNIS classifier.\nYour task is to adjust it to use an ensemble of 5 models instead of a single one.\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nfrom time import time\nfrom torchvision import datasets, transforms\nfrom torch import nn, optim\ntransform = transforms.Compose([transforms.ToTensor(),\n                              transforms.Normalize((0.5,), (0.5,)),\n                              ])\nSetup the dataset and dataloaders.\ntrainset = datasets.MNIST(\n    'datasets', download=True, train=True, transform=transform)\nvalset = datasets.MNIST(\n    'datasets', download=True, train=False, transform=transform)\ntrainloader = torch.utils.data.DataLoader(\n    trainset, batch_size=64, shuffle=True)\nvalloader = torch.utils.data.DataLoader(\n    valset, batch_size=64, shuffle=True)\nDefine the model.\ninput_size = 784\nhidden_sizes = [128, 64]\noutput_size = 10\n\nmodel = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n                      nn.ReLU(),\n                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n                      nn.ReLU(),\n                      nn.Linear(hidden_sizes[1], output_size),\n                      nn.LogSoftmax(dim=1))\ncriterion = nn.NLLLoss()\nimages, labels = next(iter(trainloader))\nimages = images.view(images.shape[0], -1)\n\nlogps = model(images) #log probabilities\nloss = criterion(logps, labels) #calculate the NLL loss\nRun the main training loop.\noptimizer = optim.SGD(model.parameters(), lr=0.003, momentum=0.9)\ntime0 = time()\nepochs = 15\nfor e in range(epochs):\n    running_loss = 0\n    for images, labels in trainloader:\n        # Flatten MNIST images into a 784 long vector\n        images = images.view(images.shape[0], -1)\n        optimizer.zero_grad()\n        output = model(images)\n        loss = criterion(output, labels)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n    else:\n        print(\"Epoch {} - Training loss: {}\".format(e, running_loss/len(trainloader)))\nprint(\"\\nTraining Time (in minutes) =\",(time()-time0)/60)\ncorrect_count, all_count = 0, 0\nfor images,labels in valloader:\n  for i in range(len(labels)):\n    img = images[i].view(1, 784)\n    with torch.no_grad():\n        logps = model(img)\n\n    \n    ps = torch.exp(logps)\n    probab = list(ps.numpy()[0])\n    pred_label = probab.index(max(probab))\n    true_label = labels.numpy()[i]\n    if(true_label == pred_label):\n      correct_count += 1\n    all_count += 1\n\nprint(\"Number Of Images Tested =\", all_count)\nprint(\"\\nModel Accuracy =\", (correct_count/all_count))"
  },
  {
    "objectID": "ex/w06/exercises06-ensembling.html#version-with-ensembles",
    "href": "ex/w06/exercises06-ensembling.html#version-with-ensembles",
    "title": "Ensembling",
    "section": "Version with Ensembles",
    "text": "Version with Ensembles\nTODO: Add your code below."
  },
  {
    "objectID": "ex/w08/questions/rnn-addition.html",
    "href": "ex/w08/questions/rnn-addition.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "In this problem, you will implement a recurrent neural network which implements binary addition. The inputs are given as binary sequences, starting with the significant binary digit. (It is easier to start from the least significant bit, just like how you did addition in grade school.) The sequences will be padded with at least one zero as the most significant digit, so that the output length is the same as the input length. For example, the problem \\(100111 + 110010\\), whose target output value is \\(1011001\\), will be represented as follows: \\[\\begin{align*}\n\\bf{x}^{(1)} = \\begin{bmatrix}1 \\\\ 0\\end{bmatrix},\n\\bf{x}^{(2)} = \\begin{bmatrix}1 \\\\ 1\\end{bmatrix},\n\\bf{x}^{(3)} = \\begin{bmatrix}1 \\\\ 0\\end{bmatrix},\n\\bf{x}^{(4)} = \\begin{bmatrix}0 \\\\ 0\\end{bmatrix},\n\\bf{x}^{(5)} = \\begin{bmatrix}0 \\\\ 1\\end{bmatrix},\n\\bf{x}^{(6)} = \\begin{bmatrix}1 \\\\ 1\\end{bmatrix},\n\\bf{x}^{(7)} = \\begin{bmatrix}0 \\\\ 0\\end{bmatrix}\n\\end{align*}\\]\nWith the target output: \\[\\begin{align*}\ny^{(1)} = 1,\ny^{(2)} = 0,\ny^{(3)} = 0,\ny^{(4)} = 1,\ny^{(5)} = 1,\ny^{(6)} = 0,\ny^{(7)} = 1,\n\\end{align*}\\]\nThere are two input units corresponding to the two inputs, and one output unit. Therefore, the pattern of inputs and outputs for this example would be:\n\nDesign, by hand, the weights and biases for an RNN which has two input units, three hidden units, and one output unit, which implements binary addition as discussed above. All of the units use the hard threshold activation function (\\(f(x) = 1\\) if \\(x &gt; 0\\) and \\(0\\) otherwise). In particular, specify weight matrices \\(\\mathbf{U}\\), \\(\\mathbf{v}\\), and \\(\\mathbf{W}\\), bias vector \\(\\mathbf{b}_{\\mathbf{h}}\\), and scalar bias \\(b_y\\) for the following architecture: \\[\\begin{align*}\nh^{(t)} &= f(\\bf{W}h^{(t-1)} + \\bf{U}\\bf{x}^{(t)} + \\bf{b_h}) \\\\\ny^{(t)} &= f(\\bf{v}^T h^{(t)} + b_y)\n\\end{align*}\\]\n\nWhat are the shapes of \\(\\mathbf{U}\\), \\(\\mathbf{v}\\), \\(\\mathbf{W}\\), and \\(\\mathbf{b}_{\\mathbf{h}}\\)?\nCome up with values for \\(\\mathbf{U}\\), \\(\\mathbf{W}\\), and \\(\\mathbf{b}_{\\mathbf{h}}\\). Justify your answer. Hint: When performing binary addition, in addition to adding up two digits in a column, we need to track whether there is a digit from the previous column. We will choose one of the three units in \\(\\bf{h}^{(t)}\\), say \\(\\bf{h}_2^{(t)}\\), to represent this carry digit. You may also find it helpful to set \\(\\bf{h}_1\\) to activate if the sum of the 3 digits is at least 1, \\(\\bf{h}_2\\) to activate if the sum is at least 2, and \\(\\bf{h}_3\\) to activate if the sum is at least 3.\nCome up with the values of \\(\\bf{v}\\) and \\(b_y\\). Justify your answer."
  },
  {
    "objectID": "ex/w08/questions/rnn-sentiment-sol.html",
    "href": "ex/w08/questions/rnn-sentiment-sol.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "We will need to compute \\(h^{(t)}\\) for \\(t = 1, \\ldots, T\\). Each of this computation requires applying the weight matrices \\(W\\) and \\(T\\) once. The matrix \\(V\\) is only applied once at the end. Therefore, we need to apply \\(W\\) and \\(U\\) \\(T\\) times each and \\(V\\) once.\nThe shape of \\(U\\) is \\(d_h \\times d_x\\), the shape of \\(W\\) is \\(d_h \\times d_h\\), and the shape of \\(V\\) is \\(d_y \\times d_h\\), where \\(d_h\\) is the dimensionality of the \\(h^{(i)}\\) (i.e. \\(h^{(i)}\\in\\R^{d_h}\\)), \\(d_x\\) is the dimensionality of the inputs \\(x^{(i)}\\), and \\(d_y\\) is the dimensionality of the ouput \\(y\\).\nFor each of the \\(T\\) steps, we need to perform two matrix-vector multiplications (one for \\(Ux^{(i)}\\) and one for \\(Uh^{(i)}\\)) and two vector additions. To compute the output, we need one additional matrix-vector multiplication and one vector addition."
  },
  {
    "objectID": "ex/w08/questions/rnn-scalar.html",
    "href": "ex/w08/questions/rnn-scalar.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "Suppose we have the following vanilla RNN network, where the inputs and hidden units are scalars. \\[\\begin{align*}\nh^{(t)} &= \\tanh\\li(w \\cdot h^{(t-1)} + u \\cdot x^{(t-1)} + b_h\\ri) \\\\\ny &= \\sigma\\li(v \\cdot h^{(T)} + b_y\\ri)\n\\end{align*}\\]\n\nShow that if \\(|w| &lt; 1\\), and the number of time steps \\(T\\) is large, then the gradient \\(\\frac{\\partial y}{\\partial x^{(0)}}\\) vanishes.\nWhy is the result from Part (a) troubling?"
  },
  {
    "objectID": "ex/w08/exercises08.html",
    "href": "ex/w08/exercises08.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "Suppose we are training a vanilla RNN like below to determine whether a sentence expresses positive or negative sentiment. This RNN will be a character-level RNN where \\(x^{(1)}, \\ldots, x^{(T)}\\) is the sequence of input characters. The RNN is given as follows: \\[\\begin{align*}\nh^{(t)} &= \\tanh\\li(U x^{(t)} + W h^{(t-1)} + b\\ri) \\\\\ny &= \\sigma\\li(V h^{(T)} + d\\ri)\n\\end{align*}\\]\n\nHow many times do we need to apply the weight matrix \\(U\\), \\(W\\), and \\(V\\)?\nWhat are the shapes of the matrices \\(U\\), \\(W\\), and \\(V\\)?\nHow many addition and multiplication operations are required to make a prediction? You can assume that no addition and multiplications are performed when applying the tanh and sigmoid activation functions."
  },
  {
    "objectID": "ex/w08/exercises08.html#exercise-1---rnn-for-sentiment-analysis",
    "href": "ex/w08/exercises08.html#exercise-1---rnn-for-sentiment-analysis",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "Suppose we are training a vanilla RNN like below to determine whether a sentence expresses positive or negative sentiment. This RNN will be a character-level RNN where \\(x^{(1)}, \\ldots, x^{(T)}\\) is the sequence of input characters. The RNN is given as follows: \\[\\begin{align*}\nh^{(t)} &= \\tanh\\li(U x^{(t)} + W h^{(t-1)} + b\\ri) \\\\\ny &= \\sigma\\li(V h^{(T)} + d\\ri)\n\\end{align*}\\]\n\nHow many times do we need to apply the weight matrix \\(U\\), \\(W\\), and \\(V\\)?\nWhat are the shapes of the matrices \\(U\\), \\(W\\), and \\(V\\)?\nHow many addition and multiplication operations are required to make a prediction? You can assume that no addition and multiplications are performed when applying the tanh and sigmoid activation functions."
  },
  {
    "objectID": "ex/w08/exercises08.html#exercise-2---scalar-rnn",
    "href": "ex/w08/exercises08.html#exercise-2---scalar-rnn",
    "title": "CSC477 - Fall 2024",
    "section": "Exercise 2 - Scalar RNN",
    "text": "Exercise 2 - Scalar RNN\n\nSuppose we have the following vanilla RNN network, where the inputs and hidden units are scalars. \\[\\begin{align*}\nh^{(t)} &= \\tanh\\li(w \\cdot h^{(t-1)} + u \\cdot x^{(t-1)} + b_h\\ri) \\\\\ny &= \\sigma\\li(v \\cdot h^{(T)} + b_y\\ri)\n\\end{align*}\\]\n\nShow that if \\(|w| &lt; 1\\), and the number of time steps \\(T\\) is large, then the gradient \\(\\frac{\\partial y}{\\partial x^{(0)}}\\) vanishes.\nWhy is the result from Part (a) troubling?"
  },
  {
    "objectID": "ex/w08/exercises08.html#exercise-3---rnn-addition",
    "href": "ex/w08/exercises08.html#exercise-3---rnn-addition",
    "title": "CSC477 - Fall 2024",
    "section": "Exercise 3 - RNN Addition",
    "text": "Exercise 3 - RNN Addition\n\nIn this problem, you will implement a recurrent neural network which implements binary addition. The inputs are given as binary sequences, starting with the significant binary digit. (It is easier to start from the least significant bit, just like how you did addition in grade school.) The sequences will be padded with at least one zero as the most significant digit, so that the output length is the same as the input length. For example, the problem \\(100111 + 110010\\), whose target output value is \\(1011001\\), will be represented as follows: \\[\\begin{align*}\n\\bf{x}^{(1)} = \\begin{bmatrix}1 \\\\ 0\\end{bmatrix},\n\\bf{x}^{(2)} = \\begin{bmatrix}1 \\\\ 1\\end{bmatrix},\n\\bf{x}^{(3)} = \\begin{bmatrix}1 \\\\ 0\\end{bmatrix},\n\\bf{x}^{(4)} = \\begin{bmatrix}0 \\\\ 0\\end{bmatrix},\n\\bf{x}^{(5)} = \\begin{bmatrix}0 \\\\ 1\\end{bmatrix},\n\\bf{x}^{(6)} = \\begin{bmatrix}1 \\\\ 1\\end{bmatrix},\n\\bf{x}^{(7)} = \\begin{bmatrix}0 \\\\ 0\\end{bmatrix}\n\\end{align*}\\]\nWith the target output: \\[\\begin{align*}\ny^{(1)} = 1,\ny^{(2)} = 0,\ny^{(3)} = 0,\ny^{(4)} = 1,\ny^{(5)} = 1,\ny^{(6)} = 0,\ny^{(7)} = 1,\n\\end{align*}\\]\nThere are two input units corresponding to the two inputs, and one output unit. Therefore, the pattern of inputs and outputs for this example would be:\n\nDesign, by hand, the weights and biases for an RNN which has two input units, three hidden units, and one output unit, which implements binary addition as discussed above. All of the units use the hard threshold activation function (\\(f(x) = 1\\) if \\(x &gt; 0\\) and \\(0\\) otherwise). In particular, specify weight matrices \\(\\mathbf{U}\\), \\(\\mathbf{v}\\), and \\(\\mathbf{W}\\), bias vector \\(\\mathbf{b}_{\\mathbf{h}}\\), and scalar bias \\(b_y\\) for the following architecture: \\[\\begin{align*}\nh^{(t)} &= f(\\bf{W}h^{(t-1)} + \\bf{U}\\bf{x}^{(t)} + \\bf{b_h}) \\\\\ny^{(t)} &= f(\\bf{v}^T h^{(t)} + b_y)\n\\end{align*}\\]\n\nWhat are the shapes of \\(\\mathbf{U}\\), \\(\\mathbf{v}\\), \\(\\mathbf{W}\\), and \\(\\mathbf{b}_{\\mathbf{h}}\\)?\nCome up with values for \\(\\mathbf{U}\\), \\(\\mathbf{W}\\), and \\(\\mathbf{b}_{\\mathbf{h}}\\). Justify your answer. Hint: When performing binary addition, in addition to adding up two digits in a column, we need to track whether there is a digit from the previous column. We will choose one of the three units in \\(\\bf{h}^{(t)}\\), say \\(\\bf{h}_2^{(t)}\\), to represent this carry digit. You may also find it helpful to set \\(\\bf{h}_1\\) to activate if the sum of the 3 digits is at least 1, \\(\\bf{h}_2\\) to activate if the sum is at least 2, and \\(\\bf{h}_3\\) to activate if the sum is at least 3.\nCome up with the values of \\(\\bf{v}\\) and \\(b_y\\). Justify your answer."
  },
  {
    "objectID": "ex/w09/questions/attn-transformers-sol.html",
    "href": "ex/w09/questions/attn-transformers-sol.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "The softmax function is applied row-wise and the shape of the result is \\(n_q\\times n_k\\). One way to see this is by looking at the shape of the dot product \\(QK^\\top\\) which is \\(n_q\\times n_k\\). Each row represents the pre-softmax scores of all keys and a given query. Because we need to normalize our attention weights per query, the normalization happens along the rows.\nThe value of \\(d\\) is \\(d_k\\). It is needed to scale the dot product so that the gradient of the softmax function does not vanish.\nTo obtain the computational complexity, let’s look at all the operations individually:\n\n\\(QK^\\top\\) requires \\(n_q n_k d_k\\) multiplications and \\(n_qn_k(d_k-1)\\) additions.\nDividing by \\(\\sqrt{d_k}\\) needs to be carried out \\(n_q n_k\\) times.\nApplying the softmax function can be implemented in \\(n_q n_k\\) divisions and \\(n_q(n_k-1)\\) additions.\nThe final matrix multiplication requires \\(n_qd_vn_k\\) multiplications and \\(n_q d_v (n_k-1)\\) additions.\n\nThe masking matrix is a triangular matrix with \\(-\\infty\\) on its top right half. This results in softmax weights being \\(0\\) for all key-query combinations to which \\(-\\infty\\) is added."
  },
  {
    "objectID": "ex/w09/questions/attn-transformers-notes.html",
    "href": "ex/w09/questions/attn-transformers-notes.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "import torch\nimport torch.nn.functional as F\n\n\nQ = torch.tensor([[1, 2], [3, 1]]).float()\n\nK = torch.tensor([[2, 1], [1, 1], [0, 1]]).float()\n\nV = torch.tensor([[1, 2, -2], [1, 1, 2], [0, 1, -1]]).float()\n\n\nd_k = torch.tensor(K.shape[1])\nM = torch.matmul(Q, K.transpose(0, 1)) / torch.sqrt(d_k)\nS = torch.exp(M) / torch.sum(torch.exp(M), dim=1).view(-1,1)\ntorch.matmul(S, V)\n\ntensor([[ 0.8600,  1.5760, -0.7240],\n        [ 0.9873,  1.8816, -1.5646]])\n\n\nLazy version\n\nF.scaled_dot_product_attention(Q, K, V)\n\ntensor([[ 0.8600,  1.5760, -0.7240],\n        [ 0.9873,  1.8816, -1.5646]])"
  },
  {
    "objectID": "ex/w09/questions/attn-transformers_by_hand.html",
    "href": "ex/w09/questions/attn-transformers_by_hand.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "Consider the matrices \\(Q\\), \\(K\\), \\(V\\) given by \\[\nQ = \\bpmat\n1 & 2\\\\\n3 & 1\n\\epmat,\\quad\nK = \\bpmat\n2 & 1\\\\\n1 & 1\\\\\n0 & 1\n\\epmat,\\quad\nV=\\bpmat\n1 & 2 & -2\\\\\n1 & 1 & 2 \\\\\n0 & 1 & -1\n\\epmat.\n\\] Compute the context matrix \\(C\\) using the scaled dot product attention."
  },
  {
    "objectID": "ex/w09/questions/attn-transformers_by_hand-notes.html",
    "href": "ex/w09/questions/attn-transformers_by_hand-notes.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "import torch\nimport torch.nn.functional as F\n\n\nQ = torch.tensor([[1, 2], [3, 1]]).float()\n\nK = torch.tensor([[2, 1], [1, 1], [0, 1]]).float()\n\nV = torch.tensor([[1, 2, -2], [1, 1, 2], [0, 1, -1]]).float()\n\n\nd_k = torch.tensor(K.shape[1])\nM = torch.matmul(Q, K.transpose(0, 1)) / torch.sqrt(d_k)\nS = torch.exp(M) / torch.sum(torch.exp(M), dim=1).view(-1,1)\ntorch.matmul(S, V)\n\ntensor([[ 0.8600,  1.5760, -0.7240],\n        [ 0.9873,  1.8816, -1.5646]])\n\n\nLazy version\n\nF.scaled_dot_product_attention(Q, K, V)\n\ntensor([[ 0.8600,  1.5760, -0.7240],\n        [ 0.9873,  1.8816, -1.5646]])"
  },
  {
    "objectID": "ex/w09/questions/attn-transformers.html",
    "href": "ex/w09/questions/attn-transformers.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "Transformers use a scaled dot product attention mechanism given by \\[\nC\n= \\text{attention}(Q, K, V)\n= \\text{softmax}\\left(\\fr{QK^\\top}{\\sqrt{d}}\\right) V,\n\\] where \\(Q\\in\\R^{n_q\\times d_k}\\), \\(K\\in\\R^{n_k\\times d_k}\\), \\(V\\in\\R^{n_k\\times d_v}\\).\n\nIs the softmax function here applied row-wise or column-wise? What is the shape of the result?\nWhat is the value of \\(d\\)? Why is it needed?\nWhat is the computational complexity of this attention mechanism? How many additions and multiplications are required? Assume the canonical matrix multiplcation and not counting \\(\\exp(x)\\) towards computational cost.\nIn the masked variant of the module, a masking matrix is added before the softmax function is applied. What are its values and its shape? For simplicity, assume \\(n_q=n_k\\)."
  },
  {
    "objectID": "ex/w09/exercises09.html",
    "href": "ex/w09/exercises09.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "You are given a set of vectors \\[\n\\fh_1 = (1,2,3)^\\top,\\quad\n\\fh_2 = (1,2,1)^\\top,\\quad\n\\fh_3 = (0,1,-1)^\\top\n\\] and an alignment source vector \\(\\fs=(1,2,1)^\\top\\). Compute the resulting dot-product attention weights \\(\\alpha_i\\) for \\(i=1,2,3\\) and the resulting context vector \\(\\fc\\)."
  },
  {
    "objectID": "ex/w09/exercises09.html#exercise-1---dot-product-attention",
    "href": "ex/w09/exercises09.html#exercise-1---dot-product-attention",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "You are given a set of vectors \\[\n\\fh_1 = (1,2,3)^\\top,\\quad\n\\fh_2 = (1,2,1)^\\top,\\quad\n\\fh_3 = (0,1,-1)^\\top\n\\] and an alignment source vector \\(\\fs=(1,2,1)^\\top\\). Compute the resulting dot-product attention weights \\(\\alpha_i\\) for \\(i=1,2,3\\) and the resulting context vector \\(\\fc\\)."
  },
  {
    "objectID": "ex/w09/exercises09.html#exercise-2---attention-in-transformers",
    "href": "ex/w09/exercises09.html#exercise-2---attention-in-transformers",
    "title": "CSC477 - Fall 2024",
    "section": "Exercise 2 - Attention in Transformers",
    "text": "Exercise 2 - Attention in Transformers\nTransformers use a scaled dot product attention mechanism given by \\[\nC\n= \\text{attention}(Q, K, V)\n= \\text{softmax}\\left(\\fr{QK^\\top}{\\sqrt{d}}\\right) V,\n\\] where \\(Q\\in\\R^{n_q\\times d_k}\\), \\(K\\in\\R^{n_k\\times d_k}\\), \\(V\\in\\R^{n_k\\times d_v}\\).\n\nIs the softmax function here applied row-wise or column-wise? What is the shape of the result?\nWhat is the value of \\(d\\)? Why is it needed?\nWhat is the computational complexity of this attention mechanism? How many additions and multiplications are required? Assume the canonical matrix multiplcation and not counting \\(\\exp(x)\\) towards computational cost.\nIn the masked variant of the module, a masking matrix is added before the softmax function is applied. What are its values and its shape? For simplicity, assume \\(n_q=n_k\\)."
  },
  {
    "objectID": "ex/w09/exercises09.html#exercise-3---scaled-dot-product-attention-by-hand",
    "href": "ex/w09/exercises09.html#exercise-3---scaled-dot-product-attention-by-hand",
    "title": "CSC477 - Fall 2024",
    "section": "Exercise 3 - Scaled Dot-Product Attention by Hand",
    "text": "Exercise 3 - Scaled Dot-Product Attention by Hand\nConsider the matrices \\(Q\\), \\(K\\), \\(V\\) given by \\[\nQ = \\bpmat\n1 & 2\\\\\n3 & 1\n\\epmat,\\quad\nK = \\bpmat\n2 & 1\\\\\n1 & 1\\\\\n0 & 1\n\\epmat,\\quad\nV=\\bpmat\n1 & 2 & -2\\\\\n1 & 1 & 2 \\\\\n0 & 1 & -1\n\\epmat.\n\\] Compute the context matrix \\(C\\) using the scaled dot product attention."
  },
  {
    "objectID": "ex/w07/questions/cnn-by_hand.html",
    "href": "ex/w07/questions/cnn-by_hand.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "Consider the following \\(4\\times 4 \\times 1\\) input X and a \\(2\\times 2 \\times 1\\) convolutional kernel K with no bias term\n\\[\nX =\n\\bpmat\n1 & 2 & -1 & 1 \\\\\n1 & 0 & 1 & 0 \\\\\n0 & 1 & 0 & 2 \\\\\n2 & 1 & 0 & -1\n\\epmat, \\qquad\n%\nK = \\bpmat\n1, & 0 \\\\\n2, & 1 \\\\\n\\epmat\n\\]\n\nWhat is the output of the convolutional layer for the case of stride 1 and no padding?\nWhat if we have stride 2 and no padding?\nWhat if we have stride 2 and zero-padding of size 1?"
  },
  {
    "objectID": "ex/w07/questions/opt-influence_functions-sol.html",
    "href": "ex/w07/questions/opt-influence_functions-sol.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "First, let’s recall the definitions of \\(\\te\\) and \\(\\hte(\\epsilon)\\): \\[\\begin{align*}\n\\hat{\\theta}\n&= \\text{argmin}_{\\theta} \\frac{1}{N} \\left[ \\sum_{i=1}^{N} L(x_i, y_i; \\theta) \\right] \\\\\n\\hte({\\epsilon})\n  &= \\text{argmin}_{\\theta} \\frac{1}{N} \\left[ \\sum_{i=1}^{N} L(x_i, y_i; \\theta) \\right] + \\epsilon L(x,y; \\theta)\n\\end{align*}\\] The first order taylor series expansion around \\(\\epsilon = 0\\) is given by \\[\n\\hte(0) + \\epsilon \\fr{d\\hte(\\epsilon)}{d\\epsilon} {\\Bigr |}_{\\epsilon=0}\n\\] From the definitions above, it can directly be seen that \\(\\hte(0) = \\hte\\) which completes the proof."
  },
  {
    "objectID": "ex/w07/questions/prob-mle_exp_dist-sol.html",
    "href": "ex/w07/questions/prob-mle_exp_dist-sol.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "First, let’s quickly remember that the maximum likelihood estimator (MLE) of a probability distribution from dataapoints \\(\\fx_1, \\ldots, \\fx_N\\) is given by \\[\n\\hte_{\\mathrm{MLE}} = \\argmax_{\\te \\in \\Te} \\prod_{i=1}^N f(\\fx_i | \\te),\n\\] where \\(f\\) is the probability density function of the considered probability distribution family, \\(\\te\\) are the parameters of the distribution, and \\(\\Te\\) is the parameter space (a set containing all possible parameters).\nAs mentioned in our previous exercise, we usually work with the log-likelihood in practice. In this particular case, the log likelihood is given by \\[\\begin{aligned}\nl(\\la | x_1, \\ldots, x_N)\n  & := \\sum_{i=1}^N \\ln f(\\fx_i | \\la) \\\\\n  & = \\sum_{i=1}^N \\ln\\li(\\la \\exp(-\\la x_i) \\ri) \\\\\n  & = \\sum_{i=1}^N \\ln(\\la) + \\ln\\li( \\exp(-\\la x_i) \\ri) \\\\\n  & =  N \\ln(\\la) - \\sum_{i=1}^N \\la x_i .\n\\end{aligned}\\] The derivative with respect to \\(\\la\\) is \\[\n\\fr{\\partial l(\\la | x_1, \\ldots, x_N)}{\\partial \\la}\n  = \\fr{N}{\\la} - \\sum_{i=1}^N x_i .\n\\] The MLE is obtained by setting it to 0 and solving for \\(\\la\\) as \\[\n\\hla_{MLE} = N \\li( \\sum_{i=1}^N x_i\\ri)^{-1}.\n\\]"
  },
  {
    "objectID": "ex/w07/questions/opt-influence_functions.html",
    "href": "ex/w07/questions/opt-influence_functions.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "Let \\(\\hte\\) and \\(\\hte(\\ve)\\) be as defined in class. Show that the first order Taylor expansion of \\(\\hte(\\ve)\\) around \\(\\ve=0\\) is given by the equation given in class, i.e. by \\[\\begin{align*}\n\\hat{\\theta}({\\epsilon}) \\approx \\hat{\\theta} + \\epsilon\\frac{d\\hat{\\theta}(\\epsilon)}{d\\epsilon} {\\Bigr |}_{\\epsilon=0} .\n\\end{align*}\\]"
  },
  {
    "objectID": "ex/w07/exercises07_solution.html",
    "href": "ex/w07/exercises07_solution.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "The exercises this week involve some old material so you can check your learning and understanding."
  },
  {
    "objectID": "ex/w07/exercises07_solution.html#exercise-1---maximum-likelihood-estimator",
    "href": "ex/w07/exercises07_solution.html#exercise-1---maximum-likelihood-estimator",
    "title": "CSC477 - Fall 2024",
    "section": "Exercise 1 - Maximum Likelihood Estimator",
    "text": "Exercise 1 - Maximum Likelihood Estimator\nAssume you are given datapoints \\((x_i)_{i=1}^N\\) with \\(x_i\\in\\R\\) coming from a Exponential distribution. The probability density function of a exponential distribution is given by \\(f(x) = \\la \\exp(-\\la x)\\) with \\(x\\in\\R\\). Derive the maximum likelihood estimator of the parameter \\(\\la\\).\n\nSolution\nFirst, let’s quickly remember that the maximum likelihood estimator (MLE) of a probability distribution from dataapoints \\(\\fx_1, \\ldots, \\fx_N\\) is given by \\[\n\\hte_{\\mathrm{MLE}} = \\argmax_{\\te \\in \\Te} \\prod_{i=1}^N f(\\fx_i | \\te),\n\\] where \\(f\\) is the probability density function of the considered probability distribution family, \\(\\te\\) are the parameters of the distribution, and \\(\\Te\\) is the parameter space (a set containing all possible parameters).\nAs mentioned in our previous exercise, we usually work with the log-likelihood in practice. In this particular case, the log likelihood is given by \\[\\begin{aligned}\nl(\\la | x_1, \\ldots, x_N)\n  & := \\sum_{i=1}^N \\ln f(\\fx_i | \\la) \\\\\n  & = \\sum_{i=1}^N \\ln\\li(\\la \\exp(-\\la x_i) \\ri) \\\\\n  & = \\sum_{i=1}^N \\ln(\\la) + \\ln\\li( \\exp(-\\la x_i) \\ri) \\\\\n  & =  N \\ln(\\la) - \\sum_{i=1}^N \\la x_i .\n\\end{aligned}\\] The derivative with respect to \\(\\la\\) is \\[\n\\fr{\\partial l(\\la | x_1, \\ldots, x_N)}{\\partial \\la}\n  = \\fr{N}{\\la} - \\sum_{i=1}^N x_i .\n\\] The MLE is obtained by setting it to 0 and solving for \\(\\la\\) as \\[\n\\hla_{MLE} = N \\li( \\sum_{i=1}^N x_i\\ri)^{-1}.\n\\]"
  },
  {
    "objectID": "ex/w07/exercises07_solution.html#exercise-2---convolutional-layers",
    "href": "ex/w07/exercises07_solution.html#exercise-2---convolutional-layers",
    "title": "CSC477 - Fall 2024",
    "section": "Exercise 2 - Convolutional Layers",
    "text": "Exercise 2 - Convolutional Layers\nConsider the following \\(4\\times 4 \\times 1\\) input X and a \\(2\\times 2 \\times 1\\) convolutional kernel K with no bias term\n\\[\nX =\n\\bpmat\n1 & 2 & -1 & 1 \\\\\n1 & 0 & 1 & 0 \\\\\n0 & 1 & 0 & 2 \\\\\n2 & 1 & 0 & -1\n\\epmat, \\qquad\n%\nK = \\bpmat\n1, & 0 \\\\\n2, & 1 \\\\\n\\epmat\n\\]\n\nWhat is the output of the convolutional layer for the case of stride 1 and no padding?\nWhat if we have stride 2 and no padding?\nWhat if we have stride 2 and zero-padding of size 1?\n\n\nSolution\n\nHere, we simply apply the convolutional kernel over each \\(2\\times 2\\) patch of the input. There are 9 such patches. The output \\(Y\\) is then\n\n\\[\nY = \\bpmat\n3 &  3 &  1 \\\\\n2 &  2 &  3 \\\\\n5 &  3 & -1\n\\epmat\n\\]\n\nSame idea except that we skip every other patch resulting in only 4 patches. The output \\(Y\\) is then\n\n\\[\nY = \\bpmat\n3 & 1 \\\\\n5 & -1\n\\epmat\n\\]\n\nNow, we have added zeros on each side of the input. The resulting \\(6\\times 6\\) padded input \\(X_\\mathrm{padded}\\) and corresponding output \\(Y\\) are\n\n\\[\nX_\\mathrm{padded} =\n\\bpmat\n0 & 0 & 0 &  0 &  0 & 0 \\\\\n0 & 1 & 2 & -1 &  1 & 0 \\\\\n0 & 1 & 0 &  1 &  0 & 0 \\\\\n0 & 0 & 1 &  0 &  2 & 0 \\\\\n0 & 2 & 1 &  0 & -1 & 0 \\\\\n0 & 0 & 0 &  0 &  0 & 0\n\\epmat,\n\\qquad\nY = \\bpmat\n1 & 3 & 2 \\\\\n0 & 2 & 4 \\\\\n0 & 1 & -1\n\\epmat\n\\]"
  },
  {
    "objectID": "ex/w07/exercises07_solution.html#exercise-3---computational-parameter-counting",
    "href": "ex/w07/exercises07_solution.html#exercise-3---computational-parameter-counting",
    "title": "CSC477 - Fall 2024",
    "section": "Exercise 3 - Computational Parameter Counting",
    "text": "Exercise 3 - Computational Parameter Counting\nUse PyTorch to load the vgg11 model and automatically compute its number of parameters. Output the number of parameters for each layer and the total number of parameters in the model.\n\nSolution\nFirst, we hvae to load the vgg11 model which is part of torchvision as has been shown in the lecture:\nimport torchvision\nvgg11 = torchvision.models.vgg.vgg11(pretrained=False)\nThe number of parameters for the entire model, is the easier part: We can simply use the parameters() iterator which returns the set of parameters for each module. Those can then be counted using the numel() method resulting in\nsum(p.numel() for p in vgg11.parameters())\nObtaining the number of parameters for each of the layer requires looking into the source code of the vgg11 model. All VGG models are ultimately instantiated by using the VGG class. Its forward pass looks like this:\nx = self.features(x)\nx = self.avgpool(x)\nx = torch.flatten(x, 1)\nx = self.classifier(x)\nA closer look at the implementation reveals that we can obtain the individual layers parameter by simply iterating over the self.features and self.classifier modules. The self.avgpool module does not have any parameters. The following code snippet shows how to obtain the number of parameters for each layer of the convolutional backbone:\nfor layer in vgg11.features:\n    print(layer, sum(p.numel() for p in layer.parameters()))\nTo get the number of paramers in the cnn head, simply update the code snippet to iterate over vgg11.classifier instead of vgg11.features."
  },
  {
    "objectID": "ex/w07/exercises07_solution.html#exercise-4---influence-functions",
    "href": "ex/w07/exercises07_solution.html#exercise-4---influence-functions",
    "title": "CSC477 - Fall 2024",
    "section": "Exercise 4 - Influence Functions",
    "text": "Exercise 4 - Influence Functions\nLet \\(\\hte\\) and \\(\\hte(\\ve)\\) be as defined in class. Show that the first order Taylor expansion of \\(\\hte(\\ve)\\) around \\(\\ve=0\\) is given by the equation given in class, i.e. by \\[\\begin{align*}\n\\hat{\\theta}({\\epsilon}) \\approx \\hat{\\theta} + \\epsilon\\frac{d\\hat{\\theta}(\\epsilon)}{d\\epsilon} {\\Bigr |}_{\\epsilon=0} .\n\\end{align*}\\]\n\nSolution\nFirst, let’s recall the definitions of \\(\\te\\) and \\(\\hte(\\epsilon)\\): \\[\\begin{align*}\n\\hat{\\theta}\n&= \\text{argmin}_{\\theta} \\frac{1}{N} \\left[ \\sum_{i=1}^{N} L(x_i, y_i; \\theta) \\right] \\\\\n\\hte({\\epsilon})\n  &= \\text{argmin}_{\\theta} \\frac{1}{N} \\left[ \\sum_{i=1}^{N} L(x_i, y_i; \\theta) \\right] + \\epsilon L(x,y; \\theta)\n\\end{align*}\\] The first order taylor series expansion around \\(\\epsilon = 0\\) is given by \\[\n\\hte(0) + \\epsilon \\fr{d\\hte(\\epsilon)}{d\\epsilon} {\\Bigr |}_{\\epsilon=0}\n\\] From the definitions above, it can directly be seen that \\(\\hte(0) = \\hte\\) which completes the proof."
  },
  {
    "objectID": "ex/w10/questions/tconv-params.html",
    "href": "ex/w10/questions/tconv-params.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "What is the number of learnable parameters for each of the following transposed convolution layers defined in PyTorch. Try to calculate those by hand first and use pytorch later to verify your results.\n\nnn.ConvTranspose2d(in_channels=3, out_channels=2, kernel_size=3, stride=1)\nnn.ConvTranspose2d(in_channels=3, out_channels=10, kernel_size=3, stride=2)\nnn.ConvTranspose2d(in_channels=3, out_channels=2, kernel_size=4, stride=5)\nnn.ConvTranspose2d(in_channels=3, out_channels=4, kernel_size=3, stride=23)"
  },
  {
    "objectID": "ex/w10/questions/tconv-sizes-sol.html",
    "href": "ex/w10/questions/tconv-sizes-sol.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "Output tensor size: \\(2\\times 4\\times 4\\)\nOutput tensor size: \\(4\\times 10\\times 10\\)\nThe first dimension is always the number of input channels, i.e. \\(c_{out}\\). The output for the first value of the input tensor is of size \\(3\\times 3\\). For all remaining values, the stride determines the additional values of the ouput. Stride 2 means, that we need two values for each additional value. Thus, the output height \\(h_{out}\\) and width are \\(w_{out}\\): \\[\n\\begin{aligned}\nh_{out} &= 3 + (h-1)\\cdot 2 = 2h+1\\\\\nw_{out} &= 3 + (w-1)\\cdot 2 = 2w+1\\\\\n\\end{aligned}\n\\] Thus, the resulting size is \\(2\\times (2h+1) \\times (2w+1)\\).\nThis generalization requires a similar consideration and we can basically reuse the equations from above: \\[\n\\begin{aligned}\nh_{out} &= h_k + (h-1)\\cdot s\\\\\nw_{out} &= w_k + (w-1)\\cdot s\\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "ex/w10/questions/tconv-by_hand-sol.html",
    "href": "ex/w10/questions/tconv-by_hand-sol.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "The resulting output tensor is of shape \\(6 \\times 6\\) and is given by: \\[\nY = \\bpmat\n1 &  0 &  0 &  0 &  2 &  0\\\\\n1 &  2 &  0 &  0 &  2 &  4\\\\\n2 &  0 &  3 &  0 &  0 &  0\\\\\n2 &  4 &  3 &  6 &  0 &  0\\\\\n-1 &  0 &  0 &  0 &  3 &  0\\\\\n-1 & -2 &  0 &  0 &  3 &  6\n\\epmat\n\\]\nThe resulting output tensor is of shape \\(4 \\times 4\\) and is given by: \\[\nY = \\bpmat\n1 &  0 &  2 &  0 \\\\\n3 &  5 &  2 &  4 \\\\\n1 &  7 &  9 &  0 \\\\\n-1 & -2 &  3 &  6\n\\epmat\n\\]\nWe can verify our answers with PyTorch with the following commands:\nnn.functional.conv_transpose2d(X.unsqueeze(0), K, stride=2)\n\nnn.functional.conv_transpose2d(X.unsqueeze(0), K)\nThe call to .unsqueeze(0) can be left out if X has a batch dimension and is already a 4D tensor.\nA potential implementaiton (that isn’t optimized in any way) of transposed convolutions looks like this:\n\n\ndef conv_transpose2d(input, weight, stride=1):\n  # input - input of shape (C_in, H, W)\n  # weight - kernel of shape (C_in, C_out, K, K)\n  # stride - stride of the transposed convolutio\n  # RETURNS\n  # output - output of shape (C_out, H_out, W_out)\n  (c_in, h_in, w_in) = X.size()\n  (c2_in, c_out, k, k2) = K.size()\n\n  assert c_in == c2_in, \"Number of input channels must match\"\n  assert k == k2, \"Kernel must be square\"\n\n  h_out = (h_in - 1) * stride + k\n  w_out = (w_in - 1) * stride + k\n  output = torch.zeros((c_out, h_out, w_out))\n\n  for c_cur_in in range(c_in):\n    for c_cur_out in range(c_out):\n      for h in range(0, h_in):\n        for w in range(0, w_in):\n          output[c_cur_out, h*stride:h*stride+k, w*stride:w*stride+k] \\\n              += weight[c_cur_in, c_cur_out,:,:] * input[c_cur_in,h,w]\n\n  return output"
  },
  {
    "objectID": "ex/w10/questions/tconv-params-sol.html",
    "href": "ex/w10/questions/tconv-params-sol.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "The number of parameters is the product of input channels, output channels and kernel size. For each output channel, there is one bias parameter (if bias=True which is default in PyTorch). The stride does not matter for the number of paramters.\n\n\\(3\\cdot 2 \\cdot 9 + 2=56\\)\n\\(3\\cdot 10 \\cdot 9 + 10=280\\)\n\\(3\\cdot 2 \\cdot 16 + 2=98\\)\n\\(3\\cdot 4 \\cdot 9 + 4=112\\)"
  },
  {
    "objectID": "ex/w10/questions/tcons-by_hand-notes.html",
    "href": "ex/w10/questions/tcons-by_hand-notes.html",
    "title": "Solutions to (a), (b), and (c)",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nX = torch.tensor(\n    [[1, 0, 2], \n     [2, 3, 0], \n     [-1, 0, 3]]).unsqueeze(0)\n\nK = torch.tensor(\n    [[1, 0],\n     [1, 2]]).unsqueeze(0).unsqueeze(0)\nnn.functional.conv_transpose2d(X.unsqueeze(0), K, stride=2)\n\ntensor([[[[ 1,  0,  0,  0,  2,  0],\n          [ 1,  2,  0,  0,  2,  4],\n          [ 2,  0,  3,  0,  0,  0],\n          [ 2,  4,  3,  6,  0,  0],\n          [-1,  0,  0,  0,  3,  0],\n          [-1, -2,  0,  0,  3,  6]]]])\nnn.functional.conv_transpose2d(X.unsqueeze(0), K, stride=1)\n\ntensor([[[[ 1,  0,  2,  0],\n          [ 3,  5,  2,  4],\n          [ 1,  7,  9,  0],\n          [-1, -2,  3,  6]]]])"
  },
  {
    "objectID": "ex/w10/questions/tcons-by_hand-notes.html#solution-to-d",
    "href": "ex/w10/questions/tcons-by_hand-notes.html#solution-to-d",
    "title": "Solutions to (a), (b), and (c)",
    "section": "Solution to (d)",
    "text": "Solution to (d)\nBelow are some computations for verification.\n\ndef conv_transpose2d(input, weight, stride=1):\n  # input - input of shape (C_in, H, W)\n  # weight - kernel of shape (C_in, C_out, K, K)\n  # stride - stride of the transposed convolutio\n  # RETURNS\n  # output - output of shape (C_out, H_out, W_out)\n  (c_in, h_in, w_in) = X.size()\n  (c2_in, c_out, k, k2) = K.size()\n\n  assert c_in == c2_in, \"Number of input channels must match\"\n  assert k == k2, \"Kernel must be square\"\n\n  h_out = (h_in - 1) * stride + k\n  w_out = (w_in - 1) * stride + k\n  output = torch.zeros((c_out, h_out, w_out))\n\n  for c_cur_in in range(c_in):\n    for c_cur_out in range(c_out):\n      for h in range(0, h_in):\n        for w in range(0, w_in):\n          output[c_cur_out, h*stride:h*stride+k, w*stride:w*stride+k] \\\n              += weight[c_cur_in, c_cur_out,:,:] * input[c_cur_in,h,w]\n\n  return output\n\n\nconv_transpose2d(X, K, stride=2)\n\ntensor([[[ 1.,  0.,  0.,  0.,  2.,  0.],\n         [ 1.,  2.,  0.,  0.,  2.,  4.],\n         [ 2.,  0.,  3.,  0.,  0.,  0.],\n         [ 2.,  4.,  3.,  6.,  0.,  0.],\n         [-1.,  0.,  0.,  0.,  3.,  0.],\n         [-1., -2.,  0.,  0.,  3.,  6.]]])\n\n\n\nconv_transpose2d(X, K, stride=1)\n\ntensor([[[ 1.,  0.,  2.,  0.],\n         [ 3.,  5.,  2.,  4.],\n         [ 1.,  7.,  9.,  0.],\n         [-1., -2.,  3.,  6.]]])\n\n\n\nX = torch.randn((2,3,3))\nK = torch.randn((2,1,2,2))\n\n\nconv_transpose2d(X, K, stride=1)\n\ntensor([[[ 2.5196, -3.0373, -1.7501,  1.1531],\n         [-0.2329, -1.2616, -1.2943,  0.5111],\n         [-1.0364, -0.8051, -2.0395, -1.4085],\n         [ 1.9276,  0.1688, -3.1863, -1.4459]]])\n\n\n\nnn.functional.conv_transpose2d(X.unsqueeze(0), K, stride=1)\n\ntensor([[[[ 2.5196, -3.0373, -1.7501,  1.1531],\n          [-0.2329, -1.2616, -1.2943,  0.5111],\n          [-1.0364, -0.8051, -2.0395, -1.4085],\n          [ 1.9276,  0.1688, -3.1863, -1.4459]]]])"
  },
  {
    "objectID": "ex/w10/exercises10_solution.html",
    "href": "ex/w10/exercises10_solution.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "What is the size of the output for a input tensor and a transposed convolutional layer if the parameters are given as follows (assume the number of channels is given in the first dimension).\n\nInput tensor size: \\(3\\times 2\\times 2\\)\nTransposed convolution: \\(3\\times 3\\) kernel, stride 1, output channels: 2\nInput tensor size: \\(3\\times 5\\times 5\\)\nTransposed convolutional: \\(2\\times 2\\) kernel, stride 2, output channels: 4\nInput tensor size: \\(c_{in}\\times h\\times w\\)\nTransposed convolutional: \\(3\\times 3\\) kernel, stride 2, output channels: 2\nInput tensor size: \\(c_{in}\\times h\\times w\\)\nTransposed convolutional: \\(h_k\\times w_k\\) kernel, stride \\(s\\), output channels: \\(c_{out}\\)\n\n\n\n\nOutput tensor size: \\(2\\times 4\\times 4\\)\nOutput tensor size: \\(4\\times 10\\times 10\\)\nThe first dimension is always the number of input channels, i.e. \\(c_{out}\\). The output for the first value of the input tensor is of size \\(3\\times 3\\). For all remaining values, the stride determines the additional values of the ouput. Stride 2 means, that we need two values for each additional value. Thus, the output height \\(h_{out}\\) and width are \\(w_{out}\\): \\[\n\\begin{aligned}\nh_{out} &= 3 + (h-1)\\cdot 2 = 2h+1\\\\\nw_{out} &= 3 + (w-1)\\cdot 2 = 2w+1\\\\\n\\end{aligned}\n\\] Thus, the resulting size is \\(2\\times (2h+1) \\times (2w+1)\\).\nThis generalization requires a similar consideration and we can basically reuse the equations from above: \\[\n\\begin{aligned}\nh_{out} &= h_k + (h-1)\\cdot s\\\\\nw_{out} &= w_k + (w-1)\\cdot s\\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "ex/w10/exercises10_solution.html#exercise-1---transposed-convolution-output-sizes",
    "href": "ex/w10/exercises10_solution.html#exercise-1---transposed-convolution-output-sizes",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "What is the size of the output for a input tensor and a transposed convolutional layer if the parameters are given as follows (assume the number of channels is given in the first dimension).\n\nInput tensor size: \\(3\\times 2\\times 2\\)\nTransposed convolution: \\(3\\times 3\\) kernel, stride 1, output channels: 2\nInput tensor size: \\(3\\times 5\\times 5\\)\nTransposed convolutional: \\(2\\times 2\\) kernel, stride 2, output channels: 4\nInput tensor size: \\(c_{in}\\times h\\times w\\)\nTransposed convolutional: \\(3\\times 3\\) kernel, stride 2, output channels: 2\nInput tensor size: \\(c_{in}\\times h\\times w\\)\nTransposed convolutional: \\(h_k\\times w_k\\) kernel, stride \\(s\\), output channels: \\(c_{out}\\)\n\n\n\n\nOutput tensor size: \\(2\\times 4\\times 4\\)\nOutput tensor size: \\(4\\times 10\\times 10\\)\nThe first dimension is always the number of input channels, i.e. \\(c_{out}\\). The output for the first value of the input tensor is of size \\(3\\times 3\\). For all remaining values, the stride determines the additional values of the ouput. Stride 2 means, that we need two values for each additional value. Thus, the output height \\(h_{out}\\) and width are \\(w_{out}\\): \\[\n\\begin{aligned}\nh_{out} &= 3 + (h-1)\\cdot 2 = 2h+1\\\\\nw_{out} &= 3 + (w-1)\\cdot 2 = 2w+1\\\\\n\\end{aligned}\n\\] Thus, the resulting size is \\(2\\times (2h+1) \\times (2w+1)\\).\nThis generalization requires a similar consideration and we can basically reuse the equations from above: \\[\n\\begin{aligned}\nh_{out} &= h_k + (h-1)\\cdot s\\\\\nw_{out} &= w_k + (w-1)\\cdot s\\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "ex/w10/exercises10_solution.html#exercise-2---transposed-convolution-parameter-sizes",
    "href": "ex/w10/exercises10_solution.html#exercise-2---transposed-convolution-parameter-sizes",
    "title": "CSC477 - Fall 2024",
    "section": "Exercise 2 - Transposed Convolution Parameter Sizes",
    "text": "Exercise 2 - Transposed Convolution Parameter Sizes\nWhat is the number of learnable parameters for each of the following transposed convolution layers defined in PyTorch. Try to calculate those by hand first and use pytorch later to verify your results.\n\nnn.ConvTranspose2d(in_channels=3, out_channels=2, kernel_size=3, stride=1)\nnn.ConvTranspose2d(in_channels=3, out_channels=10, kernel_size=3, stride=2)\nnn.ConvTranspose2d(in_channels=3, out_channels=2, kernel_size=4, stride=5)\nnn.ConvTranspose2d(in_channels=3, out_channels=4, kernel_size=3, stride=23)\n\n\nSolution\nThe number of parameters is the product of input channels, output channels and kernel size. For each output channel, there is one bias parameter (if bias=True which is default in PyTorch). The stride does not matter for the number of paramters.\n\n\\(3\\cdot 2 \\cdot 9 + 2=56\\)\n\\(3\\cdot 10 \\cdot 9 + 10=280\\)\n\\(3\\cdot 2 \\cdot 16 + 2=98\\)\n\\(3\\cdot 4 \\cdot 9 + 4=112\\)"
  },
  {
    "objectID": "ex/w10/exercises10_solution.html#exercise-3---transposed-convolution-by-hand",
    "href": "ex/w10/exercises10_solution.html#exercise-3---transposed-convolution-by-hand",
    "title": "CSC477 - Fall 2024",
    "section": "Exercise 3 - Transposed Convolution by Hand",
    "text": "Exercise 3 - Transposed Convolution by Hand\nYou are given an input matrix \\(X\\) (consisting of a single channel) and a kernel \\(K\\) as follows: \\[\nX = \\bpmat\n1 & 0 & 2 \\\\\n2 & 3 & 0 \\\\\n-1 & 0 & 3\n\\epmat, \\quad\nK = \\bpmat\n1 & 0 \\\\\n1 & 2\n\\epmat\n\\]\n\nCompute the transposed convolution by hand assuming stride 2.\nCompute the transposed convolution by hand assuming stride 1.\nUse PyTorch to verify your answers.\nImplement the transposed convolution in PyTorch without using its own implementaiton. You can assume no bias term a square kernel and no separate batch dimension. I.e. your task is to implement the following function\n\ndef conv_transpose2d(inp, weight, stride=1):\n    # inp - input of shape (C_in, H, W)\n    # weight - kernel of shape (C_in, C_out, K, K)\n    # stride - stride of the transposed convolution\n    # RETURNS\n    # output - output of shape (C_out, H_out, W_out)\n    #\n    # YOUR CODE HERE\n    return output\n\nSolution\n\nThe resulting output tensor is of shape \\(6 \\times 6\\) and is given by: \\[\nY = \\bpmat\n1 &  0 &  0 &  0 &  2 &  0\\\\\n1 &  2 &  0 &  0 &  2 &  4\\\\\n2 &  0 &  3 &  0 &  0 &  0\\\\\n2 &  4 &  3 &  6 &  0 &  0\\\\\n-1 &  0 &  0 &  0 &  3 &  0\\\\\n-1 & -2 &  0 &  0 &  3 &  6\n\\epmat\n\\]\nThe resulting output tensor is of shape \\(4 \\times 4\\) and is given by: \\[\nY = \\bpmat\n1 &  0 &  2 &  0 \\\\\n3 &  5 &  2 &  4 \\\\\n1 &  7 &  9 &  0 \\\\\n-1 & -2 &  3 &  6\n\\epmat\n\\]\nWe can verify our answers with PyTorch with the following commands:\nnn.functional.conv_transpose2d(X.unsqueeze(0), K, stride=2)\n\nnn.functional.conv_transpose2d(X.unsqueeze(0), K)\nThe call to .unsqueeze(0) can be left out if X has a batch dimension and is already a 4D tensor.\nA potential implementaiton (that isn’t optimized in any way) of transposed convolutions looks like this:\n\n\ndef conv_transpose2d(input, weight, stride=1):\n  # input - input of shape (C_in, H, W)\n  # weight - kernel of shape (C_in, C_out, K, K)\n  # stride - stride of the transposed convolutio\n  # RETURNS\n  # output - output of shape (C_out, H_out, W_out)\n  (c_in, h_in, w_in) = X.size()\n  (c2_in, c_out, k, k2) = K.size()\n\n  assert c_in == c2_in, \"Number of input channels must match\"\n  assert k == k2, \"Kernel must be square\"\n\n  h_out = (h_in - 1) * stride + k\n  w_out = (w_in - 1) * stride + k\n  output = torch.zeros((c_out, h_out, w_out))\n\n  for c_cur_in in range(c_in):\n    for c_cur_out in range(c_out):\n      for h in range(0, h_in):\n        for w in range(0, w_in):\n          output[c_cur_out, h*stride:h*stride+k, w*stride:w*stride+k] \\\n              += weight[c_cur_in, c_cur_out,:,:] * input[c_cur_in,h,w]\n\n  return output"
  },
  {
    "objectID": "ex/w11/questions/cnn-by_hand.html",
    "href": "ex/w11/questions/cnn-by_hand.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "Consider the following \\(4\\times 4 \\times 1\\) input X and a \\(2\\times 2 \\times 1\\) convolutional kernel K with no bias term\n\\[\nX =\n\\bpmat\n1 & 0 & 1 & -1 \\\\\n1 & 0 & 1 & 0 \\\\\n0 & 3 & 0 & 1 \\\\\n1 & -1 & 0 & 1\n\\epmat, \\qquad\n%\nK = \\bpmat\n1, & 2 \\\\\n0, & 1 \\\\\n\\epmat\n\\]\n\nWhat is the output of the convolutional layer for the case of stride 1 and no padding?\nWhat if we have stride 2 and no padding?\nWhat if we have stride 2 and zero-padding of size 1?"
  },
  {
    "objectID": "ex/w11/questions/pytorch-parameter_count-sol.html",
    "href": "ex/w11/questions/pytorch-parameter_count-sol.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "First, we hvae to load the alexnet model which is part of torchvision:\nimport torchvision\nalexnet = torchvision.models.alexnet()\nThe number of parameters for the entire model, is the easier part: We can simply use the parameters() iterator which returns the set of parameters for each module. Those can then be counted using the numel() method resulting in\nsum(p.numel() for p in alexnet.parameters())\nObtaining the number of parameters for each of the layer requires looking into the source code of the alexnet model. The structure is similar to vgg and the forward pass looks like this:\nx = self.features(x)\nx = self.avgpool(x)\nx = torch.flatten(x, 1)\nx = self.classifier(x)\nA closer look at the implementation reveals that we can obtain the individual layers parameter by simply iterating over the self.features and self.classifier modules. The self.avgpool module does not have any parameters. The following code snippet shows how to obtain the number of parameters for each layer of the convolutional backbone:\nfor layer in alexnet.features:\n    print(layer, sum(p.numel() for p in layer.parameters()))\nTo get the number of paramers in the cnn head, simply update the code snippet to iterate over alexnet.classifier instead of alexnet.features."
  },
  {
    "objectID": "ex/w11/questions/linalg-evs_to_mat-sol.html",
    "href": "ex/w11/questions/linalg-evs_to_mat-sol.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "First, remember that the normalized eigenvectors of a symmetric matrix are orthogonal. Thus, we have \\[\n\\fe_i^\\top \\fe_j = \\begin{cases} 1 & i=j \\\\ 0 & i\\neq j \\end{cases}.\n\\]\nSecond, for symmetric \\(\\fA\\), its spectral decomposition is given by \\(\\fA = \\fQ \\fLa \\fQ^\\top\\), where \\(\\fQ\\) is a matrix where each column is an (orthogonal) eigenvector of unit length.\nIn our case, the eigenvectors are already normalized and orthogonal, so we can simply write \\(\\fQ = (\\fe_1, \\fe_2)\\) and \\(\\fLa = \\diag(\\lambda_1, \\lambda_2)\\). Then, we have \\[\n  \\fA = \\bpmat 1.5 & -0.5 \\\\\n    -0.5 & 1.5\n    \\epmat\n  \\]"
  },
  {
    "objectID": "ex/w11/questions/attn-transformers_by_hand.html",
    "href": "ex/w11/questions/attn-transformers_by_hand.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "Consider the matrices \\(Q\\), \\(K\\), \\(V\\) given by \\[\nQ = \\bpmat\n1 & 3\\\\\n0 & 1\n\\epmat,\\quad\nK = \\bpmat\n1 & 1\\\\\n1 & 2\\\\\n0 & 1\n\\epmat,\\quad\nV=\\bpmat\n1 & 0 & -2\\\\\n2 & 1 & 2 \\\\\n0 & 3 & -1\n\\epmat.\n\\] Compute the context matrix \\(C\\) using the scaled dot product attention."
  },
  {
    "objectID": "ex/w11/questions/attn-transformers_by_hand-sol.html",
    "href": "ex/w11/questions/attn-transformers_by_hand-sol.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "The resulting context matrix is given by: \\[\nC\\approx\n\\bpmat\n1.80 & 1.00 & 1.44\\\\\n1.26 & 1.25 & 0.26\n\\epmat\n\\] A simple implementation would look as follows:\nimport torch\nQ = torch.tensor([[1, 2], [3, 1]]).float()\nK = torch.tensor([[2, 1], [1, 1], [0, 1]]).float()\nV = torch.tensor([[1, 2, -2], [1, 1, 2], [0, 1, -1]]).float()\nd_k = torch.tensor(K.shape[1])\nM = torch.matmul(Q, K.transpose(0, 1)) / torch.sqrt(d_k)\nS = torch.exp(M) / torch.sum(torch.exp(M), dim=1).view(-1,1)\ntorch.matmul(S, V)\nPytorch also provides a function for scaled dot product attention:\nimport torch.nn.functional as F\nF.scaled_dot_product_attention(Q, K, V)"
  },
  {
    "objectID": "ex/w11/questions/cnn-by_hand-sol.html",
    "href": "ex/w11/questions/cnn-by_hand-sol.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "Here, we simply apply the convolutional kernel over each \\(2\\times 2\\) patch of the input. There are 9 such patches. The output \\(Y\\) is then\n\n\\[\nY = \\bpmat\n1 &  3 &  -1 \\\\\n4 &  2 &  2 \\\\\n5 &  3 & 3\n\\epmat\n\\]\n\nSame idea except that we skip every other patch resulting in only 4 patches. The output \\(Y\\) is then\n\n\\[\nY = \\bpmat\n1 & -1 \\\\\n5 & 3\n\\epmat\n\\]\n\nNow, we have added zeros on each side of the input. The resulting \\(6\\times 6\\) padded input \\(X_\\mathrm{padded}\\) and corresponding output \\(Y\\) are\n\n\\[\nX_\\mathrm{padded} =\n\\bpmat\n0 & 0 &  0 & 0 &  0 & 0 \\\\\n0 & 1 &  0 & 1 & -1 & 0 \\\\\n0 & 1 &  0 & 1 &  0 & 0 \\\\\n0 & 0 &  3 & 0 &  1 & 0 \\\\\n0 & 1 & -1 & 0 &  1 & 0 \\\\\n0 & 0 &  0 & 0 &  0 & 0\n\\epmat,\n\\qquad\nY = \\bpmat\n1 & 1 & 0 \\\\\n2 & 2 & 0 \\\\\n2 & -1 & 1\n\\epmat\n\\]"
  },
  {
    "objectID": "ex/w11/exercises11_solution.html",
    "href": "ex/w11/exercises11_solution.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "You are given the following set of eigevalues and eigenvectors. Compute the corresponding matrix.\n\\(\\la_1 = 1\\), \\(\\la_2 = 2\\), \\(\\fv_1 = (\\sqrt{0.5}, \\sqrt{0.5})^\\top\\), \\(\\fv_2 = (\\sqrt{0.5},-\\sqrt{0.5})^\\top\\).\n\n\nFirst, remember that the normalized eigenvectors of a symmetric matrix are orthogonal. Thus, we have \\[\n\\fe_i^\\top \\fe_j = \\begin{cases} 1 & i=j \\\\ 0 & i\\neq j \\end{cases}.\n\\]\nSecond, for symmetric \\(\\fA\\), its spectral decomposition is given by \\(\\fA = \\fQ \\fLa \\fQ^\\top\\), where \\(\\fQ\\) is a matrix where each column is an (orthogonal) eigenvector of unit length.\nIn our case, the eigenvectors are already normalized and orthogonal, so we can simply write \\(\\fQ = (\\fe_1, \\fe_2)\\) and \\(\\fLa = \\diag(\\lambda_1, \\lambda_2)\\). Then, we have \\[\n  \\fA = \\bpmat 1.5 & -0.5 \\\\\n    -0.5 & 1.5\n    \\epmat\n  \\]"
  },
  {
    "objectID": "ex/w11/exercises11_solution.html#exercise-1---eigenvalues-and-eigenvectors",
    "href": "ex/w11/exercises11_solution.html#exercise-1---eigenvalues-and-eigenvectors",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "You are given the following set of eigevalues and eigenvectors. Compute the corresponding matrix.\n\\(\\la_1 = 1\\), \\(\\la_2 = 2\\), \\(\\fv_1 = (\\sqrt{0.5}, \\sqrt{0.5})^\\top\\), \\(\\fv_2 = (\\sqrt{0.5},-\\sqrt{0.5})^\\top\\).\n\n\nFirst, remember that the normalized eigenvectors of a symmetric matrix are orthogonal. Thus, we have \\[\n\\fe_i^\\top \\fe_j = \\begin{cases} 1 & i=j \\\\ 0 & i\\neq j \\end{cases}.\n\\]\nSecond, for symmetric \\(\\fA\\), its spectral decomposition is given by \\(\\fA = \\fQ \\fLa \\fQ^\\top\\), where \\(\\fQ\\) is a matrix where each column is an (orthogonal) eigenvector of unit length.\nIn our case, the eigenvectors are already normalized and orthogonal, so we can simply write \\(\\fQ = (\\fe_1, \\fe_2)\\) and \\(\\fLa = \\diag(\\lambda_1, \\lambda_2)\\). Then, we have \\[\n  \\fA = \\bpmat 1.5 & -0.5 \\\\\n    -0.5 & 1.5\n    \\epmat\n  \\]"
  },
  {
    "objectID": "ex/w11/exercises11_solution.html#exercise-2---parameter-counting",
    "href": "ex/w11/exercises11_solution.html#exercise-2---parameter-counting",
    "title": "CSC477 - Fall 2024",
    "section": "Exercise 2 - Parameter Counting",
    "text": "Exercise 2 - Parameter Counting\nUse PyTorch to load the alexnet model and automatically compute its number of parameters. Output the number of parameters for each layer and the total number of parameters in the model.\n\nSolution\nFirst, we hvae to load the alexnet model which is part of torchvision:\nimport torchvision\nalexnet = torchvision.models.alexnet()\nThe number of parameters for the entire model, is the easier part: We can simply use the parameters() iterator which returns the set of parameters for each module. Those can then be counted using the numel() method resulting in\nsum(p.numel() for p in alexnet.parameters())\nObtaining the number of parameters for each of the layer requires looking into the source code of the alexnet model. The structure is similar to vgg and the forward pass looks like this:\nx = self.features(x)\nx = self.avgpool(x)\nx = torch.flatten(x, 1)\nx = self.classifier(x)\nA closer look at the implementation reveals that we can obtain the individual layers parameter by simply iterating over the self.features and self.classifier modules. The self.avgpool module does not have any parameters. The following code snippet shows how to obtain the number of parameters for each layer of the convolutional backbone:\nfor layer in alexnet.features:\n    print(layer, sum(p.numel() for p in layer.parameters()))\nTo get the number of paramers in the cnn head, simply update the code snippet to iterate over alexnet.classifier instead of alexnet.features."
  },
  {
    "objectID": "ex/w11/exercises11_solution.html#exercise-3---convolutional-layers",
    "href": "ex/w11/exercises11_solution.html#exercise-3---convolutional-layers",
    "title": "CSC477 - Fall 2024",
    "section": "Exercise 3 - Convolutional Layers",
    "text": "Exercise 3 - Convolutional Layers\nConsider the following \\(4\\times 4 \\times 1\\) input X and a \\(2\\times 2 \\times 1\\) convolutional kernel K with no bias term\n\\[\nX =\n\\bpmat\n1 & 0 & 1 & -1 \\\\\n1 & 0 & 1 & 0 \\\\\n0 & 3 & 0 & 1 \\\\\n1 & -1 & 0 & 1\n\\epmat, \\qquad\n%\nK = \\bpmat\n1, & 2 \\\\\n0, & 1 \\\\\n\\epmat\n\\]\n\nWhat is the output of the convolutional layer for the case of stride 1 and no padding?\nWhat if we have stride 2 and no padding?\nWhat if we have stride 2 and zero-padding of size 1?\n\n\nSolution\n\nHere, we simply apply the convolutional kernel over each \\(2\\times 2\\) patch of the input. There are 9 such patches. The output \\(Y\\) is then\n\n\\[\nY = \\bpmat\n1 &  3 &  -1 \\\\\n4 &  2 &  2 \\\\\n5 &  3 & 3\n\\epmat\n\\]\n\nSame idea except that we skip every other patch resulting in only 4 patches. The output \\(Y\\) is then\n\n\\[\nY = \\bpmat\n1 & -1 \\\\\n5 & 3\n\\epmat\n\\]\n\nNow, we have added zeros on each side of the input. The resulting \\(6\\times 6\\) padded input \\(X_\\mathrm{padded}\\) and corresponding output \\(Y\\) are\n\n\\[\nX_\\mathrm{padded} =\n\\bpmat\n0 & 0 &  0 & 0 &  0 & 0 \\\\\n0 & 1 &  0 & 1 & -1 & 0 \\\\\n0 & 1 &  0 & 1 &  0 & 0 \\\\\n0 & 0 &  3 & 0 &  1 & 0 \\\\\n0 & 1 & -1 & 0 &  1 & 0 \\\\\n0 & 0 &  0 & 0 &  0 & 0\n\\epmat,\n\\qquad\nY = \\bpmat\n1 & 1 & 0 \\\\\n2 & 2 & 0 \\\\\n2 & -1 & 1\n\\epmat\n\\]"
  },
  {
    "objectID": "ex/w11/exercises11_solution.html#exercise-4---scaled-dot-product-attention",
    "href": "ex/w11/exercises11_solution.html#exercise-4---scaled-dot-product-attention",
    "title": "CSC477 - Fall 2024",
    "section": "Exercise 4 - Scaled Dot-Product Attention",
    "text": "Exercise 4 - Scaled Dot-Product Attention\nConsider the matrices \\(Q\\), \\(K\\), \\(V\\) given by \\[\nQ = \\bpmat\n1 & 3\\\\\n0 & 1\n\\epmat,\\quad\nK = \\bpmat\n1 & 1\\\\\n1 & 2\\\\\n0 & 1\n\\epmat,\\quad\nV=\\bpmat\n1 & 0 & -2\\\\\n2 & 1 & 2 \\\\\n0 & 3 & -1\n\\epmat.\n\\] Compute the context matrix \\(C\\) using the scaled dot product attention.\n\nSolution\nThe resulting context matrix is given by: \\[\nC\\approx\n\\bpmat\n1.80 & 1.00 & 1.44\\\\\n1.26 & 1.25 & 0.26\n\\epmat\n\\] A simple implementation would look as follows:\nimport torch\nQ = torch.tensor([[1, 2], [3, 1]]).float()\nK = torch.tensor([[2, 1], [1, 1], [0, 1]]).float()\nV = torch.tensor([[1, 2, -2], [1, 1, 2], [0, 1, -1]]).float()\nd_k = torch.tensor(K.shape[1])\nM = torch.matmul(Q, K.transpose(0, 1)) / torch.sqrt(d_k)\nS = torch.exp(M) / torch.sum(torch.exp(M), dim=1).view(-1,1)\ntorch.matmul(S, V)\nPytorch also provides a function for scaled dot product attention:\nimport torch.nn.functional as F\nF.scaled_dot_product_attention(Q, K, V)"
  },
  {
    "objectID": "ex/w02/questions/linalg-evs.html",
    "href": "ex/w02/questions/linalg-evs.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "For a square matrix \\(\\fA\\) of size \\(n \\times n\\), a vector \\(\\fu_i \\neq 0\\) which satisﬁes \\[\\begin{equation}\n\\fA\\fu_i = \\la_i \\fu_i\n\\label{eq:eigen}\n\\end{equation}\\] is called a eigenvector of \\(\\fA\\), and \\(\\la_i\\) is the corresponding eigenvalue. For a matrix of size \\(n \\times n\\), there are \\(n\\) eigenvalues \\(\\la_i\\) (which are not necessarily distinct).\nShow that if \\(\\fu_1\\) and \\(\\fu_2\\) are eigenvectors with equal corresponding eigenvalues \\(\\la_1 = \\la_2\\), then \\(\\fu = \\al \\fu_1 + \\be \\fu_2\\) is also an eigenvector with the same eigenvalue."
  },
  {
    "objectID": "ex/w02/questions/linalg-evs-sol.html",
    "href": "ex/w02/questions/linalg-evs-sol.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "Because \\(\\la_1=\\la_2\\), we will write \\(\\la\\) for simplicity. The result is obtained by applying the definition of eigenvalues and distributivity via \\[\\begin{equation*}\n\\fA\\fu\n  = \\fA (\\al \\fu_1 + \\be \\fu_2)\n  =   \\al \\fA \\fu_1 + \\be \\fA \\fu_2\n  =   \\al \\la \\fu_1 + \\be \\la \\fu_2\n  =   \\la (\\al\\fu_1 + \\be \\fu_2)\n  =   \\la \\fu .\n\\end{equation*}\\]"
  },
  {
    "objectID": "ex/w02/questions/nn-compgraph-sol.html",
    "href": "ex/w02/questions/nn-compgraph-sol.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "Applying the chain rule, we have\n\\[ \\frac{\\partial \\mathcal{L}}{\\partial w_j} = \\frac{\\partial \\mathcal{L}}{\\partial y} \\frac{\\partial y}{\\partial z} \\frac{\\partial z}{\\partial w_j} \\]\nLooking at each term individually yields \\[\n\\begin{aligned}\n\\frac{\\partial \\mathcal{L}}{\\partial y}\n  &= \\frac{\\partial}{\\partial y} [-t \\log(y) - (1 - t) \\log(1 - y)]\n  = - \\frac{t}{y} + \\frac{1 - t}{1 - y}\\\\\n\\frac{\\partial y}{\\partial z}\n  &= \\frac{\\partial \\sigma(z)}{\\partial z}\n  = \\sigma(z) (1 - \\sigma(z))\n  = y (1 - y)\\\\\n\\frac{\\partial z}{\\partial w_j}\n  &= \\frac{\\partial}{\\partial w_j} (w^\\top x) = x_j\n\\end{aligned}\n\\]\nBringing it all together yields: \\[\n\\begin{aligned}\n\\frac{\\partial \\mathcal{L}}{\\partial w_j}\n  &= \\left( - \\frac{t}{y} + \\frac{1 - t}{1 - y} \\right) \\cdot y (1 - y) \\cdot x_j \\\\\n  &= (-t + ty + 1 - t - y + ty) x_j \\\\\n  &= (y - t) x_j\n\\end{aligned}\n\\]\nThe computation graph is given in the figure below.\n\n\n\n\nComputation graph for exercise 4 (b)"
  },
  {
    "objectID": "ex/w02/questions/prob-evvar-sol.html",
    "href": "ex/w02/questions/prob-evvar-sol.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "The key idea here is to compute the gradient of the objective function and solve for \\(\\fmu\\). The gradient is obtained by applying the chain rule resulting in \\[0=\\nabla_\\fmu \\sum_i \\|\\fx_i - \\fmu\\|^2 = -2 \\sum_i (\\fx_i - \\fmu) .\\] Now, we solve this for \\(\\fmu\\) to obtain \\[\\fmu = \\frac{1}{N} \\sum_i \\fx_i .\\]\nHere, we simply need to apply some algebraic manipulations to show that the two definitions are equivalent. We start with the first definition and expand the square: \\[\\begin{align*}\n\\E[(X - \\E[X])^2]\n  &= \\E\\li[X^2 - 2X\\E[X]+\\E[X]^2\\ri] \\\\\n  &= \\E\\li[X^2\\ri] - \\E[2X\\E[X]]+\\E\\li[\\E[X]^2\\ri]\\\\\n  &= \\E\\li[X^2\\ri] - 2\\E[X]\\E[X]+\\E[X]^2\\\\\n  &= \\E\\li[X^2\\ri] - \\E[X]^2\n\\end{align*}\\]"
  },
  {
    "objectID": "ex/w02/exercises02.html",
    "href": "ex/w02/exercises02.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "For a square matrix \\(\\fA\\) of size \\(n \\times n\\), a vector \\(\\fu_i \\neq 0\\) which satisﬁes \\[\\begin{equation}\n\\fA\\fu_i = \\la_i \\fu_i\n\\label{eq:eigen}\n\\end{equation}\\] is called a eigenvector of \\(\\fA\\), and \\(\\la_i\\) is the corresponding eigenvalue. For a matrix of size \\(n \\times n\\), there are \\(n\\) eigenvalues \\(\\la_i\\) (which are not necessarily distinct).\nShow that if \\(\\fu_1\\) and \\(\\fu_2\\) are eigenvectors with equal corresponding eigenvalues \\(\\la_1 = \\la_2\\), then \\(\\fu = \\al \\fu_1 + \\be \\fu_2\\) is also an eigenvector with the same eigenvalue."
  },
  {
    "objectID": "ex/w02/exercises02.html#exercise-1---eigenvectors-and-eigenvalues",
    "href": "ex/w02/exercises02.html#exercise-1---eigenvectors-and-eigenvalues",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "For a square matrix \\(\\fA\\) of size \\(n \\times n\\), a vector \\(\\fu_i \\neq 0\\) which satisﬁes \\[\\begin{equation}\n\\fA\\fu_i = \\la_i \\fu_i\n\\label{eq:eigen}\n\\end{equation}\\] is called a eigenvector of \\(\\fA\\), and \\(\\la_i\\) is the corresponding eigenvalue. For a matrix of size \\(n \\times n\\), there are \\(n\\) eigenvalues \\(\\la_i\\) (which are not necessarily distinct).\nShow that if \\(\\fu_1\\) and \\(\\fu_2\\) are eigenvectors with equal corresponding eigenvalues \\(\\la_1 = \\la_2\\), then \\(\\fu = \\al \\fu_1 + \\be \\fu_2\\) is also an eigenvector with the same eigenvalue."
  },
  {
    "objectID": "ex/w02/exercises02.html#exercise-2---variance-and-expectation",
    "href": "ex/w02/exercises02.html#exercise-2---variance-and-expectation",
    "title": "CSC477 - Fall 2024",
    "section": "Exercise 2 - Variance and Expectation",
    "text": "Exercise 2 - Variance and Expectation\n\nGiven a set of vectors \\(\\{\\fx_i\\}_{i=1}^N\\). Show that their empirical mean is equivalent to \\[\\hmu=\\argmin_\\fmu \\sum_i \\|\\fx_i - \\fmu\\|^2.\\]\nThere are two equivalent definitons of variance of a random variable. The first one is \\(\\Var(X) := \\E[(X - \\E[X])^2]\\) and the second is \\(\\Var(X) = \\E[X^2] - \\E[X]^2\\). Show that these two definitions actually are equivalent."
  },
  {
    "objectID": "ex/w02/exercises02.html#exercise-3---linear-regression",
    "href": "ex/w02/exercises02.html#exercise-3---linear-regression",
    "title": "CSC477 - Fall 2024",
    "section": "Exercise 3 - Linear Regression",
    "text": "Exercise 3 - Linear Regression\n\n\nIn the linear regression model with one feature, we have the following model/hypothesis: \\[y = f(x) = w x + b\\] ​with parameters, \\(w\\) and \\(b\\), which we wish to find by minimizing the cost: \\[\\mathcal{E}(w, b) = \\frac{1}{2N}\\sum_i ((w x^{(i)} + b) - t^{(i)})^2\\] What are the derivatives \\(\\frac{\\partial \\mathcal{E}}{\\partial w}\\) and \\(\\frac{\\partial \\mathcal{E}}{\\partial b}\\)?\nIn the linear regression model with many features, we have the following model/hypothesis: \\[y = f(x) = {\\bf w}^\\top {\\bf x}+ b\\] with parameters, \\({\\bf w} = [w_1, w_2, \\dots w_d]^T\\) and \\(b\\), which we wish to find by minimizing the cost: \\[\\mathcal{E}({\\bf w}, b) = \\frac{1}{2N}\\sum_i ((\\fw^\\top \\fx^{(i)}+b) - t^{(i)})^2\\] What is the derivative \\(\\frac{\\partial \\mathcal{E}}{\\partial w_j}\\) for a weight \\(w_j\\)?"
  },
  {
    "objectID": "ex/w02/exercises02.html#exercise-4---gradients-and-computation-graphs",
    "href": "ex/w02/exercises02.html#exercise-4---gradients-and-computation-graphs",
    "title": "CSC477 - Fall 2024",
    "section": "Exercise 4 - Gradients and Computation Graphs",
    "text": "Exercise 4 - Gradients and Computation Graphs\n\n\nCompute the \\(\\frac{\\partial \\mathcal{L}}{\\partial w_j}\\) gradient of \\(\\mathcal{L}\\) with respect to a \\(w_j\\) in the following computation: \\[\\begin{align*}\n  \\mathcal{L}(y, t) &= - t \\log(y) - (1-t) \\log(1-y) ,\n& y &= \\sigma(z) ,\n&  z &= {\\bf w}^\\top {\\bf x} .\n  \\end{align*}\\]\nDraw the computation graph for the following neural network, showing the relevant scalar quantities. Assume that \\(\\fy, \\fh, \\fx \\in \\mathbb{R}^2\\) \\[\\begin{align*}\n  \\mathcal{L} &= \\frac{1}{2}\\sum_k (y_k - t_k)^2 ,\n& y_k &= \\sum_i w_{ki}^{(2)} h_i + b_k^{(2)} ,\n& h_i &= \\sigma(z_i) ,\n& z_i &= \\sum_j w_{ij}^{(1)} x_j + b_i^{(1)} .\n  \\end{align*}\\]"
  },
  {
    "objectID": "ex/w05/questions/opt-momentum-sol.html",
    "href": "ex/w05/questions/opt-momentum-sol.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "An example implementation could look like this. First we deinfe the objective function and its gradient:\ndef objective(x):\n    return x**2\n\ndef obj_grad(x):\n    return 2*x\nThen, we implement the momentum optimizer itself:\ndef sgd_with_momentum(obj, grad, x_init, learning_rate, momentum, max_iter):\n  x = x_init\n  update = 0\n  for i in range(max_iter):\n    update = momentum * update - learning_rate * grad(x)\n    x = x + update\n\n    print('&gt;%d f(%s) = %.5f' % (i, x, obj(x)))\n  return x\nIt can now be invoked directly via:\nsgd_with_momentum(\n    objective, obj_grad, x_init=3.0, learning_rate=0.1, momentum=0.5, \n    max_iter=20\n    )"
  },
  {
    "objectID": "ex/w05/questions/linalg-evs_to_mat.html",
    "href": "ex/w05/questions/linalg-evs_to_mat.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "You are given the sets of eigevalues and eigenvectors. Compute the corresponding matrix.\n\n\\(\\la_1 = 2\\), \\(\\la_2 = 3\\), \\(\\fv_1 = (1,0)^\\top\\), \\(\\fv_2 = (0,1)^\\top\\).\n\\(\\la_1 = 2\\), \\(\\la_2 = 3\\), \\(\\fv_1 = (1,1)^\\top\\), \\(\\fv_2 = (1,-1)^\\top\\)."
  },
  {
    "objectID": "ex/w05/questions/calc-taylor-sol.html",
    "href": "ex/w05/questions/calc-taylor-sol.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "There are different ways to write the second order taylor series expansion a at a point \\(\\fa\\) for multivariate functions \\(f\\). We will use the following form\n\\[T(x) = f(\\fa) + \\nabla f(\\fa)^\\top (\\fx-\\fa) + \\frac{1}{2}(\\fx-\\fa)^\\top \\fH (\\fx-\\fa)\\]\nwhere \\(\\nabla f(\\fa)\\) is the gradient of \\(f\\) at \\(\\fa\\) and \\(\\fH\\) is the Hessian of \\(f\\) at \\(\\fa\\). As a reminder, the Hessian is the matrix of second order partial derivatives. So, all we need to do for all of the exercises is evaluate \\(f(\\fa)\\) and compute its gradient and Hessian.\n\n\\(T(x) = 5 + 15 (x-1) + 15 (x-1)^2\\)\nFirst, we simplify \\(f(x,y)=x^2 (y^3+1)\\) and compute \\(f(3,2)=81\\) and then we compute the gradient and Hessian of \\(f\\) resulting in \\[\n\\nabla f(x,y) = \\bpmat 2x (y^3-1) \\\\ x^2(3y^2-1) \\epmat,\\quad\n\\fH = \\bpmat 2(y^3-1) & 6 xy^2 \\\\ 6xy^2 & 6x^2y \\epmat.\n\\]\nThis one is similar to (b) but we have to be careful with the logarithm. First, we have \\(f(\\fx_0)=0\\) because \\(\\log(1)=0\\). Then, we compute the gradient and Hessian of \\(f\\) resulting in \\[\n    \\nabla f(\\fx) = \\bpmat\n   3x_1^2 x_2 \\log(x_2) \\\\ x_1^3 (1+\\log(x_2))\n   \\epmat,\\quad\n    \\fH = \\bpmat\n   6x_1 x_2 \\log(x_2)   & 3x_1^2 (1+\\log(x_2)) \\\\\n   3x_1^2 (1+\\log(x_2)) & x_1^3 x_2^{-1}\n    \\epmat\n    \\]\nEvaluating the trigonometric functions here is simpler than it seems because they are evaluated at \\(\\pi\\) and \\(-\\pi\\). We get \\(f(\\fx_0) = \\sin(-\\pi) + \\cos(\\pi) = 0 - 1 = -1\\) and \\[\n\\nabla f(\\fx) = \\bpmat \\cos(x_1) \\\\ -\\sin(x_2) \\epmat, \\quad\n\\fH = \\bpmat -\\sin(x_1) & 0 \\\\ 0 & -\\cos(x_2) \\epmat.\n\\]"
  },
  {
    "objectID": "ex/w05/exercises05_solution.html",
    "href": "ex/w05/exercises05_solution.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "Compute the Taylor series expansion up to the second order term for the following multivariate functions around a given point:\n\n\\(f(x) = 5x^3\\) around \\(x_0=1\\).\n\\(f(x,y) = x^2 \\cdot y^3 + x^2\\) around \\(x_0=3\\), \\(y_0=2\\).\n\\(f(\\fx) = x_1^3 \\cdot x_2 \\cdot \\log(x_2)\\) around \\(\\fx_0=(2,1)^\\top\\).\n\\(f(\\fx) = \\sin(x_1) + \\cos(x_2)\\) around \\(\\fx_0=(-\\pi,\\pi)^\\top\\).\n\n\n\nThere are different ways to write the second order taylor series expansion a at a point \\(\\fa\\) for multivariate functions \\(f\\). We will use the following form\n\\[T(x) = f(\\fa) + \\nabla f(\\fa)^\\top (\\fx-\\fa) + \\frac{1}{2}(\\fx-\\fa)^\\top \\fH (\\fx-\\fa)\\]\nwhere \\(\\nabla f(\\fa)\\) is the gradient of \\(f\\) at \\(\\fa\\) and \\(\\fH\\) is the Hessian of \\(f\\) at \\(\\fa\\). As a reminder, the Hessian is the matrix of second order partial derivatives. So, all we need to do for all of the exercises is evaluate \\(f(\\fa)\\) and compute its gradient and Hessian.\n\n\\(T(x) = 5 + 15 (x-1) + 15 (x-1)^2\\)\nFirst, we simplify \\(f(x,y)=x^2 (y^3+1)\\) and compute \\(f(3,2)=81\\) and then we compute the gradient and Hessian of \\(f\\) resulting in \\[\n\\nabla f(x,y) = \\bpmat 2x (y^3-1) \\\\ x^2(3y^2-1) \\epmat,\\quad\n\\fH = \\bpmat 2(y^3-1) & 6 xy^2 \\\\ 6xy^2 & 6x^2y \\epmat.\n\\]\nThis one is similar to (b) but we have to be careful with the logarithm. First, we have \\(f(\\fx_0)=0\\) because \\(\\log(1)=0\\). Then, we compute the gradient and Hessian of \\(f\\) resulting in \\[\n    \\nabla f(\\fx) = \\bpmat\n   3x_1^2 x_2 \\log(x_2) \\\\ x_1^3 (1+\\log(x_2))\n   \\epmat,\\quad\n    \\fH = \\bpmat\n   6x_1 x_2 \\log(x_2)   & 3x_1^2 (1+\\log(x_2)) \\\\\n   3x_1^2 (1+\\log(x_2)) & x_1^3 x_2^{-1}\n    \\epmat\n    \\]\nEvaluating the trigonometric functions here is simpler than it seems because they are evaluated at \\(\\pi\\) and \\(-\\pi\\). We get \\(f(\\fx_0) = \\sin(-\\pi) + \\cos(\\pi) = 0 - 1 = -1\\) and \\[\n\\nabla f(\\fx) = \\bpmat \\cos(x_1) \\\\ -\\sin(x_2) \\epmat, \\quad\n\\fH = \\bpmat -\\sin(x_1) & 0 \\\\ 0 & -\\cos(x_2) \\epmat.\n\\]"
  },
  {
    "objectID": "ex/w05/exercises05_solution.html#exercise-1---taylor-series",
    "href": "ex/w05/exercises05_solution.html#exercise-1---taylor-series",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "Compute the Taylor series expansion up to the second order term for the following multivariate functions around a given point:\n\n\\(f(x) = 5x^3\\) around \\(x_0=1\\).\n\\(f(x,y) = x^2 \\cdot y^3 + x^2\\) around \\(x_0=3\\), \\(y_0=2\\).\n\\(f(\\fx) = x_1^3 \\cdot x_2 \\cdot \\log(x_2)\\) around \\(\\fx_0=(2,1)^\\top\\).\n\\(f(\\fx) = \\sin(x_1) + \\cos(x_2)\\) around \\(\\fx_0=(-\\pi,\\pi)^\\top\\).\n\n\n\nThere are different ways to write the second order taylor series expansion a at a point \\(\\fa\\) for multivariate functions \\(f\\). We will use the following form\n\\[T(x) = f(\\fa) + \\nabla f(\\fa)^\\top (\\fx-\\fa) + \\frac{1}{2}(\\fx-\\fa)^\\top \\fH (\\fx-\\fa)\\]\nwhere \\(\\nabla f(\\fa)\\) is the gradient of \\(f\\) at \\(\\fa\\) and \\(\\fH\\) is the Hessian of \\(f\\) at \\(\\fa\\). As a reminder, the Hessian is the matrix of second order partial derivatives. So, all we need to do for all of the exercises is evaluate \\(f(\\fa)\\) and compute its gradient and Hessian.\n\n\\(T(x) = 5 + 15 (x-1) + 15 (x-1)^2\\)\nFirst, we simplify \\(f(x,y)=x^2 (y^3+1)\\) and compute \\(f(3,2)=81\\) and then we compute the gradient and Hessian of \\(f\\) resulting in \\[\n\\nabla f(x,y) = \\bpmat 2x (y^3-1) \\\\ x^2(3y^2-1) \\epmat,\\quad\n\\fH = \\bpmat 2(y^3-1) & 6 xy^2 \\\\ 6xy^2 & 6x^2y \\epmat.\n\\]\nThis one is similar to (b) but we have to be careful with the logarithm. First, we have \\(f(\\fx_0)=0\\) because \\(\\log(1)=0\\). Then, we compute the gradient and Hessian of \\(f\\) resulting in \\[\n    \\nabla f(\\fx) = \\bpmat\n   3x_1^2 x_2 \\log(x_2) \\\\ x_1^3 (1+\\log(x_2))\n   \\epmat,\\quad\n    \\fH = \\bpmat\n   6x_1 x_2 \\log(x_2)   & 3x_1^2 (1+\\log(x_2)) \\\\\n   3x_1^2 (1+\\log(x_2)) & x_1^3 x_2^{-1}\n    \\epmat\n    \\]\nEvaluating the trigonometric functions here is simpler than it seems because they are evaluated at \\(\\pi\\) and \\(-\\pi\\). We get \\(f(\\fx_0) = \\sin(-\\pi) + \\cos(\\pi) = 0 - 1 = -1\\) and \\[\n\\nabla f(\\fx) = \\bpmat \\cos(x_1) \\\\ -\\sin(x_2) \\epmat, \\quad\n\\fH = \\bpmat -\\sin(x_1) & 0 \\\\ 0 & -\\cos(x_2) \\epmat.\n\\]"
  },
  {
    "objectID": "ex/w05/exercises05_solution.html#exercise-2---eigenvalues-eigenvectors",
    "href": "ex/w05/exercises05_solution.html#exercise-2---eigenvalues-eigenvectors",
    "title": "CSC477 - Fall 2024",
    "section": "Exercise 2 - Eigenvalues, Eigenvectors",
    "text": "Exercise 2 - Eigenvalues, Eigenvectors\nYou are given the sets of eigevalues and eigenvectors. Compute the corresponding matrix.\n\n\\(\\la_1 = 2\\), \\(\\la_2 = 3\\), \\(\\fv_1 = (1,0)^\\top\\), \\(\\fv_2 = (0,1)^\\top\\).\n\\(\\la_1 = 2\\), \\(\\la_2 = 3\\), \\(\\fv_1 = (1,1)^\\top\\), \\(\\fv_2 = (1,-1)^\\top\\).\n\n\nSolution\nFirst, remember that the normalized eigenvectors of a symmetric matrix are orthogonal. Thus, we have \\[\n\\fe_i^\\top \\fe_j = \\begin{cases} 1 & i=j \\\\ 0 & i\\neq j \\end{cases}.\n\\]\nSecond, for symmetric \\(\\fA\\), its spectral decomposition is given by \\(\\fA = \\fQ \\fLa \\fQ^\\top\\), where \\(\\fQ\\) is a matrix where each column is an (orthogonal) eigenvector of unit length.\n\nHere, the eigenvectors are already normalized and orthogonal, so we can simply write \\(\\fQ = (\\fe_1, \\fe_2)\\) and \\(\\fLa = \\diag(\\lambda_1, \\lambda_2)\\). Then, we have \\[\n  \\fA = \\bpmat 2 & 0 \\\\\n0 & 3\n\\epmat\n  \\]\nHere, we have to normalize the eigenvectors first. Each has length \\(\\sqrt{2}\\), so we have to divide each of them by \\(\\sqrt{2}\\), i.e. we set \\(\\fte_i:=\\fe_i/\\sqrt{2}\\). With this, we can construct an orthogonal matrix of eigenvalues as \\(\\fQ = (\\fte_1, \\fte_2)\\). The resulting matrix \\(\\fA\\) is \\[\n  \\fA = \\bpmat\n2.5 & -0.5 \\\\\n-0.5 & 2.5\n\\epmat\n  \\]"
  },
  {
    "objectID": "ex/w05/exercises05_solution.html#exercise-3---sgd-with-momentum",
    "href": "ex/w05/exercises05_solution.html#exercise-3---sgd-with-momentum",
    "title": "CSC477 - Fall 2024",
    "section": "Exercise 3 - SGD with Momentum",
    "text": "Exercise 3 - SGD with Momentum\nImplement stochastic gradient descent with momentum and apply it to optimize some elementary functions in 1d and 2d.\n\nSolution\nAn example implementation could look like this. First we deinfe the objective function and its gradient:\ndef objective(x):\n    return x**2\n\ndef obj_grad(x):\n    return 2*x\nThen, we implement the momentum optimizer itself:\ndef sgd_with_momentum(obj, grad, x_init, learning_rate, momentum, max_iter):\n  x = x_init\n  update = 0\n  for i in range(max_iter):\n    update = momentum * update - learning_rate * grad(x)\n    x = x + update\n\n    print('&gt;%d f(%s) = %.5f' % (i, x, obj(x)))\n  return x\nIt can now be invoked directly via:\nsgd_with_momentum(\n    objective, obj_grad, x_init=3.0, learning_rate=0.1, momentum=0.5, \n    max_iter=20\n    )"
  },
  {
    "objectID": "ex/w04/questions/cnn-sizes-sol.html",
    "href": "ex/w04/questions/cnn-sizes-sol.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "The outputs dimensions after each layer are:\n\n1. 98 x 98 x 5 \n2. 49 x 49 x 5\n3. 47 x 47 x 10\n4. 23 x 23 x 10\n5. 21 x 21 x 5 \n6. 2205\n7. 20\n8. 10\n\nThe number of parameters for each layer is:\n\n1. 140\n2. 0\n3. 460\n4. 0\n5. 455\n6. 0\n7. 44120\n8. 210"
  },
  {
    "objectID": "ex/w04/questions/nlp-co_occurrence-sol.html",
    "href": "ex/w04/questions/nlp-co_occurrence-sol.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "The solution is not unique because we can change the order of the reference entries. In our case we will simply follow the intuitive ordering resulting in the following co-occurence matrix:\n\n\n\nReference\nA\nbird\nin\nthe\nhand\nis\nworth\ntwo\nbush\n\n\n\n\nA\n0\n1\n0\n0\n0\n0\n0\n0\n0\n\n\nbird\n0\n0\n1\n0\n0\n0\n0\n0\n0\n\n\nin\n0\n0\n0\n2\n0\n0\n0\n0\n0\n\n\nthe\n0\n0\n0\n0\n1\n0\n0\n0\n1\n\n\nhand\n0\n0\n0\n0\n0\n1\n0\n0\n0\n\n\nis\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\nworth\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\ntwo\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nbush\n0\n0\n0\n0\n0\n0\n0\n0\n0"
  },
  {
    "objectID": "ex/w04/questions/nlp-co_occurrence.html",
    "href": "ex/w04/questions/nlp-co_occurrence.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "Write a co-occurrence matrix for the following sentence:\n\n“A bird in the hand is worth two in the bush.”\n\nCount each word only if it appears directly after the reference word. Is the co-occurrence matrix unique?"
  },
  {
    "objectID": "ex/w04/questions/cnn-by_hand-sol.html",
    "href": "ex/w04/questions/cnn-by_hand-sol.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "Here, we simply apply the convolutional kernel over each \\(2\\times 2\\) patch of the input. There are 9 such patches. The output \\(Y\\) is then\n\n\\[\nY = \\bpmat\n3 & -1 & -3 \\\\\n2 & 3 &  3 \\\\\n5 & 2 &  1\n\\epmat\n\\]\n\nSame idea except that we skip every other patch resulting in only 4 patches. The output \\(Y\\) is then\n\n\\[\nY = \\bpmat\n3 & -3 \\\\\n5 &  1\n\\epmat\n\\]\n\nNow, we have added zeros on each side of the input. The resulting \\(6\\times 6\\) padded input \\(X_\\mathrm{padded}\\) and corresponding output \\(Y\\) are\n\n\\[\nX_\\mathrm{padded} =\n\\bpmat\n0 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & -2 & 1 & 0\\\\\n0 & 0 & 1 & 1 & 0 & 0\\\\\n0 & 0 & 1 & 0 & 1 & 0\\\\\n0 & -3 & 4 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0\n\\epmat,\n\\qquad\nY = \\bpmat\n1 & -2 & 0 \\\\\n0 & 3  &  0 \\\\\n-3 & 8 & 0\n\\epmat\n\\]"
  },
  {
    "objectID": "ex/w04/exercises04.html",
    "href": "ex/w04/exercises04.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "Write a co-occurrence matrix for the following sentence:\n\n“A bird in the hand is worth two in the bush.”\n\nCount each word only if it appears directly after the reference word. Is the co-occurrence matrix unique?"
  },
  {
    "objectID": "ex/w04/exercises04.html#exercise-1---co-occurrence-matrix",
    "href": "ex/w04/exercises04.html#exercise-1---co-occurrence-matrix",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "Write a co-occurrence matrix for the following sentence:\n\n“A bird in the hand is worth two in the bush.”\n\nCount each word only if it appears directly after the reference word. Is the co-occurrence matrix unique?"
  },
  {
    "objectID": "ex/w04/exercises04.html#exercise-2---convolutional-layers",
    "href": "ex/w04/exercises04.html#exercise-2---convolutional-layers",
    "title": "CSC477 - Fall 2024",
    "section": "Exercise 2 - Convolutional Layers",
    "text": "Exercise 2 - Convolutional Layers\nConsider the following \\(4\\times 4 \\times 1\\) input X and a \\(2\\times 2 \\times 1\\) convolutional kernel K with no bias term\n\\[\nX =\n\\bpmat\n1 & 0 & -2 & 1 \\\\\n0 & 1 & 1 & 0 \\\\\n0 & 1 & 0 & 1 \\\\\n-3 & 4 & 0 & 0\n\\epmat, \\qquad\n%\nK = \\bpmat\n2, & 1 \\\\\n0, & 1 \\\\\n\\epmat\n\\]\n\nWhat is the output of the convolutional layer for the case of stride 1 and no padding?\nWhat if we have stride 2 and no padding?\nWhat if we have stride 2 and zero-padding of size 1?"
  },
  {
    "objectID": "ex/w04/exercises04.html#exercise-3---sizes-in-mlps-refresher",
    "href": "ex/w04/exercises04.html#exercise-3---sizes-in-mlps-refresher",
    "title": "CSC477 - Fall 2024",
    "section": "Exercise 3 - Sizes in MLPs Refresher",
    "text": "Exercise 3 - Sizes in MLPs Refresher\nYou are given an MLP with ReLU activations. It has 3 layers consisting of 5, 10, and 5 neurons respectively. The input is a vector of size 10. How many parameters does this network have?"
  },
  {
    "objectID": "ex/w04/exercises04.html#exercise-4---sizes-in-cnns",
    "href": "ex/w04/exercises04.html#exercise-4---sizes-in-cnns",
    "title": "CSC477 - Fall 2024",
    "section": "Exercise 4 - Sizes in CNNs",
    "text": "Exercise 4 - Sizes in CNNs\nYou are givne a neural network with the following architecture:\nInput: 100 x 100 x 3 Image\n\nLayers:\n1. Conv(in_channels=3, out_channels=5, kernel_size=3, stride=1, padding=0)\n2. MaxPool2d(kernel_size=2, stride=2, padding=0)\n3. Conv(in_channels=5, out_channels=10, kernel_size=3, stride=1, padding=0)\n4. MaxPool2d(kernel_size=2, stride=2, padding=0)\n5. Conv(in_channels=10, out_channels=5, kernel_size=3, stride=1, padding=0)\n6. Flatten()\n7. MLP(neurons=20)\n8. MLP(neurons=10)\n\nWhat is the dimensionality of the activations after each layer.\nHow many parameters does this network have?"
  },
  {
    "objectID": "ex/w04/exercises04_solution.html",
    "href": "ex/w04/exercises04_solution.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "Write a co-occurrence matrix for the following sentence:\n\n“A bird in the hand is worth two in the bush.”\n\nCount each word only if it appears directly after the reference word. Is the co-occurrence matrix unique?\n\n\nThe solution is not unique because we can change the order of the reference entries. In our case we will simply follow the intuitive ordering resulting in the following co-occurence matrix:\n\n\n\nReference\nA\nbird\nin\nthe\nhand\nis\nworth\ntwo\nbush\n\n\n\n\nA\n0\n1\n0\n0\n0\n0\n0\n0\n0\n\n\nbird\n0\n0\n1\n0\n0\n0\n0\n0\n0\n\n\nin\n0\n0\n0\n2\n0\n0\n0\n0\n0\n\n\nthe\n0\n0\n0\n0\n1\n0\n0\n0\n1\n\n\nhand\n0\n0\n0\n0\n0\n1\n0\n0\n0\n\n\nis\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\nworth\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\ntwo\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nbush\n0\n0\n0\n0\n0\n0\n0\n0\n0"
  },
  {
    "objectID": "ex/w04/exercises04_solution.html#exercise-1---co-occurrence-matrix",
    "href": "ex/w04/exercises04_solution.html#exercise-1---co-occurrence-matrix",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "Write a co-occurrence matrix for the following sentence:\n\n“A bird in the hand is worth two in the bush.”\n\nCount each word only if it appears directly after the reference word. Is the co-occurrence matrix unique?\n\n\nThe solution is not unique because we can change the order of the reference entries. In our case we will simply follow the intuitive ordering resulting in the following co-occurence matrix:\n\n\n\nReference\nA\nbird\nin\nthe\nhand\nis\nworth\ntwo\nbush\n\n\n\n\nA\n0\n1\n0\n0\n0\n0\n0\n0\n0\n\n\nbird\n0\n0\n1\n0\n0\n0\n0\n0\n0\n\n\nin\n0\n0\n0\n2\n0\n0\n0\n0\n0\n\n\nthe\n0\n0\n0\n0\n1\n0\n0\n0\n1\n\n\nhand\n0\n0\n0\n0\n0\n1\n0\n0\n0\n\n\nis\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\nworth\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\ntwo\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nbush\n0\n0\n0\n0\n0\n0\n0\n0\n0"
  },
  {
    "objectID": "ex/w04/exercises04_solution.html#exercise-2---convolutional-layers",
    "href": "ex/w04/exercises04_solution.html#exercise-2---convolutional-layers",
    "title": "CSC477 - Fall 2024",
    "section": "Exercise 2 - Convolutional Layers",
    "text": "Exercise 2 - Convolutional Layers\nConsider the following \\(4\\times 4 \\times 1\\) input X and a \\(2\\times 2 \\times 1\\) convolutional kernel K with no bias term\n\\[\nX =\n\\bpmat\n1 & 0 & -2 & 1 \\\\\n0 & 1 & 1 & 0 \\\\\n0 & 1 & 0 & 1 \\\\\n-3 & 4 & 0 & 0\n\\epmat, \\qquad\n%\nK = \\bpmat\n2, & 1 \\\\\n0, & 1 \\\\\n\\epmat\n\\]\n\nWhat is the output of the convolutional layer for the case of stride 1 and no padding?\nWhat if we have stride 2 and no padding?\nWhat if we have stride 2 and zero-padding of size 1?\n\n\nSolution\n\nHere, we simply apply the convolutional kernel over each \\(2\\times 2\\) patch of the input. There are 9 such patches. The output \\(Y\\) is then\n\n\\[\nY = \\bpmat\n3 & -1 & -3 \\\\\n2 & 3 &  3 \\\\\n5 & 2 &  1\n\\epmat\n\\]\n\nSame idea except that we skip every other patch resulting in only 4 patches. The output \\(Y\\) is then\n\n\\[\nY = \\bpmat\n3 & -3 \\\\\n5 &  1\n\\epmat\n\\]\n\nNow, we have added zeros on each side of the input. The resulting \\(6\\times 6\\) padded input \\(X_\\mathrm{padded}\\) and corresponding output \\(Y\\) are\n\n\\[\nX_\\mathrm{padded} =\n\\bpmat\n0 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & -2 & 1 & 0\\\\\n0 & 0 & 1 & 1 & 0 & 0\\\\\n0 & 0 & 1 & 0 & 1 & 0\\\\\n0 & -3 & 4 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0\n\\epmat,\n\\qquad\nY = \\bpmat\n1 & -2 & 0 \\\\\n0 & 3  &  0 \\\\\n-3 & 8 & 0\n\\epmat\n\\]"
  },
  {
    "objectID": "ex/w04/exercises04_solution.html#exercise-3---sizes-in-mlps-refresher",
    "href": "ex/w04/exercises04_solution.html#exercise-3---sizes-in-mlps-refresher",
    "title": "CSC477 - Fall 2024",
    "section": "Exercise 3 - Sizes in MLPs Refresher",
    "text": "Exercise 3 - Sizes in MLPs Refresher\nYou are given an MLP with ReLU activations. It has 3 layers consisting of 5, 10, and 5 neurons respectively. The input is a vector of size 10. How many parameters does this network have?\n\nSolution\nThe number of parameters for each neuron is the number of weights plus one for the biaas term. The number of weights corresponds to the number of inputs / activations from the previous layer. So for the first layer, we have 10 inputs and thus 11 parameters per neuron resulting in 55 parameters total per layer.\nA similar computation gives 60 and 55 as the number of parameters for the next two layers. Thus, the network has a total of 170 parameters."
  },
  {
    "objectID": "ex/w04/exercises04_solution.html#exercise-4---sizes-in-cnns",
    "href": "ex/w04/exercises04_solution.html#exercise-4---sizes-in-cnns",
    "title": "CSC477 - Fall 2024",
    "section": "Exercise 4 - Sizes in CNNs",
    "text": "Exercise 4 - Sizes in CNNs\nYou are givne a neural network with the following architecture:\nInput: 100 x 100 x 3 Image\n\nLayers:\n1. Conv(in_channels=3, out_channels=5, kernel_size=3, stride=1, padding=0)\n2. MaxPool2d(kernel_size=2, stride=2, padding=0)\n3. Conv(in_channels=5, out_channels=10, kernel_size=3, stride=1, padding=0)\n4. MaxPool2d(kernel_size=2, stride=2, padding=0)\n5. Conv(in_channels=10, out_channels=5, kernel_size=3, stride=1, padding=0)\n6. Flatten()\n7. MLP(neurons=20)\n8. MLP(neurons=10)\n\nWhat is the dimensionality of the activations after each layer.\nHow many parameters does this network have?\n\n\nSolution\n\nThe outputs dimensions after each layer are:\n\n1. 98 x 98 x 5 \n2. 49 x 49 x 5\n3. 47 x 47 x 10\n4. 23 x 23 x 10\n5. 21 x 21 x 5 \n6. 2205\n7. 20\n8. 10\n\nThe number of parameters for each layer is:\n\n1. 140\n2. 0\n3. 460\n4. 0\n5. 455\n6. 0\n7. 44120\n8. 210"
  },
  {
    "objectID": "ex/w03/questions/autodiff-modes.html",
    "href": "ex/w03/questions/autodiff-modes.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "Consider the function \\(F(x) = f_3(f_2(f_1(x))\\) and assume you also know the derivatives \\(f_i'\\) for all \\(f_i\\).\n\nApply the chain rule to express \\(F'(x)\\) in terms of \\(f_i'\\)s and \\(f_i\\).\nWrite down the pseudocode for computing \\(F'(x)\\) using the forward mode and the reverse mode respectively. Assume all functions to be scalar functions of a scalar variable, i.e. \\(f_i: \\R \\rightarrow \\R\\).\nIf you simply ask your interpreter / compiler to evaluate the expression in (a), will the computation be in forward mode, reverse mode, or neither of the modes? Why? You can assume that your interpreter / compiler does not do any caching or optimization and simply evaluates the expression from left to right. Does anything change if you assume that your interpreter caches results that have been computed before?"
  },
  {
    "objectID": "ex/w03/questions/prob-mle.html",
    "href": "ex/w03/questions/prob-mle.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "Assume you are given datapoints \\((\\fx_i)_{i=1}^N\\) with \\(\\fx_i\\in\\R^n\\) coming from a Gaussian distribution. Derive the maximum likelihood estimator of its mean."
  },
  {
    "objectID": "ex/w03/questions/prob-mle-sol.html",
    "href": "ex/w03/questions/prob-mle-sol.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "First, let’s quickly remember that the maximum likelihood estimator (MLE) of a probability distribution from dataapoints \\(\\fx_1, \\ldots, \\fx_N\\) is given by \\[\n\\hte_{\\mathrm{MLE}} = \\argmax_{\\te \\in \\Te} \\prod_{i=1}^N f(\\fx_i | \\te),\n\\] where \\(f\\) is the probability density function of the considered probability distribution family, \\(\\te\\) are the parameters of the distribution, and \\(\\Te\\) is the parameter space (a set containing all possible parameters). The product on the right hand side is also known as the likelihood function. In practice, we usually work with the log-likelihood function instead. Because the logarithm is monotonously increasing, the resulting estimators are the same, i.e. \\[\n\\hte_{\\mathrm{MLE}}\n= \\argmax_{\\te \\in \\Te} \\prod_{i=1}^N f(\\fx_i | \\te),\n= \\argmax_{\\te \\in \\Te} \\sum_{i=1}^N \\log \\li( f(\\fx_i | \\te)\\ri).\n\\]\nSecond, let’s remember that the probability density function of a multivariate Gaussian distribution is given by \\[\nf(\\fx_i | \\mu, \\Si)\n= \\frac{1}{(2 \\pi)^{n/2} |\\Si|^{1/2}}\n   \\exp \\li(\n     - \\frac{1}{2}\n     (\\fx_i - \\mu)^\\top\n     \\Si^{-1}\n     (\\fx_i - \\mu)\n   \\ri)\n\\] with parameters \\(\\te=(\\mu, \\Si)\\), where \\(\\mu\\in\\R^n\\) is the mean vector and \\(\\Si\\in\\R^{n\\times n}\\) is the covariance matrix. Moreover, the notation \\(|\\Si|\\) denotes the determinant of \\(\\Si\\).\nOur goal is to maximize the log-likelihood function which in this case is \\[\\begin{aligned}\nl(\\mu, \\Si| \\fx_1, \\ldots, \\fx_N)\n  & := \\sum_{i=1}^N \\log f(\\fx_i | \\mu, \\Si) \\\\\n  & = \\sum_{i=1}^N \\li(\n    - \\frac{n}{2} \\log (2 \\pi)\n    - \\frac{1}{2} \\log |\\Si|  \n    - \\frac{1}{2}  (\\fx_i - \\mu)^\\top \\Si^{-1} (\\fx_i - \\mu)\n  \\ri).\n\\end{aligned}\\] For obtaining the MLE of \\(\\mu\\), we can simply take the gradient of \\(l\\) with respect to \\(\\mu\\) and set it to zero resulting in: \\[\n\\nabla_\\mu l(\\mu, \\Si| \\fx_1, \\ldots, \\fx_N)\n= \\sum_{i=1}^N  \\Si^{-1} ( \\fx_i - \\mu )\n= 0\n\\] Since \\(\\Si\\) is a covariance matrix, it is positive definite. Thus, we can multiply both sides of the equation by \\(\\Si\\) and obtain \\[\n\\begin{aligned}\n0 & = N \\mu - \\sum_{i=1}^N  \\fx_i,\n\\\\\n\\Rightarrow \\hmu_{\\mathrm{MLE}} &=  \\frac{1}{N} \\sum_{i=1}^N \\fx_i .\n\\end{aligned}\n\\] Conveniently, \\(\\Si\\) disappears and thus we do not have to worry about it."
  },
  {
    "objectID": "ex/w03/exercises03.html",
    "href": "ex/w03/exercises03.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "Assume you are given datapoints \\((\\fx_i)_{i=1}^N\\) with \\(\\fx_i\\in\\R^n\\) coming from a Gaussian distribution. Derive the maximum likelihood estimator of its mean."
  },
  {
    "objectID": "ex/w03/exercises03.html#exercise-1---maximum-likelihood-estimation-refresher",
    "href": "ex/w03/exercises03.html#exercise-1---maximum-likelihood-estimation-refresher",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "Assume you are given datapoints \\((\\fx_i)_{i=1}^N\\) with \\(\\fx_i\\in\\R^n\\) coming from a Gaussian distribution. Derive the maximum likelihood estimator of its mean."
  },
  {
    "objectID": "ex/w03/exercises03.html#exercise-2---more-gradients",
    "href": "ex/w03/exercises03.html#exercise-2---more-gradients",
    "title": "CSC477 - Fall 2024",
    "section": "Exercise 2 - More Gradients",
    "text": "Exercise 2 - More Gradients\nYou are an ML Engineer at Googlezon where you are working on an internal ML framework called TorchsorFlow. You are tasked with implementing a new layer known as BatchNormalization. The idea of this layer is as follows:\nDuring training, consider the outputs of the previous layer \\(\\fa_i=(a_i^{(1)}, \\ldots, a_i^{(N)})\\) for each element \\(i\\in \\{1, \\ldots, M\\}\\) of the input batch. Compute the mean \\(\\mu_j\\) and variance \\(\\si_j^2\\) over each input dimension \\(j\\). Use the resulting statistics to normalize the output of the previous layer. Finally, rescale the resulting vector with a learned constant \\(\\gm\\) and shift it by another learned constant \\(\\be\\).\n\nWrite down the mathematical expression for the BatchNormalization layer. What are its learnable parameters?\nCompute the gradient of the loss \\(\\mcL\\) with respect to the input of the BatchNormalization \\(\\fa_i\\) layer.\nAt test time, the batch size is usually 1. So, it is not meaningful (or even possible) to compute mean / variance. How would you implement a layer like this?"
  },
  {
    "objectID": "ex/w03/exercises03.html#exercise-3---autodiff-modes",
    "href": "ex/w03/exercises03.html#exercise-3---autodiff-modes",
    "title": "CSC477 - Fall 2024",
    "section": "Exercise 3 - Autodiff Modes",
    "text": "Exercise 3 - Autodiff Modes\nConsider the function \\(F(x) = f_3(f_2(f_1(x))\\) and assume you also know the derivatives \\(f_i'\\) for all \\(f_i\\).\n\nApply the chain rule to express \\(F'(x)\\) in terms of \\(f_i'\\)s and \\(f_i\\).\nWrite down the pseudocode for computing \\(F'(x)\\) using the forward mode and the reverse mode respectively. Assume all functions to be scalar functions of a scalar variable, i.e. \\(f_i: \\R \\rightarrow \\R\\).\nIf you simply ask your interpreter / compiler to evaluate the expression in (a), will the computation be in forward mode, reverse mode, or neither of the modes? Why? You can assume that your interpreter / compiler does not do any caching or optimization and simply evaluates the expression from left to right. Does anything change if you assume that your interpreter caches results that have been computed before?"
  },
  {
    "objectID": "ex/w03/exercises03.html#exercise-4---glove-embeddings",
    "href": "ex/w03/exercises03.html#exercise-4---glove-embeddings",
    "title": "CSC477 - Fall 2024",
    "section": "Exercise 4 - GloVe Embeddings",
    "text": "Exercise 4 - GloVe Embeddings\nOpen the notebook presented in class and work through it by trying some of the ideas presented therein for different word combinations."
  },
  {
    "objectID": "project-tips-resources.html",
    "href": "project-tips-resources.html",
    "title": "Project tips + resources",
    "section": "",
    "text": "R Data Sources for Regression Analysis\nFiveThirtyEight data\nTidyTuesday\n\n\n\n\n\nWorld Health Organization\nThe National Bureau of Economic Research\nInternational Monetary Fund\nGeneral Social Survey\nUnited Nations Data\nUnited Nations Statistics Division\nU.K. Data\nU.S. Data\nU.S. Census Data\nEuropean Statistics\nStatistics Canada\nPew Research\nUNICEF\nCDC\nWorld Bank\nElection Studies"
  },
  {
    "objectID": "project-tips-resources.html#data-sources",
    "href": "project-tips-resources.html#data-sources",
    "title": "Project tips + resources",
    "section": "",
    "text": "R Data Sources for Regression Analysis\nFiveThirtyEight data\nTidyTuesday\n\n\n\n\n\nWorld Health Organization\nThe National Bureau of Economic Research\nInternational Monetary Fund\nGeneral Social Survey\nUnited Nations Data\nUnited Nations Statistics Division\nU.K. Data\nU.S. Data\nU.S. Census Data\nEuropean Statistics\nStatistics Canada\nPew Research\nUNICEF\nCDC\nWorld Bank\nElection Studies"
  },
  {
    "objectID": "project-tips-resources.html#tips",
    "href": "project-tips-resources.html#tips",
    "title": "Project tips + resources",
    "section": "Tips",
    "text": "Tips\n\nAsk questions if any of the expectations are unclear.\nCode: In your write up your code should be hidden (echo = FALSE) so that your document is neat and easy to read. However your document should include all your code such that if I re-knit your qmd file I should be able to obtain the results you presented.\n\nException: If you want to highlight something specific about a piece of code, you’re welcome to show that portion.\n\nMerge conflicts will happen, issues will arise, and that’s fine! Commit and push often, and ask questions when stuck.\nMake sure each team member is contributing, both in terms of quality and quantity of contribution (we will be reviewing commits from different team members).\nAll team members are expected to contribute equally to the completion of this assignment and group assessments will be given at its completion - anyone judged to not have sufficient contributed to the final product will have their grade penalized. While different teams members may have different backgrounds and abilities, it is the responsibility of every team member to understand how and why all code and approaches in the assignment works."
  },
  {
    "objectID": "project-tips-resources.html#formatting-communication-tips",
    "href": "project-tips-resources.html#formatting-communication-tips",
    "title": "Project tips + resources",
    "section": "Formatting + communication tips",
    "text": "Formatting + communication tips\n\nSuppress Code, Warnings, & Messages\n\nInclude the following code in a code chunk at the top of your .qmd file to suppress all code, warnings, and other messages. Use the code chunk header {r set-up, include = FALSE} to suppress this set up code.\n\nknitr::opts_chunk$set(echo = FALSE,\n                      warning = FALSE, \n                      message = FALSE)\n\n\nHeaders\n\nUse headers to clearly label each section.\nInspect the document outline to review your headers and sub-headers.\n\n\n\nReferences\n\nInclude all references in a section called “References” at the end of the report.\nThis course does not have specific requirements for formatting citations and references.\n\n\n\nAppendix\n\nIf you have additional work that does not fit or does not belong in the body of the report, you may put it at the end of the document in section called “Appendix”.\nThe items in the appendix should be properly labeled.\nThe appendix should only be for additional material. The reader should be able to fully understand your report without viewing content in the appendix.\n\n\n\nResize figures\nResize plots and figures, so you have more space for the narrative.\n\n\nArranging plots\nArrange plots in a grid, instead of one after the other. This is especially useful when displaying plots for exploratory data analysis and to check assumptions.\nIf you’re using ggplot2 functions, the patchwork package makes it easy to arrange plots in a grid. See the documentation and examples here.\n\n\nPlot titles and axis labels\nBe sure all plot titles and axis labels are visible and easy to read.\n\nUse informative titles, not variable names, for titles and axis labels.\n\n❌ NO! The x-axis is hard to read because the names overlap.\n\nggplot(data = mpg, aes(x = manufacturer)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n✅ YES! Names are readable\n\nggplot(data = mpg, aes(y = manufacturer)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\n\nDo a little more to make the plot look professional!\n\nInformative title and axis labels\nFlipped coordinates to make names readable\nArranged bars based on count\nCapitalized manufacturer names\nOptional: Added color - Use a coordinated color scheme throughout paper / presentation\nOptional: Applied a theme - Use same theme throughout paper / presentation\n\n\nmpg %&gt;%\n  count(manufacturer) %&gt;%\n  mutate(manufacturer = str_to_title(manufacturer)) %&gt;%\n  ggplot(aes(y = fct_reorder(manufacturer,n), x = n)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  labs(x = \"Manufacturer\", \n       y = \"Count\", \n       title = \"The most common manufacturer is Dodge\") +\n  theme_minimal() \n\n\n\n\n\n\n\n\n\n\nTables and model output\n\nUse the kable function from the knitr package to neatly output all tables and model output. This will also ensure all model coefficients are displayed.\n\nUse the digits argument to display only 3 or 4 significant digits.\nUse the caption argument to add captions to your table.\n\n\n\nmodel &lt;- lm(mpg ~ hp, data = mtcars)\ntidy(model) %&gt;%\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n30.099\n1.634\n18.421\n0\n\n\nhp\n-0.068\n0.010\n-6.742\n0\n\n\n\n\n\n\n\nGuidelines for communicating results\n\nDon’t use variable names in your narrative! Use descriptive terms, so the reader understands your narrative without relying on the data dictionary.\n\n❌ There is a negative linear relationship between mpg and hp.\n✅ There is a negative linear relationship between a car’s fuel economy (in miles per gallon) and its horsepower.\n\nKnow your audience: Your report should be written for a general audience who has an understanding of statistics at the level of STA 210.\nAvoid subject matter jargon: Don’t assume the audience knows all of the specific terminology related to your subject area. If you must use jargon, include a brief definition the first time you introduce a term.\nTell the “so what”: Your report and presentation should be more than a list of interpretations and technical definitions. Focus on what the results mean, i.e. what you want the audience to know about your topic after reading your report or viewing your presentation.\n\n❌ For every one unit increase in horsepower, we expect the miles per gallon to decrease by 0.068 units, on average.\n✅ If the priority is to have good fuel economy, then one should choose a car with lower horsepower. Based on our model, the fuel economy is expected to decrease, on average, by 0.68 miles per gallon for every 10 additional horsepower.\n\nTell a story: All visualizations, tables, model output, and narrative should tell a cohesive story!\nUse one voice: Though multiple people are writing the report, it should read as if it’s from a single author. At least one team member should read through the report before submission to ensure it reads like a cohesive document."
  },
  {
    "objectID": "project-tips-resources.html#additional-resources",
    "href": "project-tips-resources.html#additional-resources",
    "title": "Project tips + resources",
    "section": "Additional resources",
    "text": "Additional resources\n\nR for Data Science\nQuarto Documentation\nData visualization\n\nggplot2 Reference\nggplot2: Elegant Graphics for Data Analysis\nData Visualization: A Practice Introduction\nPatchwork R Package"
  },
  {
    "objectID": "ae/ae-10-flight-delays.html",
    "href": "ae/ae-10-flight-delays.html",
    "title": "AE 10: Flight delays",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate the repo titled ae-10-flight-delays-YOUR_GITHUB_USERNAME to get started."
  },
  {
    "objectID": "ae/ae-10-flight-delays.html#packages",
    "href": "ae/ae-10-flight-delays.html#packages",
    "title": "AE 10: Flight delays",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)"
  },
  {
    "objectID": "ae/ae-10-flight-delays.html#data",
    "href": "ae/ae-10-flight-delays.html#data",
    "title": "AE 10: Flight delays",
    "section": "Data",
    "text": "Data\nFor this application exercise we will work with a dataset of 25,000 randomly sampled flights that departed one of three NYC airports (JFK, LGA, EWR) in 2013.\n\nflight_data &lt;- read_csv(\"data/flight-data.csv\")\n\nRows: 25000 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): origin, dest, carrier, arr_delay\ndbl  (4): dep_time, flight, air_time, distance\ndttm (1): time_hour\ndate (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nConvert arr_delay to factor with levels \"late\" (first level) and \"on_time\" (second level). This variable is our outcome and it indicates whether the flight’s arrival was more than 30 minutes.\n\n\nflight_data &lt;- flight_data %&gt;%\n  mutate(arr_delay = as.factor(arr_delay))\n\nlevels(flight_data$arr_delay)\n\n[1] \"late\"    \"on_time\"\n\n\n\nLet’s get started with some data prep: Convert all variables that are character strings to factors.\n\n\n#flight_data &lt;- flight_data %&gt;%\n#  mutate(\n#    origin = as.factor(origin),\n#    carrier = as.factor(carrier),\n#    dest = as.factor(dest)\n#    )\n\nflight_data &lt;- flight_data %&gt;%\n  #go across all columns and convert that are characters to factors\n  #go across all columns and convert if is.character = TRUE to factors\n  #go across all columns and if is.character apply as.factor\n  mutate(across(where(is.character), as.factor))"
  },
  {
    "objectID": "ae/ae-10-flight-delays.html#modeling-prep",
    "href": "ae/ae-10-flight-delays.html#modeling-prep",
    "title": "AE 10: Flight delays",
    "section": "Modeling prep",
    "text": "Modeling prep\n\nSplit the data into testing (75%) and training (25%), and save each subset.\n\n\nset.seed(222)\n\nflight_split &lt;- initial_split(flight_data)\n\nflight_train &lt;- training(flight_split)\nflight_test &lt;- testing(flight_split)\n\n\nSpecify a logistic regression model that uses the \"glm\" engine.\n\n\nflight_spec &lt;- logistic_reg() %&gt;%\n  set_engine(\"glm\")\n\nNext, we’ll create two recipes and workflows and compare them to each other."
  },
  {
    "objectID": "ae/ae-10-flight-delays.html#model-1-everything-and-the-kitchen-sink",
    "href": "ae/ae-10-flight-delays.html#model-1-everything-and-the-kitchen-sink",
    "title": "AE 10: Flight delays",
    "section": "Model 1: Everything and the kitchen sink",
    "text": "Model 1: Everything and the kitchen sink\n\nDefine a recipe that predicts arr_delay using all variables except for flight and time_hour, which, in combination, can be used to identify a flight. Also make sure this recipe handles dummy coding as well as issues that can arise due to having categorical variables with some levels apparent in the training set but not in the testing set. Call this recipe flights_rec1.\n\n\nflights_rec1 &lt;- recipe(arr_delay ~ ., data = flight_train) %&gt;%\n  update_role(flight, time_hour, new_role = \"id\") %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_zv(all_predictors())\n\n\nCreate a workflow that uses flights_rec1 and the model you specified.\n\n\nflight_wflow1 &lt;- workflow() %&gt;%\n  add_recipe(flights_rec1) %&gt;%\n  add_model(flight_spec)\n\n\nFit the this model to the training data using your workflow and display a tidy summary of the model fit.\n\n\nflight_fit1 &lt;- flight_wflow1 %&gt;%\n  fit(data = flight_train)\n\ntidy(flight_fit1)\n\n# A tibble: 119 × 5\n   term          estimate   std.error statistic   p.value\n   &lt;chr&gt;            &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 (Intercept)  13.3      287.          0.0464  9.63e-  1\n 2 dep_time     -0.00164    0.0000504 -32.6     1.04e-233\n 3 air_time     -0.0349     0.00179   -19.5     1.75e- 84\n 4 distance      0.00533    0.00523     1.02    3.08e-  1\n 5 date          0.000227   0.000198    1.15    2.51e-  1\n 6 origin_JFK    0.0830     0.102       0.815   4.15e-  1\n 7 origin_LGA   -0.0360     0.0983     -0.366   7.14e-  1\n 8 dest_ACK    -12.4      287.         -0.0434  9.65e-  1\n 9 dest_ALB    -12.4      287.         -0.0433  9.65e-  1\n10 dest_ANC     -3.75     928.         -0.00404 9.97e-  1\n# … with 109 more rows\n\n\n\nPredict arr_delay for the testing data using this model.\n\n\nflight_aug1 &lt;- augment(flight_fit1, flight_test)\n\n\nPlot the ROC curve and find the area under the curve. Comment on how well you think this model has done for predicting arrival delay.\n\n\nflight_aug1 %&gt;%\n  roc_curve(\n    truth = arr_delay,\n    .pred_late\n  ) %&gt;%\n  autoplot()\n\n\n\n\n\n\n\nflight_aug1 %&gt;%\n  roc_auc(\n    truth = arr_delay,\n    .pred_late\n  )\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.734"
  },
  {
    "objectID": "ae/ae-10-flight-delays.html#model-2-lets-be-a-bit-more-thoughtful",
    "href": "ae/ae-10-flight-delays.html#model-2-lets-be-a-bit-more-thoughtful",
    "title": "AE 10: Flight delays",
    "section": "Model 2: Let’s be a bit more thoughtful",
    "text": "Model 2: Let’s be a bit more thoughtful\n\nDefine a new recipe, flights_rec2, that, in addition to what was done in flights_rec1, adds features for day of week and month based on date and also adds indicators for all US holidays (also based on date). A list of these holidays can be found in timeDate::listHolidays(\"US\"). Once these features are added, date should be removed from the data. Then, create a new workflow, fit the same model (logistic regression) to the training data, and do predictions on the testing data. Finally, draw another ROC curve and find the area under the curve. Compare the predictive performance of this new model to the previous one. Based on the area under the curve statistic, which model does better?"
  },
  {
    "objectID": "ae/ae-10-flight-delays.html#putting-it-altogether",
    "href": "ae/ae-10-flight-delays.html#putting-it-altogether",
    "title": "AE 10: Flight delays",
    "section": "Putting it altogether",
    "text": "Putting it altogether\n\nCreate an ROC curve that plots both models, in different colors, and adds a legend indicating which model is which."
  },
  {
    "objectID": "ae/ae-10-flight-delays.html#acknowledgement",
    "href": "ae/ae-10-flight-delays.html#acknowledgement",
    "title": "AE 10: Flight delays",
    "section": "Acknowledgement",
    "text": "Acknowledgement\nThis exercise was inspired by https://www.tidymodels.org/start/recipes/."
  },
  {
    "objectID": "ae/ae-3-duke-forest.html",
    "href": "ae/ae-3-duke-forest.html",
    "title": "AE 3: Duke Forest houses",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate the repo titled ae-3-duke-forest-YOUR_GITHUB_USERNAME to get started."
  },
  {
    "objectID": "ae/ae-3-duke-forest.html#packages",
    "href": "ae/ae-3-duke-forest.html#packages",
    "title": "AE 3: Duke Forest houses",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(openintro)\nlibrary(knitr)"
  },
  {
    "objectID": "ae/ae-3-duke-forest.html#predict-sale-price-from-area",
    "href": "ae/ae-3-duke-forest.html#predict-sale-price-from-area",
    "title": "AE 3: Duke Forest houses",
    "section": "Predict sale price from area",
    "text": "Predict sale price from area\n\ndf_fit &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(price ~ area, data = duke_forest)\n\ntidy(df_fit) %&gt;%\n  kable(digits = 2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n116652.33\n53302.46\n2.19\n0.03\n\n\narea\n159.48\n18.17\n8.78\n0.00"
  },
  {
    "objectID": "ae/ae-3-duke-forest.html#model-conditions",
    "href": "ae/ae-3-duke-forest.html#model-conditions",
    "title": "AE 3: Duke Forest houses",
    "section": "Model conditions",
    "text": "Model conditions\n\nExercise 1\nThe following code produces the residuals vs. fitted values plot for this model. Comment out the layer that defines the y-axis limits and re-create the plot. How does the plot change? Why might we want to define the limits explicitly?\n\ndf_aug &lt;- augment(df_fit$fit)\n\nggplot(df_aug, aes(x = .fitted, y = .resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  ylim(-1000000, 1000000) +\n  labs(\n    x = \"Fitted value\", y = \"Residual\",\n    title = \"Residuals vs. fitted values\"\n  )\n\n\n\n\n\n\n\n\n\n\nExercise 2\nImprove how the values on the axes of the plot are displayed by modifying the code below.\n\nggplot(df_aug, aes(x = .fitted, y = .resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  ylim(-1000000, 1000000) +\n  labs(\n    x = \"Fitted value\", y = \"Residual\",\n    title = \"Residuals vs. fitted values\"\n  )"
  },
  {
    "objectID": "ae/ae-7-exam-2-review.html",
    "href": "ae/ae-7-exam-2-review.html",
    "title": "AE 7: Exam 2 Review",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate the repo titled ae-7-exam-2-review-YOUR_GITHUB_USERNAME to get started."
  },
  {
    "objectID": "ae/ae-7-exam-2-review.html#packages",
    "href": "ae/ae-7-exam-2-review.html#packages",
    "title": "AE 7: Exam 2 Review",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(openintro)\n\n# fix data!\nloans_full_schema &lt;- droplevels(loans_full_schema)"
  },
  {
    "objectID": "ae/ae-7-exam-2-review.html#goal",
    "href": "ae/ae-7-exam-2-review.html#goal",
    "title": "AE 7: Exam 2 Review",
    "section": "Goal",
    "text": "Goal\nCreate a model for precicting interest_rate."
  },
  {
    "objectID": "ae/ae-7-exam-2-review.html#view-data",
    "href": "ae/ae-7-exam-2-review.html#view-data",
    "title": "AE 7: Exam 2 Review",
    "section": "View data",
    "text": "View data\nNote the dimensions of the data and the variable names. Review the data dictionary.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-7-exam-2-review.html#split-data-into-training-and-testing",
    "href": "ae/ae-7-exam-2-review.html#split-data-into-training-and-testing",
    "title": "AE 7: Exam 2 Review",
    "section": "Split data into training and testing",
    "text": "Split data into training and testing\nSplit your data into testing and training sets.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-7-exam-2-review.html#write-the-model",
    "href": "ae/ae-7-exam-2-review.html#write-the-model",
    "title": "AE 7: Exam 2 Review",
    "section": "Write the model",
    "text": "Write the model\nWrite the model for predicting interest rate (interest_rate) from debt to income ratio (debt_to_income), the term of loan (term), the number of inquiries (credit checks) into the applicant’s credit during the last 12 months (inquiries_last_12m), whether there are any bankruptcies listed in the public record for this applicant (bankrupt), and the type of application (application_type). The model should allow for the effect of to income ratio on interest rate to vary by application type.\nAdd model here"
  },
  {
    "objectID": "ae/ae-7-exam-2-review.html#exploration",
    "href": "ae/ae-7-exam-2-review.html#exploration",
    "title": "AE 7: Exam 2 Review",
    "section": "Exploration",
    "text": "Exploration\nExplore characteristics of the variables you’ll use for the model using the training data only.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-7-exam-2-review.html#specify-model",
    "href": "ae/ae-7-exam-2-review.html#specify-model",
    "title": "AE 7: Exam 2 Review",
    "section": "Specify model",
    "text": "Specify model\nSpecify a linear regression model. Call it office_spec.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-7-exam-2-review.html#create-recipe",
    "href": "ae/ae-7-exam-2-review.html#create-recipe",
    "title": "AE 7: Exam 2 Review",
    "section": "Create recipe",
    "text": "Create recipe\n\nPredict interest_rate from debt_to_income, term, inquiries_last_12m, public_record_bankrupt, and application_type.\nMean center debt_to_income.\nMake term a factor.\nCreate a new variable: bankrupt that takes on the value “no” if public_record_bankrupt is 0 and the value “yes” if public_record_bankrupt is 1 or higher. Then, remove public_record_bankrupt.\nInteract application_type with debt_to_income.\nCreate dummy variables where needed and drop any zero variance variables.\n\n\n# add code here"
  },
  {
    "objectID": "ae/ae-7-exam-2-review.html#create-workflow",
    "href": "ae/ae-7-exam-2-review.html#create-workflow",
    "title": "AE 7: Exam 2 Review",
    "section": "Create workflow",
    "text": "Create workflow\nCreate the workflow that brings together the model specification and recipe.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-7-exam-2-review.html#cross-validation",
    "href": "ae/ae-7-exam-2-review.html#cross-validation",
    "title": "AE 7: Exam 2 Review",
    "section": "Cross validation",
    "text": "Cross validation\nConduct 10-fold cross validation.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-7-exam-2-review.html#summarize-cv-metrics",
    "href": "ae/ae-7-exam-2-review.html#summarize-cv-metrics",
    "title": "AE 7: Exam 2 Review",
    "section": "Summarize CV metrics",
    "text": "Summarize CV metrics\nSummarize metrics from your CV resamples.\n\n# add code here\n\nWhy are we focusing on R-squared and RMSE instead of adjusted R-squared, AIC, BIC?\n[Add response here]"
  },
  {
    "objectID": "ae/ae-7-exam-2-review.html#next-steps",
    "href": "ae/ae-7-exam-2-review.html#next-steps",
    "title": "AE 7: Exam 2 Review",
    "section": "Next steps…",
    "text": "Next steps…\nDepending on time, either\n\nCreate a workflow for another model with a new recipe (omitting the interaction variable), conduct CV, do model selection between these two, and then interpret the coefficients for the selected model.\nOr interpret the coefficients for the one model you fit.\n\nMake sure to interpret the intercept and slope coefficient for at least one numerical, one categorical, and one interaction predictor."
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html",
    "href": "ae/ae-12-exam-3-review.html",
    "title": "AE 12: Exam 3 Review",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate the repo titled ae-12-exam-3-review-YOUR_GITHUB_USERNAME to get started."
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#packages",
    "href": "ae/ae-12-exam-3-review.html#packages",
    "title": "AE 12: Exam 3 Review",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(Stat2Data)\nlibrary(rms)\nlibrary(nnet)"
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#data",
    "href": "ae/ae-12-exam-3-review.html#data",
    "title": "AE 12: Exam 3 Review",
    "section": "Data",
    "text": "Data\nAs part of a study of the effects of predatory intertidal crab species on snail populations, researchers measured the mean closing forces and the propodus heights of the claws on several crabs of three species.\n\n\nclaws &lt;- read_csv(here::here(\"ae\", \"data/claws.csv\"))\n\nWe will use the following variables:\n\nforce: Closing force of claw (newtons)\nheight: Propodus height (mm)\nspecies: Crab species - Cp(Cancer productus), Hn (Hemigrapsus nudus), Lb(Lophopanopeus bellus)\nlb: 1 if Lophopanopeus bellus species, 0 otherwise\nhn: 1 if Hemigrapsus nudus species, 0 otherwise\ncp: 1 if Cancer productus species, 0 otherwise\nforce_cent: mean centered force\nheight_cent: mean centered height\n\nBefore we get started, let’s make the categorical and indicator variables factors.\n\nclaws &lt;- claws %&gt;%\n  mutate(\n    species = as_factor(species),\n    lb = as_factor(lb),\n    hn = as_factor(hn),\n    cp = as_factor(cp)\n  )"
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#probabilities-vs.-odds-vs.-log-odds",
    "href": "ae/ae-12-exam-3-review.html#probabilities-vs.-odds-vs.-log-odds",
    "title": "AE 12: Exam 3 Review",
    "section": "Probabilities vs. odds vs. log-odds",
    "text": "Probabilities vs. odds vs. log-odds\nWhy we use log-odds as response variable: https://sta210-s22.github.io/website/slides/lec-18.html#/do-teenagers-get-7-hours-of-sleep"
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#exercise-1",
    "href": "ae/ae-12-exam-3-review.html#exercise-1",
    "title": "AE 12: Exam 3 Review",
    "section": "Exercise 1",
    "text": "Exercise 1\nFill in the blanks:\n\nUse log-odds to fit the model (outcome)\nUse odds to interpret model results\nUse probabilities to make predictions for individual observations and ultimately to make classification decisions"
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#exercise-2",
    "href": "ae/ae-12-exam-3-review.html#exercise-2",
    "title": "AE 12: Exam 3 Review",
    "section": "Exercise 2",
    "text": "Exercise 2\nSuppose we want to use force to determine whether or not a crab is from the Lophopanopeus bellus (Lb) species. Why should we use a logistic regression model for this analysis?\n\nclaws %&gt;%\n  distinct(lb)\n\n# A tibble: 2 × 1\n  lb   \n  &lt;fct&gt;\n1 0    \n2 1"
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#exercise-3",
    "href": "ae/ae-12-exam-3-review.html#exercise-3",
    "title": "AE 12: Exam 3 Review",
    "section": "Exercise 3",
    "text": "Exercise 3\nWe will use the mean-centered variables for force in the model. The model output is below. Write the equation of the model produced by R. Don’t forget to fill in the blanks for ….\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-0.798\n0.358\n-2.233\n0.026\n-1.542\n-0.123\n\n\nforce_cent\n0.043\n0.039\n1.090\n0.276\n-0.034\n0.123\n\n\n\n\n\nLet \\(\\pi\\) be probability that a crab is from Lb species.\n\\[\n\\log\\Big(\\frac{\\hat{\\pi}}{1 - \\hat{\\pi}}\\Big) = -0.798 + 0.043 * force\\_cent\n\\]"
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#exercise-4",
    "href": "ae/ae-12-exam-3-review.html#exercise-4",
    "title": "AE 12: Exam 3 Review",
    "section": "Exercise 4",
    "text": "Exercise 4\nInterpret the intercept in the context of the data.\n\nmean_force &lt;- round(mean(claws$force), 2)\n\nFor crabs with average closing force (12.13 newtons), we expect odds of the crab being Lophopanopeus bellus is 0.45 (exp(-0.798))."
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#exercise-5",
    "href": "ae/ae-12-exam-3-review.html#exercise-5",
    "title": "AE 12: Exam 3 Review",
    "section": "Exercise 5",
    "text": "Exercise 5\nInterpret the effect of force in the context of the data.\nWhen x goes up by 1 unit, we expect y to change by (slope) units.\nFor each additional unit increase in closing force, the odds of crab being from lb species multiplies on average by a factor of 1.0439379."
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#exercise-6",
    "href": "ae/ae-12-exam-3-review.html#exercise-6",
    "title": "AE 12: Exam 3 Review",
    "section": "Exercise 6",
    "text": "Exercise 6\nNow let’s consider adding height_cent to the model. Fit the model that includes height_cent. Then use AIC to choose the model that best fits the data.\n\nlb_fit_2 &lt;- logistic_reg() %&gt;%\n  set_engine(\"glm\") %&gt;%\n  fit(lb ~ force_cent + height_cent, data = claws)\n\ntidy(lb_fit_2, conf.int = TRUE)\n\n# A tibble: 3 × 7\n  term        estimate std.error statistic p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   -1.13     0.463      -2.44  0.0146  -2.17      -0.306\n2 force_cent     0.211    0.0925      2.28  0.0227   0.0563     0.424\n3 height_cent   -0.895    0.398      -2.25  0.0245  -1.82      -0.234\n\nglance(lb_fit_1)$AIC\n\n[1] 50.19535\n\nglance(lb_fit_2)$AIC\n\n[1] 44.11812"
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#exercise-7",
    "href": "ae/ae-12-exam-3-review.html#exercise-7",
    "title": "AE 12: Exam 3 Review",
    "section": "Exercise 7",
    "text": "Exercise 7\nWhat do the following mean in the context of this data. Explain and calculate them.\n\nSensitivity: P(predict lb | actual lb) = 6 / 12\nSpecificity: P(predict not lb | actual not lb) = 4/ 26\nNegative predictive power: P(actual not lb | predict not lb) = 22 / 28\nPositive predictive power: P(actual lb | predict lb) = 6 / 10\n\n\n\n\nActual\nPredict lb\nPredict not lb\nTOTAL predicted\n\n\n\n\nLb\n6\n6\n12\n\n\nNot lb\n4\n22\n26\n\n\nTOTAL actual\n10\n28\n38"
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#exercise-8",
    "href": "ae/ae-12-exam-3-review.html#exercise-8",
    "title": "AE 12: Exam 3 Review",
    "section": "Exercise 8",
    "text": "Exercise 8\nWrite the equation of the model.\n\\[\\log\\Big(\\frac{\\hat{\\pi}_{Hn}}{\\hat{\\pi}_{Cp}}\\Big) = \\]\n\\[\\log\\Big(\\frac{\\hat{\\pi}_{Lb}}{\\hat{\\pi}_{Cp}}\\Big) = \\]"
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#exercise-9",
    "href": "ae/ae-12-exam-3-review.html#exercise-9",
    "title": "AE 12: Exam 3 Review",
    "section": "Exercise 9",
    "text": "Exercise 9\n\nInterpret the intercept for the odds a crab is Hn vs. Cp species.\nInterpret the effect of force on the odds a crab is Lb vs. Cp species."
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#exercise-10",
    "href": "ae/ae-12-exam-3-review.html#exercise-10",
    "title": "AE 12: Exam 3 Review",
    "section": "Exercise 10",
    "text": "Exercise 10\nInterpret the effect of force on the odds a crab is in the Hn vs. Lb species.\nCAUTION: We can write an interpretation based on the estimated coefficients; however, we can’t make any inferential conclusions for this question based on the current model. We would need to refit the model with Lb as the baseline category to do so."
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#exercise-11",
    "href": "ae/ae-12-exam-3-review.html#exercise-11",
    "title": "AE 12: Exam 3 Review",
    "section": "Exercise 11",
    "text": "Exercise 11\nConditions for multinomial logistic (and logistic models as well):\n\nIndependence:\nRandomness:\nLinearity:\n\nemplogitplot1(lb ~ force, data = claws, ngroups = 10)\nemplogitplot1(lb ~ height, data = claws, ngroups = 10)\n\n\n\n\n\n\n\n\n\n\n# add code here for other species here\n\n\n\n# add code here for other species here"
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#checking-for-multicollinearity-in-logistic-and-multinomial-logistic",
    "href": "ae/ae-12-exam-3-review.html#checking-for-multicollinearity-in-logistic-and-multinomial-logistic",
    "title": "AE 12: Exam 3 Review",
    "section": "Checking for multicollinearity in logistic and multinomial logistic",
    "text": "Checking for multicollinearity in logistic and multinomial logistic\nSimilar to multiple linear regression, we can also check for multicollinearity in logistic and multinomial logistic models.\n\nUse the vif function to check for multicollinearity in logistic regression.\n\n\nThe vif function doesn’t work for the multinomial logistic regression models, so we can look at a correlation matrix of the predictors as a way to assess if the predictors are highly correlated:"
  },
  {
    "objectID": "ae/ae-8-rail-trail.html",
    "href": "ae/ae-8-rail-trail.html",
    "title": "AE 8: Rail Trail",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate the repo titled ae-8-rail-trail-YOUR_GITHUB_USERNAME to get started."
  },
  {
    "objectID": "ae/ae-8-rail-trail.html#packages-and-data",
    "href": "ae/ae-8-rail-trail.html#packages-and-data",
    "title": "AE 8: Rail Trail",
    "section": "Packages and data",
    "text": "Packages and data\n\nlibrary(tidyverse)\nlibrary(tidymodels)\n\nrail_trail &lt;- read_csv(\"data/rail_trail.csv\")"
  },
  {
    "objectID": "ae/ae-8-rail-trail.html#exercise-1",
    "href": "ae/ae-8-rail-trail.html#exercise-1",
    "title": "AE 8: Rail Trail",
    "section": "Exercise 1",
    "text": "Exercise 1\nFit a model predicting volume from hightemp and season.\n\nrt_mlr_main_fit &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(volume ~ hightemp + season, data = rail_trail)\n\ntidy(rt_mlr_main_fit)\n\n# A tibble: 4 × 5\n  term         estimate std.error statistic       p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n1 (Intercept)   -125.       71.7     -1.75  0.0841       \n2 hightemp         7.54      1.17     6.43  0.00000000692\n3 seasonSpring     5.13     34.3      0.150 0.881        \n4 seasonSummer   -76.8      47.7     -1.61  0.111        \n\n\nRecreate the following visualization which displays the three regression lines we can draw based on the results of this model.\n\n\n\n\n\n\n# add code here"
  },
  {
    "objectID": "ae/ae-8-rail-trail.html#exercise-2",
    "href": "ae/ae-8-rail-trail.html#exercise-2",
    "title": "AE 8: Rail Trail",
    "section": "Exercise 2",
    "text": "Exercise 2\nAdd an interaction effect between hightemp and season and comment on the significance of the interaction predictors. Time permitting, visualize the interaction model as well.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-8-rail-trail.html#exercise-3",
    "href": "ae/ae-8-rail-trail.html#exercise-3",
    "title": "AE 8: Rail Trail",
    "section": "Exercise 3",
    "text": "Exercise 3\nFit a model predicting volume from all available predictors.\n\nrt_full_fit &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(volume ~ ., data = rail_trail)\n\ntidy(rt_full_fit)\n\n# A tibble: 8 × 5\n  term            estimate std.error statistic p.value\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)        17.6      76.6      0.230 0.819  \n2 hightemp            7.07      2.42     2.92  0.00450\n3 avgtemp            -2.04      3.14    -0.648 0.519  \n4 seasonSpring       35.9      33.0      1.09  0.280  \n5 seasonSummer       24.2      52.8      0.457 0.649  \n6 cloudcover         -7.25      3.84    -1.89  0.0627 \n7 precip            -95.7      42.6     -2.25  0.0273 \n8 day_typeWeekend    35.9      22.4      1.60  0.113  \n\n\nRecreate the following visualization which displays a histogram of residuals and a normal density curve overlaid.\n\n\n\n\n\n\n# add code here"
  },
  {
    "objectID": "ae/ae-4-exam-1-review.html",
    "href": "ae/ae-4-exam-1-review.html",
    "title": "AE 4: Exam 1 Review",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate the repo titled ae-4-exam-1-review-YOUR_GITHUB_USERNAME to get started."
  },
  {
    "objectID": "ae/ae-4-exam-1-review.html#packages",
    "href": "ae/ae-4-exam-1-review.html#packages",
    "title": "AE 4: Exam 1 Review",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(ggfortify)\nlibrary(knitr)"
  },
  {
    "objectID": "ae/ae-4-exam-1-review.html#restaurant-tips",
    "href": "ae/ae-4-exam-1-review.html#restaurant-tips",
    "title": "AE 4: Exam 1 Review",
    "section": "Restaurant tips",
    "text": "Restaurant tips\nWhat factors are associated with the amount customers tip at a restaurant? To answer this question, we will use data collected in 2011 by a student at St. Olaf who worked at a local restaurant.1\nThe variables we’ll focus on for this analysis are\n\nTip: amount of the tip\nParty: number of people in the party\n\nView the data set to see the remaining variables.\n\ntips &lt;- read_csv(\"data/tip-data.csv\")"
  },
  {
    "objectID": "ae/ae-4-exam-1-review.html#exploratory-analysis",
    "href": "ae/ae-4-exam-1-review.html#exploratory-analysis",
    "title": "AE 4: Exam 1 Review",
    "section": "Exploratory analysis",
    "text": "Exploratory analysis\n\nVisualize, summarize, and describe the relationship between Party and Tip.\n\n\n# add your code here"
  },
  {
    "objectID": "ae/ae-4-exam-1-review.html#modeling",
    "href": "ae/ae-4-exam-1-review.html#modeling",
    "title": "AE 4: Exam 1 Review",
    "section": "Modeling",
    "text": "Modeling\nLet’s start by fitting a model using Party to predict the Tip at this restaurant.\n\nWrite the statistical model.\nFit the regression line and write the regression equation. Name the model tips_fit and display the results with kable() and a reasonable number of digits.\n\n\n# add your code here\n\n\nInterpret the slope.\nDoes it make sense to interpret the intercept? Explain your reasoning."
  },
  {
    "objectID": "ae/ae-4-exam-1-review.html#inference",
    "href": "ae/ae-4-exam-1-review.html#inference",
    "title": "AE 4: Exam 1 Review",
    "section": "Inference",
    "text": "Inference\n\nInference for the slope\n\nThe following code can be used to create a bootstrap distribution for the slope (and the intercept, though we’ll focus primarily on the slope in our inference). Describe what each line of code does, supplemented by any visualizations that might help with your description.\n\n\nset.seed(1234)\n\nboot_dist &lt;- tips %&gt;%\n  specify(Tip ~ Party) %&gt;%\n  generate(reps = 100, type = \"bootstrap\") %&gt;%\n  fit()\n\n\nUse the bootstrap distribution created in Exercise 6, boot_dist, to construct a 90% confidence interval for the slope using bootstrapping and the percentile method and interpret it in context of the data.\n\n\n# add your code here\n\n\nConduct a hypothesis test at the equivalent significance level using permutation. State the hypotheses and the significance level you’re using explicitly. Also include a visualization of the null distribution of the slope with the observed slope marked as a vertical line.\n\n\n# add your code here\n\n\nCheck the relevant conditions for Exercises 7 and 8. Are there any violations in conditions that make you reconsider your inferential findings?\n\n\n# add your code here\n\n\nNow repeat Exercises 7 and 8 using approaches based on mathematical models.\n\n\n# add your code here\n\n\nCheck the relevant conditions for Exercise 9. Are there any violations in conditions that make you reconsider your inferential findings?\n\n\n# add your code here\n\n\n\nInference for a prediction\n\nBased on your model, predict the tip for a party of 4.\n\n\n# add your code here\n\n\nSuppose you’re asked to construct a confidence and a prediction interval for your finding in Exercise 11. Which one would you expect to be wider and why? In your answer clearly state the difference between these intervals.\nNow construct the intervals from Exercise 12 and comment on whether your guess is confirmed.\n\n\n# add your code here"
  },
  {
    "objectID": "ae/ae-4-exam-1-review.html#model-diagnostics",
    "href": "ae/ae-4-exam-1-review.html#model-diagnostics",
    "title": "AE 4: Exam 1 Review",
    "section": "Model diagnostics",
    "text": "Model diagnostics\n\nLeverage (Outliers in x direction)\n\nWhat is the threshold used to identify observations with high leverage? Calculate the threshold and save the value as leverage_threshold.\n\n\n# add your code here\n\n\nMake a plot of the standardized residuals vs. leverage (you can do this with ggplot() or with autoplot(which = 5)). Use geom_vline() to add a vertical line to help identify points with high leverage.\n\n\n# add your code here\n\n\nLet’s dig into the data further. Which observations have high leverage? Why do these points have high leverage?\n\n\n# add your code here\n\n\n\nIdentifying outliers (outliers in y direction)\n\nMake a plot of the residuals vs. fitted values and a plot of the square root of the absolute value of standardized residuals vs. fitted (You can use autoplot(which = c(1, 3)) to display the plots side-by-side).\n\n\nHow are the plots similar? How do they differ?\nWhat is an advantage of using the plot of the residuals vs. fitted to check conditions and model diagnostics?\nWhat is an advantage of using the plot of the \\(\\sqrt{|\\text{standardized residuals}|}\\) vs. fitted to check conditions and model diagnostics?\n\n\n# add your code here\n\n\nAre there any observations that are outliers?\n\n\n# add your code here\n\n\n\nCook’s distance\n\nMake a plot to check Cook’s distance (autoplot(which = 4)). Based on this plot, are there any points that have a strong influence on the model coefficients?\n\n\n# add your code here"
  },
  {
    "objectID": "ae/ae-4-exam-1-review.html#adding-another-variable",
    "href": "ae/ae-4-exam-1-review.html#adding-another-variable",
    "title": "AE 4: Exam 1 Review",
    "section": "Adding another variable",
    "text": "Adding another variable\n\nAdd another variable, Alcohol, to your exploratory visualization. Describe any patterns that emerge.\n\n\n# add your code here\n\n\nFit a multiple linear regression model predicting Tip from Party and Alcohol. Display the results with kable() and a reasonable number of digits.\n\n\n# add your code here\n\n\nInterpret each of the slopes.\nDoes it make sense to interpret the intercept? Explain your reasoning.\nAccording to this model, is the rate of change in tip amount the same for various sizes of parties regardless of alcohol consumption or are they different? Explain your reasoning."
  },
  {
    "objectID": "ae/ae-4-exam-1-review.html#footnotes",
    "href": "ae/ae-4-exam-1-review.html#footnotes",
    "title": "AE 4: Exam 1 Review",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDahlquist, Samantha, and Jin Dong. 2011. “The Effects of Credit Cards on Tipping.” Project for Statistics 212-Statistics for the Sciences, St. Olaf College.↩︎"
  },
  {
    "objectID": "ae/ae-5-the-office.html",
    "href": "ae/ae-5-the-office.html",
    "title": "AE 5: The Office",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate the repo titled ae-5-the-office-YOUR_GITHUB_USERNAME to get started."
  },
  {
    "objectID": "ae/ae-5-the-office.html#packages",
    "href": "ae/ae-5-the-office.html#packages",
    "title": "AE 5: The Office",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(gghighlight)\nlibrary(knitr)"
  },
  {
    "objectID": "ae/ae-5-the-office.html#load-data",
    "href": "ae/ae-5-the-office.html#load-data",
    "title": "AE 5: The Office",
    "section": "Load data",
    "text": "Load data\n\noffice_ratings &lt;- read_csv(\"data/office_ratings.csv\")\n\nRows: 188 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): title\ndbl  (4): season, episode, imdb_rating, total_votes\ndate (1): air_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "ae/ae-5-the-office.html#exploratory-data-analysis",
    "href": "ae/ae-5-the-office.html#exploratory-data-analysis",
    "title": "AE 5: The Office",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nRecreate at least one of the exploratory visualizations from class."
  },
  {
    "objectID": "ae/ae-5-the-office.html#testtrain-split",
    "href": "ae/ae-5-the-office.html#testtrain-split",
    "title": "AE 5: The Office",
    "section": "Test/train split",
    "text": "Test/train split\nSplit your data into testing and training sets."
  },
  {
    "objectID": "ae/ae-5-the-office.html#build-a-recipe",
    "href": "ae/ae-5-the-office.html#build-a-recipe",
    "title": "AE 5: The Office",
    "section": "Build a recipe",
    "text": "Build a recipe\nBuild the recipe from class.\n\nTime permitting…"
  },
  {
    "objectID": "ae/ae-5-the-office.html#workflows-and-model-fitting",
    "href": "ae/ae-5-the-office.html#workflows-and-model-fitting",
    "title": "AE 5: The Office",
    "section": "Workflows and model fitting",
    "text": "Workflows and model fitting\nBuild the modeling workflow and fit the model to the training data after feature engineering with the recipe."
  },
  {
    "objectID": "hw/hw-2.html",
    "href": "hw/hw-2.html",
    "title": "HW 2 - Multiple linear regression",
    "section": "",
    "text": "In this assignment, you’ll get to put into practice the multiple linear regression skills you’ve developed.\n\n\nIn this assignment, you will…\n\nFit and interpret multiple linear regression models with main and interaction effects.\nCompare multiple linear regression models.\nReason around multiple linear regression concepts.\nContinue developing a workflow for reproducible data analysis.\n\n\n\n\nYour repo for this assignment is at github.com/sta210-s22 and starts with the prefix hw-2. For more detailed instructions on getting started, see HW 1.\n\n\n\nThe following packages will be used in this assignment. You can add other packages as needed.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(palmerpenguins)"
  },
  {
    "objectID": "hw/hw-2.html#introduction",
    "href": "hw/hw-2.html#introduction",
    "title": "HW 2 - Multiple linear regression",
    "section": "",
    "text": "In this assignment, you’ll get to put into practice the multiple linear regression skills you’ve developed.\n\n\nIn this assignment, you will…\n\nFit and interpret multiple linear regression models with main and interaction effects.\nCompare multiple linear regression models.\nReason around multiple linear regression concepts.\nContinue developing a workflow for reproducible data analysis.\n\n\n\n\nYour repo for this assignment is at github.com/sta210-s22 and starts with the prefix hw-2. For more detailed instructions on getting started, see HW 1.\n\n\n\nThe following packages will be used in this assignment. You can add other packages as needed.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(palmerpenguins)"
  },
  {
    "objectID": "hw/hw-2.html#part-1---conceptual",
    "href": "hw/hw-2.html#part-1---conceptual",
    "title": "HW 2 - Multiple linear regression",
    "section": "Part 1 - Conceptual",
    "text": "Part 1 - Conceptual\n\nDealing with categorical predictors. Two friends, Elliott and Adrian, want to build a model predicting typing speed (average number of words typed per minute) from whether the person wears glasses or not. Before building the model they want to conduct some exploratory analysis to evaluate the strength of the association between these two variables, but they’re in disagreement about how to evaluate how strongly a categorical predictor is associated with a numerical outcome. Elliott claims that it is not possible to calculate a correlation coefficient to summarize the relationship between a categorical predictor and a numerical outcome, however they’re not sure what a better alternative is. Adrian claims that you can recode a binary predictor as a 0/1 variable (assign one level to be 0 and the other to be 1), thus converting it to a numerical variable. According to Adrian, you can then calculate the correlation coefficient between the predictor and the outcome. Who is right: Elliott or Adrian? If you pick Elliott, can you suggest a better alternative for evaluating the association between the categorical predictor and the numerical outcome?\nHigh correlation, good or bad? Two friends, Frances and Annika, are in disagreement about whether high correlation values are always good in the context of regression. Frances claims that it’s desirable for all variables in the dataset to be highly correlated to each other when building linear models. Annika claims that while it’s desirable for each of the predictors to be highly correlated with the outcome, it is not desirable for the predictors to be highly correlated with each other. Who is right: Frances, Annika, both, or neither? Explain your reasoning using appropriate terminology.\nTraining for the 5K. Nico signs up for a 5K (a 5,000 metre running race) 30 days prior to the race. They decide to run a 5K every day to train for it, and each day they record the following information: days_since_start (number of days since starting training), days_till_race (number of days left until the race), mood (poor, good, awesome), tiredness (1-not tired to 10-very tired), and time (time it takes to run 5K, recorded as mm:ss). Top few rows of the data they collect is shown below.\n\n\n\ndays_since_start\ndays_till_race\nmood\ntiredness\ntime\n\n\n\n\n1\n29\ngood\n3\n25:45\n\n\n2\n28\npoor\n5\n27:13\n\n\n3\n27\nawesome\n4\n24:13\n\n\n…\n…\n…\n…\n…\n\n\n\nUsing these data Nico wants to build a model predicting time from the other variables. Should they include all variables shown above in their model? Why or why not?\nMultiple regression fact checking. Determine which of the following statements are true and false. For each statement that is false, explain why it is false.\n\nIf predictors are colinear, then removing one variable will have no influence on the point estimate of another variable’s coefficient.\nSuppose a numerical predictor \\(x\\) has a coefficient of \\(\\hat{\\beta}_1 = 2.5\\) in a multiple regression model. Suppose also that the first observation has \\(x_{1,1} = 7.2\\), the second observation has a value of \\(x_{2,1} = 8.2\\), and these two observations have the same values for all other predictors. Then the predicted value of the second observation will be 2.5 higher than the prediction of the first observation based on the multiple regression model.\nIf a regression model’s first predictor has a coefficient of \\(\\hat{\\beta}_1 = 5.7\\) and if we are able to influence the data so that an observation will have its \\(x_1\\) be 1 larger than it would otherwise, the value \\(\\hat{y}_1\\) for this observation would increase by 5.7."
  },
  {
    "objectID": "hw/hw-2.html#part-2---palmer-penguins",
    "href": "hw/hw-2.html#part-2---palmer-penguins",
    "title": "HW 2 - Multiple linear regression",
    "section": "Part 2 - Palmer penguins",
    "text": "Part 2 - Palmer penguins\nData were collected and made available by Dr. Kristen Gorman and the Palmer Station, Antarctica LTER, a member of the Long Term Ecological Research Network. (Gorman, Williams, and Fraser 2014)\n\n\n\nArtwork by @allison_horst\n\n\nThese data can be found in the palmerpenguins package. We’re going to be working with the penguins dataset from this package. The dataset contains data for 344 penguins. There are 3 different species of penguins in this dataset, collected from 3 islands in the Palmer Archipelago, Antarctica.\n\nBody mass. Our first goal is to fit a model predicting body mass (which is more difficult to measure) from bill length, bill depth, flipper length, species, and sex.\n\nFit a model predicting body mass (which is more difficult to measure) from the other variables listed above.\nWrite the equation of the regression model.\nInterpret each one of the slopes in this context.\nCalculate the residual for a male Adelie penguin that weighs 3750 grams with the following body measurements: bill_length_mm = 39.1, bill_depth_mm = 18.7, flipper_length_mm = 181. Does the model overpredict or underpredict this penguin’s weight?\nFind the \\(R^2\\) of this model and interpret this value in context of the data and the model.\n\n\n\n\nBill depth. Next we’ll be focusing on bill depth and bill length and also considering species.\n\nFit a model predicting bill depth from bill length. Find the adjusted R-squared, AIC, and BIC for this model.\nThen, add a new predictor: species. Fit another model predicting bill depth from bill length and species. Find the adjusted R-squared, AIC, and BIC for this model.\nFinally, add one more predictor: the interaction between bill length and species. Find the adjusted R-squared, AIC, and BIC for this model.\nUsing the three criteria you recorded for these three models, and with the goal of parsimony, which model is the “best” for predicting bill depth from bill length and/or species. Explain your reasoning.\nCreate a visualization representing your model from part a. Hint: Make a scatterplot of bill depth vs. bill length and add the linear model.\nCreate a visualization representing your model from part b. Hint: Same as part (e), but think about how many lines to plot and whether their slopes should be the same or different.\nCreate a visualization representing your model from part c. Hint: Same as part (f), but think about how many lines to plot and whether their slopes should be the same or different.\nBased on your visualizations from parts e - g, and with the goal of parsimony, is your answer for which model is the “best” for predicting bill depth from bill length and/or species the same as your answer in part d? Explain your reasoning."
  },
  {
    "objectID": "hw/hw-2.html#part-3---perceived-threat-of-covid-19",
    "href": "hw/hw-2.html#part-3---perceived-threat-of-covid-19",
    "title": "HW 2 - Multiple linear regression",
    "section": "Part 3 - Perceived threat of Covid-19",
    "text": "Part 3 - Perceived threat of Covid-19\nGarbe, Rau, and Toppe (2020), published in June 2020, aims to examine the relationship between personality traits, perceived threat of Covid-19 and stockpiling toilet paper. For this study titled Influence of perceived threat of Covid-19 and HEXACO personality traits on toilet paper stockpiling, researchers conducted an online survey March 23 - 29, 2020 and used the results to fit multiple linear regression models to draw conclusions about their research questions. From their survey, they collected data on adults across 35 countries. Given the small number of responses from people outside of the United States, Canada, and Europe, only responses from people in these three locations were included in the regression analysis.\nLet’s consider their results for the model looking at the effect on perceived threat of Covid-19. The model can be found on page 6 of the paper. The perceived threat of Covid was quantified using the responses to the following survey question:\n\nHow threatened do you feel by Coronavirus? [Users select on a 10-point visual analogue scale (Not at all threatened to Extremely Threatened)]\n\n\nInterpret the coefficient of Age (0.072) in the context of the analysis.\nInterpret the coefficient of Place of residence in the context of the analysis.\nThe model includes an interaction between Place of residence and Emotionality (capturing differential tendencies in to worry and be anxious).\n\nWhat does the coefficient for the interaction (0.101) mean in the context of the data?\nInterpret the estimated effect of Emotionality for a person who lives in the US/Canada.\nInterpret the estimated effect of Emotionality for a person who lives in Europe."
  },
  {
    "objectID": "hw/hw-2.html#submission",
    "href": "hw/hw-2.html#submission",
    "title": "HW 2 - Multiple linear regression",
    "section": "Submission",
    "text": "Submission\n\n\n\n\n\n\nWarning\n\n\n\nBefore you wrap up the assignment, make sure all documents are updated on your GitHub repo. We will be checking these to make sure you have been practicing how to commit and push changes.\nRemember – you must turn in a PDF file to the Gradescope page before the submission deadline for full credit.\n\n\nTo submit your assignment:\n\nGo to http://www.gradescope.com and click Log in in the top right corner.\nClick School Credentials ➡️ Duke NetID and log in using your NetID credentials.\nClick on your STA 210 course.\nClick on the assignment, and you’ll be prompted to submit it.\nMark the pages associated with each exercise. All of the pages of your lab should be associated with at least one question (i.e., should be “checked”).\nSelect the first page of your PDF submission to be associated with the “Workflow & formatting” section."
  },
  {
    "objectID": "hw/hw-2.html#grading",
    "href": "hw/hw-2.html#grading",
    "title": "HW 2 - Multiple linear regression",
    "section": "Grading",
    "text": "Grading\nTotal points available: 50 points.\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1 - 9\n45\n\n\nWorkflow & formatting\n51"
  },
  {
    "objectID": "hw/hw-2.html#footnotes",
    "href": "hw/hw-2.html#footnotes",
    "title": "HW 2 - Multiple linear regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe “Workflow & formatting” grade is to assess the reproducible workflow. This includes having at least 3 informative commit messages and updating the name and date in the YAML.↩︎"
  },
  {
    "objectID": "hw/hw-4.html",
    "href": "hw/hw-4.html",
    "title": "HW 4 - Multinomial logistic regression",
    "section": "",
    "text": "In this assignment, you’ll get to put into practice the logistic regression skills you’ve developed.\n\n\nIn this assignment, you will…\n\nFit and interpret multinomial logistic regression models.\nEvaluate model conditions\nContinue developing a workflow for reproducible data analysis.\n\n\n\n\nYour repo for this assignment is at github.com/sta210-s22 and starts with the prefix hw-4. For more detailed instructions on getting started, see HW 1.\n\n\n\nThe following packages will be used in this assignment. You can add other packages as needed.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(patchwork)"
  },
  {
    "objectID": "hw/hw-4.html#introduction",
    "href": "hw/hw-4.html#introduction",
    "title": "HW 4 - Multinomial logistic regression",
    "section": "",
    "text": "In this assignment, you’ll get to put into practice the logistic regression skills you’ve developed.\n\n\nIn this assignment, you will…\n\nFit and interpret multinomial logistic regression models.\nEvaluate model conditions\nContinue developing a workflow for reproducible data analysis.\n\n\n\n\nYour repo for this assignment is at github.com/sta210-s22 and starts with the prefix hw-4. For more detailed instructions on getting started, see HW 1.\n\n\n\nThe following packages will be used in this assignment. You can add other packages as needed.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(patchwork)"
  },
  {
    "objectID": "hw/hw-4.html#data",
    "href": "hw/hw-4.html#data",
    "title": "HW 4 - Multinomial logistic regression",
    "section": "Data",
    "text": "Data\nFor this assignment, we will analyze data from the eye witness identification experiment in Carlson and Carlson (2014). In this experiment, participants were asked to watch a video of a mock crime (from the first person perspective), spend a few minutes completing a random task, and then identify the perpetrator of the mock crime from a line up shown on the screen. Every lineup in this analysis included the true perpetrator from the video. After viewing the line-up , each participant could make one of the following decisions (id):\n\ncorrect: correctly identified the true perpetrator\nfoil: incorrectly identified the “foil”, i.e. a person who looks very similar to the perpetrator\nreject: incorrectly concluded the true perpetrator is not in the lineup\n\nThe main objective of the analysis is to understand how different conditions of the mock crime and suspect lineup affect the decision made by the participant. We will consider the following conditions to describe the decisions:\n\nlineup: How potential suspects are shown to the participants\n\nSimultaneous Lineup: Participants were shown photos of all 6 potential suspects at the same time and were required to make a single decision (identify someone from the lineup or reject the lineup).\nSequential 5 Lineup: Photos of the 6 suspects were shown one at a time. The participant was required to make a decision (choose or don’t choose) as each photo was shown. Once a decision was made, participants were not allowed to reexamine a photo. If the participant made an identification, the remaining photos were not shown. In each of these lineups the true perpetrator was always the 5th photo in the lineup.\n\nweapon: Whether or not a weapon was present in the video of the mock crime.\nfeature: Whether or not the perpetrator had a distinctive marking on his face. In this experiment, the distinctive feature was a large “N” sticker on one cheek. (The letter “N” was chosen to represent the first author’s alma mater - University of Nebraska.)\n\nThe data may be found in eyewitness.csv in the data folder.\n\new &lt;- read_csv(here::here(\"hw\", \"data/eyewitness.csv\"))\new &lt;- ew %&gt;%\n  mutate(id = as_factor(id))"
  },
  {
    "objectID": "hw/hw-4.html#exercises",
    "href": "hw/hw-4.html#exercises",
    "title": "HW 4 - Multinomial logistic regression",
    "section": "Exercises",
    "text": "Exercises\n\nLet’s begin by doing some exploratory data analysis. The univariate (single variable) plots for each of the predictor variables and the response variable are shown below.\n\n\n\n\n\n\n\n\n\n\nComplete the exploratory data analysis by creating the plots and/or summary statistics to examine the relationship between the response variable (id) and each of the explanatory variables (lineup, weapon, and feature).\n\nUsing the plots/tables from Exercise 1:\n\n\nWhat is one thing you learn about the data from the univariate plots?\nBased on the bivariate plots, do any of the predictors appear to have a significant effect on the id? Briefly explain.\n\n\nBriefly explain why you should use a multinomial logistic regression model to predict id using lineup, weapon and feature.\nFit the multinomial logistic model that only includes main effects. Display the model output.\n\n\nWhat is the baseline category for the response variable?\nInterpret the intercepts for each part of the model in terms of the odds.\nInterpret the coefficients of lineup for each part of the model in terms of the odds.\n\n\nYou want to consider all possible first-order interaction effects (interaction effects between two variables) for the model.\n\n\nUse the appropriate test to determine if there is at least one significant interaction effect.\nBased on your test, is there evidence of any significant interaction effects?\n\nRegardless of your answer to Question 5, use the model that includes the interaction terms for the remainder of the assignment.\n\nAccording to the model,\n\n\nIf there was no weapon but the perpetrator had a distinctive feature in the mock crime, how do the log-odds of reject vs. a correct ID change when there is a simultaneous lineup vs. a sequential lineup?\nIf there was no weapon but the perpetrator had a distinctive feature in the mock crime, how do the odds of reject vs. a correct ID change when there is a simultaneous lineup vs. a sequential lineup?\nWhich group of participants (i.e., which set of experimental conditions) is described by the intercept?\n\n\nAre the conditions inference met? List of the conditions, and, if relevant, create visualizations to check the conditions and evaluate whether each condition is met. Include an assessment about each condition and a brief explanation about your conclusion.\nUse the model to predict the decision made by each participant. Make a table of the predicted vs. the actual decisions.\n\n\nBriefly describe how the predicted decision is determined for each participant.\nWhat is the misclassification rate?"
  },
  {
    "objectID": "hw/hw-4.html#submission",
    "href": "hw/hw-4.html#submission",
    "title": "HW 4 - Multinomial logistic regression",
    "section": "Submission",
    "text": "Submission\n\n\n\n\n\n\nWarning\n\n\n\nBefore you wrap up the assignment, make sure all documents are updated on your GitHub repo. We will be checking these to make sure you have been practicing how to commit and push changes.\nRemember – you must turn in a PDF file to the Gradescope page before the submission deadline for full credit.\n\n\nTo submit your assignment:\n\nGo to http://www.gradescope.com and click Log in in the top right corner.\nClick School Credentials ➡️ Duke NetID and log in using your NetID credentials.\nClick on your STA 210 course.\nClick on the assignment, and you’ll be prompted to submit it.\nMark the pages associated with each exercise. All of the pages of your lab should be associated with at least one question (i.e., should be “checked”).\nSelect the first page of your PDF submission to be associated with the “Workflow & formatting” section."
  },
  {
    "objectID": "hw/hw-4.html#grading",
    "href": "hw/hw-4.html#grading",
    "title": "HW 4 - Multinomial logistic regression",
    "section": "Grading",
    "text": "Grading\nTotal points available: 50 points.\n\n\n\nComponent\nPoints\n\n\n\n\nExercises\n45\n\n\nWorkflow & formatting\n51"
  },
  {
    "objectID": "hw/hw-4.html#footnotes",
    "href": "hw/hw-4.html#footnotes",
    "title": "HW 4 - Multinomial logistic regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe “Workflow & formatting” grade is to assess the reproducible workflow. This includes having at least 3 informative commit messages and updating the name and date in the YAML.↩︎"
  },
  {
    "objectID": "weeks/week-14.html",
    "href": "weeks/week-14.html",
    "title": "Week 14",
    "section": "",
    "text": "Final project submission"
  },
  {
    "objectID": "weeks/week-9.html",
    "href": "weeks/week-9.html",
    "title": "Week 9",
    "section": "",
    "text": "Fall reading week. No lecture.\nMath assignment 2 due."
  },
  {
    "objectID": "weeks/week-13.html",
    "href": "weeks/week-13.html",
    "title": "Week 13",
    "section": "",
    "text": "Slides & Recordings\nIf you are interested in both groups’ research, it is worth attending or watching both lectures this week.\nIn-person lab session: text generation with transformers lab11"
  },
  {
    "objectID": "weeks/week-10.html",
    "href": "weeks/week-10.html",
    "title": "Week 10",
    "section": "",
    "text": "Week 10 Overview\nSlides & Recordings\nTransformer video: https://www.youtube.com/watch?v=XSSTuhyAmnI\nhttps://nlp.seas.harvard.edu/2018/04/03/attention.html\nhttp://peterbloem.nl/blog/transformers\nLast year’s A3 (a seq2seq autoencoder) http://modelai.gettysburg.edu/2021/headlines/\nIn-class Exercise & Solutions\nIn-person lab session: gradcam and input gradients lab09\nFinal Project - Deadline to form project groups on Markus, and submit a project proposal"
  },
  {
    "objectID": "weeks/week-3.html",
    "href": "weeks/week-3.html",
    "title": "Week 3",
    "section": "",
    "text": "Week 3 Overview\nGloVe embedding demo \nVideo on autodiff\nSlides & Recordings\nIn-class Exercise & Solutions\nProf. Roger Grosse’s notes on autodiff and word embeddings\nNotes on Backpropagation https://cs231n.github.io/optimization-2/\nAutomatic Differentiation in Machine Learning: a Survey (2018) https://arxiv.org/pdf/1502.05767.pdf\nIn-person lab session: numerical gradients / word embeddings lab02\nLec3 pre-recorded videos with quizzes"
  },
  {
    "objectID": "weeks/week-4.html",
    "href": "weeks/week-4.html",
    "title": "Week 4",
    "section": "",
    "text": "Week 4 Overview\nSlides & Recordings\nDemo: Colab Notebook\nIn-class Exercise & Solutions\nIn-person lab session: classification and medical MNIST lab03\nLecture 4 pre-recorded videos and quizzes\nMath Assignment 1 posted"
  },
  {
    "objectID": "weeks/week-7.html",
    "href": "weeks/week-7.html",
    "title": "Week 7",
    "section": "",
    "text": "Week 7 Overview\nSlides & Recordings\nTroubleshooting Deep Neural Networks\nIn-class Exercise & Solutions\nIn-person midterm #1 which covers weeks 1-5. Held during the tutorial / lab sessions\nMath assignment 2 posted"
  },
  {
    "objectID": "weekly-material.html",
    "href": "weekly-material.html",
    "title": "Weekly Materials",
    "section": "",
    "text": "Software Installations\n\n\n\nSlides & Recordings\n\n\n\nNo tutorials/labs this week but please complete the Pre-Requisite Math Problems\nSuggested Review :\n\nLinear Algebra review \nProbability review"
  },
  {
    "objectID": "weekly-material.html#week-1",
    "href": "weekly-material.html#week-1",
    "title": "Weekly Materials",
    "section": "",
    "text": "Software Installations\n\n\n\nSlides & Recordings\n\n\n\nNo tutorials/labs this week but please complete the Pre-Requisite Math Problems\nSuggested Review :\n\nLinear Algebra review \nProbability review"
  },
  {
    "objectID": "weekly-material.html#week-2",
    "href": "weekly-material.html#week-2",
    "title": "Weekly Materials",
    "section": "Week 2",
    "text": "Week 2\n\nLecture\nSlides & Recordings\nProf. Roger Grosse’s notes on backdrop\n\n\nExercises\nIn-class Exercise & Solutions\n\n\nTutorials/Labs\nIn-person lab session: PyTorch basics with linear models lab01\n\n\nQuiz\nLec2 pre-recorded videos with quizzes"
  },
  {
    "objectID": "weekly-material.html#week-3",
    "href": "weekly-material.html#week-3",
    "title": "Weekly Materials",
    "section": "Week 3",
    "text": "Week 3\n\nLecture\nGloVe embedding demo \nVideo on autodiff\nSlides & Recordings\n\n\nExercises\nIn-class Exercise & Solutions\n\n\nTutorials/Labs\nIn-person lab session: numerical gradients / word embeddings lab02\n\n\nQuiz\nLec3 pre-recorded videos with quizzes\n\n\nAdditional Resources\nProf. Roger Grosse’s notes on autodiff and word embeddings\nNotes on Backpropagation https://cs231n.github.io/optimization-2/\nAutomatic Differentiation in Machine Learning: a Survey (2018) https://arxiv.org/pdf/1502.05767.pdf"
  },
  {
    "objectID": "weekly-material.html#week-4",
    "href": "weekly-material.html#week-4",
    "title": "Weekly Materials",
    "section": "Week 4",
    "text": "Week 4\n\nLecture\nSlides & Recordings\nDemo: Colab Notebook\n\n\nExercises\nIn-class Exercise & Solutions\n\n\nTutorials/Labs\nIn-person lab session: classification and medical MNIST lab03\n\n\nQuiz\nLecture 4 pre-recorded videos and quizzes\n\n\nAssignment\nMath Assignment 1 posted"
  },
  {
    "objectID": "weekly-material.html#week-5",
    "href": "weekly-material.html#week-5",
    "title": "Weekly Materials",
    "section": "Week 5",
    "text": "Week 5\n\nLecture\nSlides & Recordings\n\n\nExercises\nIn-class Exercise & Solutions\n\n\nTutorials/Labs\nTutorial: how to implement SGD with momentum neural network optimization\n\n\nQuiz\nLecture 5 pre-recorded videos and quizzes\n\n\nAdditional Resources\nTaylor Series https://www.youtube.com/watch?v=3d6DsjIBzJ4"
  },
  {
    "objectID": "weekly-material.html#week-6",
    "href": "weekly-material.html#week-6",
    "title": "Weekly Materials",
    "section": "Week 6",
    "text": "Week 6\n\nLecture\nSlides & Recordings\nEnsembling code skeleton\nThe definition of differential privacy\n\n\nExercises\nIn-class Exercise & Solutions\n\n\nTutorials/Labs\nIn-person lab session: optimization and differential privacy lab05\n\n\nQuiz\nLecture 6 pre-recorded videos and quizzes\n\n\nAssignment\nMath assignment 1 due"
  },
  {
    "objectID": "weekly-material.html#week-7",
    "href": "weekly-material.html#week-7",
    "title": "Weekly Materials",
    "section": "Week 7",
    "text": "Week 7\n\nLecture\nSlides & Recordings\nTroubleshooting Deep Neural Networks\n\n\nExercises\nIn-class Exercise & Solutions\n\n\nTutorials/Labs\nIn-person midterm #1 which covers weeks 1-5. Held during the tutorial / lab sessions\n\n\nAssignment\nMath assignment 2 posted"
  },
  {
    "objectID": "weekly-material.html#week-8",
    "href": "weekly-material.html#week-8",
    "title": "Weekly Materials",
    "section": "Week 8",
    "text": "Week 8\n\nLecture\nSlides\nMissing recording due to technical issue\n\n\nExercises\nIn-class Exercise & Solutions\n\n\nTutorials/Labs\nIn-person lab session: transfer learning and double descent lab07\n\n\nQuiz\nLecture 8 pre-recorded videos and quizzes"
  },
  {
    "objectID": "weekly-material.html#week-9",
    "href": "weekly-material.html#week-9",
    "title": "Weekly Materials",
    "section": "Week 9",
    "text": "Week 9\nFall reading week. No lecture.\nMath assignment 2 due."
  },
  {
    "objectID": "weekly-material.html#week-10",
    "href": "weekly-material.html#week-10",
    "title": "Weekly Materials",
    "section": "Week 10",
    "text": "Week 10\n\nLecture\nSlides & Recordings\n\n\nExercises\nIn-class Exercise & Solutions\n\n\nTutorials/Labs\nIn-person lab session: gradcam and input gradients lab09\n\n\nFinal Project\nDeadline to form project groups on Markus, and submit a project proposal\n\n\nAdditional Resources\nTransformer video: https://www.youtube.com/watch?v=XSSTuhyAmnI\nhttps://nlp.seas.harvard.edu/2018/04/03/attention.html\nhttp://peterbloem.nl/blog/transformers\nLast year’s A3 (a seq2seq autoencoder) http://modelai.gettysburg.edu/2021/headlines/"
  },
  {
    "objectID": "weekly-material.html#week-11",
    "href": "weekly-material.html#week-11",
    "title": "Weekly Materials",
    "section": "Week 11",
    "text": "Week 11\n\nLecture\nSlides & Recordings\nAutoencoder Notebook\n\n\nExercises\nIn-class Exercise & Solutions\n\n\nTutorials/Labs\nIn-person midterm #2 which covers weeks 6-9. Held during the tutorial / lab sessions\n\n\nQuiz\nLecture 10 pre-recorded videos and quizzes\n\n\nFinal Project\nWritten feedback on project proposals sent by TAs and instructors."
  },
  {
    "objectID": "weekly-material.html#week-12",
    "href": "weekly-material.html#week-12",
    "title": "Weekly Materials",
    "section": "Week 12",
    "text": "Week 12\n\nLecture\nSlides & Recordings\n\n\nExercises\nIn-class Exercise & Solutions\n\n\nTutorials/Labs\nIn-person lab session: RNN text classification lab10"
  },
  {
    "objectID": "weekly-material.html#week-13",
    "href": "weekly-material.html#week-13",
    "title": "Weekly Materials",
    "section": "Week 13",
    "text": "Week 13\n\nLecture\nSlides & Recordings\nIf you are interested in both groups’ research, it is worth attending or watching both lectures this week.\n\n\nTutorials/Labs\nIn-person lab session: text generation with transformers lab11"
  },
  {
    "objectID": "weekly-material.html#week-14",
    "href": "weekly-material.html#week-14",
    "title": "Weekly Materials",
    "section": "Week 14",
    "text": "Week 14\nFinal project submission"
  },
  {
    "objectID": "course-links.html",
    "href": "course-links.html",
    "title": "Useful Resources and Links",
    "section": "",
    "text": "Recommended Simulators\nYou are encouraged to use the simplest possible simulator to accomplish the task you are interested in. You can submit links to simulators not included here by opening a github issue.\n\n\n\nSimulately\nA detailed wiki comparing various widely-used robotics simulators. Read this first. The rest of this table shows simulators and environments not mentioned by Simulately.\n\n\nIsaac Lab (formerly Isaac Orbit / Isaac Gym)\nLayer of abstraction and tools to make using Isaac Sim easier.\n\n\nDrake Simulator\nA framework of simulation, analysis and control tools for robotics.\n\n\nDeepmind Control Suite\nSet of robotics environments on top of Mujoco.\n\n\nMujoco Menagerie\nHigh-quality description files and assets for robots, built on top of Mujoco.\n\n\nOpenAI Gym\nAtari, Mujoco, classic control, and third-party environments for RL.\n\n\nRoboSuite\nRobotics simulation environments on top of Mujoco. Also a benchmark.\n\n\nKlampt\nModeling, simulating, planning, and optimization for complex robots, particularly for manipulation and locomotion tasks.\n\n\nDART\nPhysics simulator for robotics and animation.\n\n\nCARLA\nSelf-driving environment and benchmarks on top of the Unreal simulation engine.\n\n\nAirSim\nRobotics simulation environments for flying and driving, built on top of Unreal engine.\n\n\ngym-pybullet-drones\nRobotics simulation environments and tools for quadrotors on top of PyBullet.\n\n\nHabitat 3.0\nSimulation of indoor scenes, humans, and robots. Good for visual navigation and social navigation tasks.\n\n\nGPUDrive\nGPU-accelerated multi-agent driving simulator.\n\n\nProcGen\nProcedurally generated simulation environments (not robotics, but useful).\n\n\nRaiSim\nRigid body physics engine. Supports biomechanics of human motion, as well as quadrupeds.\n\n\nFlightmare\nSimulation environment for flying vehicles built on top of the Unity simulation engine.\n\n\nIKEA Furniture Assembly\nIKEA furniture assembly environment.\n\n\nFurnitureBench\nSimulators, datasets, and real environments for furniture assembly\n\n\nRLBench\nSimulation environments for manipulation, built on top of the CoppeliaSim simulator.\n\n\nALFRED\nSimulation environments for visual and language-based navigation and manipulation tasks.\n\n\nMyoSuite\nMuscosceletal simulation environments for biomechanics, based on Mujoco.\n\n\nMetaWorld\nMulti-task RL environments and benchmarks.\n\n\nBimanual Manipulation Gym\nBimanual manipulation environments\n\n\n\n\n\nRecommended datasets\n\n\n\nSimulately\nA detailed wiki comparing various widely-used robotics datasets. Read this first. The rest of this table shows datasets not mentioned by Simulately.\n\n\nD4RL\nManipulation and navigation datasets for offline RL\n\n\nRoboMimic\nManipulation datasets and imitation learning algorithms\n\n\nMimicGen\nAutomatic augmentation of manipulation datasets starting from human demonstrations\n\n\nOptimus\nAutomatically generating long-horizon manipulation dataset from Task and Motion Planners.\n\n\nDROID\nManipulation dataset across various labs and robots\n\n\nD4RL\nManipulation and navigation datasets for offline RL\n\n\n\n\n\nRecommended RL, IL, trajectory optimization, and motion planning libraries\n\n\n\nSimulately\nA detailed wiki comparing various widely-used RL libaries. Read this first. The rest of this table shows libraries not mentioned by Simulately.\n\n\nRSL RL\nRL library used for training quadrupeds at the RSL lab at ETHZ. Used in Isaac Lab.\n\n\nSTORM\nMPC motion planner on the GPU\n\n\nOMPL\nOpen motion planning library\n\n\nMink\nInverse kinematics library, built on top of pink and pinocchio\n\n\nPureJaxRL\nRL library in JAX, with training and environments running fully on GPU\n\n\nCleanRL\nClean implementations of Online RL baselines\n\n\nClean Offline RL\nClean implementations of Offline RL baselines\n\n\nrliable\nMethod and library for reliable evaluation of RL algorithms\n\n\nDiffusion policy\nImplementation of diffusion policy in action space for imitation learning\n\n\nImplicit behavior cloning\nImplementation of behavior cloning with energy based models\n\n\nTheseus\nA library for differentiable nonlinear optimization in Pytorch\n\n\nModel-based RL algorithms\nList of model-based RL algorithms"
  },
  {
    "objectID": "labs/lab09.html",
    "href": "labs/lab09.html",
    "title": "CSC413 Lab 9: GradCAM and Input Gradients",
    "section": "",
    "text": "We have seen that convolutional neural networks (CNN) are successful in many computer vision tasks, including classification, object detection and others. However, it is not immediately clear how CNNs work, and how one can explain the predictions made by CNNs. A deeper understanding of how CNNs work can also help us identify reasons why CNNs may fail to produce correct predictions for some samples.\nA line of work started to visualize and interpret computed features of convolutional neural networks. CAM and Grad-CAM are two influential and fundamental works to find which parts of the input have the most impact on the final output of the models by analyzing the model’s extracted feature maps.\nBy the end of this lab, you will be able to:\nAcknowledgements: 2. We have borrowed some codes from CAM Official Repo. 3. We have borrowd texts, figures and formulas from main papers of CAM and Grad-CAM."
  },
  {
    "objectID": "labs/lab09.html#submission",
    "href": "labs/lab09.html#submission",
    "title": "CSC413 Lab 9: GradCAM and Input Gradients",
    "section": "Submission",
    "text": "Submission\nIf you are working with a partner, start by creating a group on Markus. If you are working alone, click “Working Alone”.\nSubmit the ipynb file lab09.ipynb on Markus containing all your solutions to the Graded Tasks. Your notebook file must contain your code and outputs where applicable, including printed lines and images. Your TA will not run your code for the purpose of grading.\nFor this lab, you should submit the following:\n\nPart 1. Your implementation of the predict function (1 point)\nPart 1. Your implementation of the get_resnet_features function (1 point)\nPart 1. Your implementation of the compute_cam function (3 point)\nPart 1. Your interpretation of the grad cam outputs (2 point)\nPart 2. Your implementation of the compute_gradcam function (3 point)"
  },
  {
    "objectID": "labs/lab09.html#part-1.-class-activation-maps-cam",
    "href": "labs/lab09.html#part-1.-class-activation-maps-cam",
    "title": "CSC413 Lab 9: GradCAM and Input Gradients",
    "section": "Part 1. Class Activation Maps (CAM)",
    "text": "Part 1. Class Activation Maps (CAM)\nCAM and its extension Grad-CAM takes the approach of identifying the regions of the an image that contributes most to the model’s prediction. This information can be visualized as a heat map, and provides a way to interpret a model’s prediction: did the model predict that the image is that of a “boat” because of the shape of the ears, or because of the water in the background?\nWe discussed, during lecture, that convolutional layers preserve the geometry of the image, and that these convolutional layers actually behave as feature/ object detectors of various complexity. Since the geometry of the output of a CNN layer corresponds to the geometry of input image, it is straightforward to locate the region of the image that corresponds to a particularly high activation value. This is because the computations that we use in a CNN (convolutions, max pooling, activations) are all geometry preserving (equivariant).\nHowever, fully-connected layers are typically used for classification in the final layers of a CNN. These fully-connected layers are not geometry preserving, thus information about the locations of discriminating features are lost when fully-connected layers are used for classification.\nThe idea behind CAM is to avoid using these fully-connected layers for classification, so that we can reconstruct location information in a straightforward way. Instead of fully-connected layers, we use:\n\nA global average pooling (GAP) layer. This layer will take as input the output of a CNN layer (e.g., of shape H x W x C) and perform an average operation for each channel along the entire activation height/width (producing an output vector of shape C).\nA single linear layer to map this vector (of length C) into the output space.\n\nSince both the pooling and linear layers have straightforward computation, it is possible to assign credit for a output score for a class back to specific activation values of the CNN output.\nThe framework of the Class Activation Mapping is as below (from https://github.com/zhoubolei/CAM):\n\nIn this part of the lab, we will implement CAM to produce a heatmap of the contribution to locations in the image to a predicted class. We will use the pre-trained convolutional neural network ResNet, chosen because this model’s architecture uses global average pooling (GAP). ResNet is trained on the ImageNet data set.\n\nfrom torchvision import models, transforms\nimport torch.nn.functional as F\nimport torch\nimport json\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\n# Download resnet pretrained weights\nresnet = models.resnet18(pretrained=True)\n\n# CAM can only be used when the models are in \"evaluation phase\".\nresnet.eval()\n\n# Print model architecture\n# We will use the CNN activations computed after layer 4, and before GAP.\nresnet\n\nThe ImageNet labels are a bit challenging to read. We will download a list of human-readable labels from here:\n\n!wget https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json\n\n\n# Load the imagenet category list\nwith open('imagenet-simple-labels.json') as f:\n    classes = json.load(f)\n\nTo remind ourselves of how ResNet works, let’s predict what class these images belongs to:\n \n\n!wget https://www.cs.toronto.edu/~lczhang/413/cat.jpg\n!wget https://www.cs.toronto.edu/~lczhang/413/boat.jpg\n\n\nfrom PIL import Image\n\ndef process_input(image_file):\n    # open the image\n    img = Image.open(image_file)\n\n    # transform the images by resizing and normalizing\n    preprocess = transforms.Compose([\n       transforms.Resize((224,224)),\n       transforms.ToTensor(),\n       transforms.Normalize(\n          mean=[0.485, 0.456, 0.406],\n          std=[0.229, 0.224, 0.225])])\n\n    return preprocess(img).unsqueeze(0)\n\nGraded Task: Write a function that takes a model and an image file and produces a list of the top 5 predictions with the corresponding probability score.\n\ndef predict(model, image_file):\n    \"\"\"\n    Return the top 5 class predictions with the corresponding probability score.\n\n    Parameters:\n        `model`      - nn.Module\n        `image_file` - file path to the image\n\n    Returns: a list of 5 (string, int, float) pairs: \n        the string is the predicted ResNet class name (see classes above),\n        the int is the predicted ResNet class id,\n        and the float is the prediction probability. The list should be ordered\n        so that the highest probabilty score appears first.\n\n    Example:\n        &gt;&gt;&gt; predict(resnet, 'cat.jpg')\n        [('prison', 743, 0.23517875373363495),\n         ('shopping cart', 791, 0.07393667101860046),\n         ('rocking chair', 765, 0.06884343922138214),\n         ('wheelbarrow', 428, 0.06603048741817474),\n         ('ring-tailed lemur', 383, 0.0434008426964283)]\n    \"\"\"\n    x = process_input(image_file)\n\n    result = None # TODO\n\n    return result\n\nPlease include the output of the below cell in your submission.\n\npredict(resnet, 'cat.jpg')\n\nNow that we can use ResNet to make predictions, we need two additional pieces of information for CAM.\nFirst, given an image, we need to be able to compute the features/activations of the last convolutional layer. This feature map is the input to the GAP layer. Although this information is computed in a forward pass, we will need to write some code to extract this information.\nSecond, we will need the weights of the final fully-connected layer in ResNet.\nGraded Task: Complete the following function that takes an image file and produces the weights of the finally fully-connected layer in Resnet. You may find the named_children() method of resnet helpful, as it produces a sequence of (named) layers. We would like the feature map directly before the global average pooling layer.\n\nfor (name, model) in resnet.named_children():\n    print(name)\n\n\ndef get_resnet_features(image_file):\n    \"\"\"\n    Return the final CNN layer (layer4) feature map in resnet\n\n    Parameters:\n        `image_file` - file path to the image\n\n    Returns: PyTorch tensor of shape [1, 512, 7, 7]\n    \"\"\"\n\n    x = process_input(image_file)\n\n    result = None # TODO\n    for (name, model) in resnet.named_children():\n        # TODO -- update result\n        if name == 'layer4':\n            break\n    return result\n\n\nfets = get_resnet_features('cat.jpg')\nprint(fets.shape) # should be [1, 512, 7, 7]\n\nTask: Assign the variable fc_weight to the weights of the final fully-connected layer in resnet.\n\nweights = None # TODO\nprint(weights.shape) # should be [1000, 512]\n\nGraded Task: Complete the function compute_cam, which takes the CNN feature map (from the get_resnet_features function), the ImageNet label of interest, and produces a heat map of the features that contribute to the label score according to the following approach.\nWe will use the notation \\({\\bf X}\\) to denote the CNN feature map (the input to the GAP), with \\(X_{i,j,c}\\) being the activation at location \\((i, j)\\) and channel \\(c\\). Here, \\({\\bf X}\\) is a tensor with shape \\(H \\times W \\times C\\), where \\(H \\times W\\) is the height and width of the feature map and \\(C\\) is the number of channels. We will use the vector \\({\\bf h}\\) to denote the output of the GAP, so that \\(h_c = \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} X_{i,j,c}\\). Finally, we will use \\({\\bf W}\\) to denote the finally fully connected layer weights, and \\({\\bf z}\\) to the denote the prediction score, so that \\({\\bf z} = {\\bf W}{\\bf h}\\).\nNow, we would like to relate the features \\(X_{i,j,c}\\) to the scores \\(z_k\\), so that we can compute the contribution of the features at position \\((i,j)\\) to the score \\(k\\).\nFor an output class \\(k\\), we have:\n\\[z_k  = \\sum_{c=1}^{C} w_{k,c} h_c\\]\nSubstituing \\(h_c\\) for its definition, we have:\n\\[z_k  = \\sum_{c=1}^{C} w_{k,c} \\frac{1}{HW} \\sum_{i=1}^H \\sum_{j=1}^W X_{i,j,c}\\]\nRearranging the sums, we have:\n\\[z_k  = \\frac{1}{HW} \\sum_{i=1}^H \\sum_{j=1}^W \\sum_{c=1}^C w_{k,c} X_{i,j,c}\\]\nThe inner term \\(\\sum_c w_{k,c} X_{i,j,c}\\) is exactly what we are looking for: this term indicates how much the value in location \\((i, j)\\) of the feature map \\({\\bf X}\\) attributes to class \\(k\\).\n\ndef compute_cam(features, label):\n    \"\"\"\n    Computes the contribution of each location in `features` towards \n    `label` using CAM.\n\n    Parameters:\n        `features`: PyTorch Tensor of shape [1, 512, 7, 7] representing\n                    final layer feature map in ResNet (e.g., from calling\n                    `get_resnet_features`)\n        `label`   : resnet label, integer between 0-999\n\n    Returns: PyTorch Tensor of shape [7, 7]\n    \"\"\"\n    features = features.squeeze(0) # remove the first dimension\n    result = None # TODO \n    return result\n\nTask: Run the below code, which superimposes the result of the compute_cam operation on the image.\n\ndef visualize_cam(image_file, label):\n    # open the image\n    img = Image.open(image_file)\n\n    # compute CAM features\n    fets = get_resnet_features('cat.jpg')\n    m = compute_cam(fets, label)\n\n    # normalize \"m\"\n    m = m - m.min()\n    m = m / m.max()\n    # convert \"m\" into pixel intensities\n    m = np.uint8(255 * m.detach().numpy())\n    # apply a color map\n    m = cv2.resize(m, img.size)\n    heatmap = cv2.applyColorMap(m, cv2.COLORMAP_JET)\n\n    plt.figure(figsize=(6, 6))\n    plt.title(\"%s %s\" % (image_file, classes[label]))\n    plt.imshow((0.3 * heatmap + 0.5 * np.array(img)).astype(np.uint8)) # superimpose heat map on img\n    plt.show()\n\n\nvisualize_cam('cat.jpg', 743)\nvisualize_cam('cat.jpg', 383)\n\nGraded Task Compare the above two outputs, and explain what conclusion you may be able to draw about the contribution of the pixel locations to those two classes. Why do you think the model misclassified the image?\n\n# TODO: your explanation goes here"
  },
  {
    "objectID": "labs/lab09.html#part-2.-grad-cam",
    "href": "labs/lab09.html#part-2.-grad-cam",
    "title": "CSC413 Lab 9: GradCAM and Input Gradients",
    "section": "Part 2. Grad-CAM",
    "text": "Part 2. Grad-CAM\nAlthough CAM was an important step toward understanding convolutional neural networks, the technique is only applicable to convolutional networks with GAP and a single fully-connected layer. Recall that it leveraged the following relationship between the output of the GAP layer \\({\\bf h}\\) and the score for the output class \\(z_k\\):\n\\[z_k  = \\sum_c w_{k,c} h_c\\]\nWhere \\(w_{k,c}\\) is the fully connected layer weight that describes the strength of the connection betwee \\(h_c\\) and \\(z_k\\). In other words, \\(w_{k,c}\\) describes the following gradient:\n\\[\\frac{\\partial z_k}{\\partial h_c}\\]\nWith this in mind, you may be able to see how CAM may be generalized so that \\({\\bf z}\\) may be a more complex function of \\({\\bf h}\\)—e.g., a MLP or even an RNN!\nGradient-weighted Class Activation Mapping (Grad-CAM) is a generalized form of CAM, and can be applied to any convolutional neural network. In Grad-CAM, we use the gradient \\(\\frac{\\partial z_k}{\\partial h_c}\\) in place of \\(w_{k,c}\\) when attributing class scores to locations \\((i, j)\\). In other words, the below term indicates how much the value in location \\((i, j)\\) of the feature map \\({\\bf X}\\) attributes to class \\(k\\).\n\\[ReLU(\\sum_c \\frac{\\partial z_k}{\\partial h_c} X_{i,j,c})\\]\nThe addition of the ReLU activation only allows positive contributions to be visualized.\nSidenote: To generalize this result even further, we can replace \\(z_k\\) with any target we would like! Grad-CAM has been used on neural networks that performs image caption generation: a model with a CNN encoder and an RNN decoder. We can use use the gradients of any target concept (say “dog” in a classification network or a sequence of words in a captioning network) flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. Taking a look at this video helps you to understand the power of Grad-CAM.\nLet’s explore GradCAM with the VGG network:\n\nvgg19 = models.vgg19(pretrained=True)\nvgg19.eval()\nvgg19\n\n\npredict(vgg19, 'cat.jpg')\n\nTask Just like with CAM, we will need to extract the feature map obtained from the last convolutional layer. This step is actually very straightforward with VGG since vgg19 splits the network into a features network and a classifier network.\n\ndef get_vgg_features(image_file):\n    \"\"\"\n    Return the output of `vgg19.features` network for the image\n\n    Parameters:\n        `image_file` - file path to the image\n\n    Returns: PyTorch tensor of shape [1, 512, 7, 7]\n    \"\"\"\n\n    x = process_input(image_file)\n    result = None # TODO\n    return result\n\n\nget_vgg_features('cat.jpg').shape\n\nTask: Read the forward method of the VGG model here. https://github.com/pytorch/vision/blob/main/torchvision/models/vgg.py What other steps are remaining in the forward pass?\n\n# TODO: Explain the remaining steps here\n\nGraded Task: Complete the function compute_gradcam, which takes an image file path, the ImageNet label of interest, and produces a heat map of the features that contribute to the label score according to the GradCAM approach described at the beginning of Part 2.\n\ndef compute_gradcam(image_file, label):\n    \"\"\"\n    Computes the contribution of each location in `features` towards \n    `label` using GradCAM.\n\n    Parameters:\n        `image_file` - file path to the image\n        `label`   : resnet label, integer between 0-999\n\n    Returns: PyTorch Tensor of shape [7, 7]\n    \"\"\"\n    # obtain the image input features\n    x = process_input(image_file)\n\n    # obtain the output of the features network in the CNN\n    fets = vgg19.features(x)\n\n    # tell PyTorch to compute the gradients with respect\n    # to \"fets\"\n    fets.retain_grad()\n\n    # TODO: compute the rest of the vgg19 forward pass from `fets`\n    out = None # should be the output of the classifier network\n\n    z_k = out.squeeze(0)[label] # identify the target output class\n    z_k.backward()              # backpropagation to compute gradients\n\n    features_grad = fets.grad   # identify the gradient of z_k with respect to fets\n\n    # account for the pooling operation, so that \"pooled_grad\"\n    # aligns with the notation used\n    n, c, h, w = features_grad.shape\n    features_grad = torch.reshape(features_grad, (c, h*w))\n    pooled_grad = features_grad.sum(dim=1)\n\n    # rearrange \"fets\" so that \"X\" aligns with the notation\n    # used above\n    X = fets.squeeze(0).permute((1, 2, 0))\n\n    # TODO: Compute the heatmap using the gradcam\n    m = None\n    m = F.relu(m) # apply the ReLU operation\n    return m\n\nTask: Run the below code, which superimposes the result of the compute_gradcam operation on the image.\n\ndef visualize_gradcam(image_file, label):\n    # open the image\n    img = Image.open(image_file)\n\n    # compute CAM features\n    m = compute_gradcam(image_file, label)\n\n    # normalize \"m\"\n    m = m - m.min()\n    m = m / m.max()\n    # convert \"m\" into pixel intensities\n    m = np.uint8(255 * m.detach().numpy())\n    # apply a color map\n    m = cv2.resize(m, img.size)\n    heatmap = cv2.applyColorMap(m, cv2.COLORMAP_JET)\n\n    plt.figure(figsize=(6, 6))\n    plt.title(\"%s %s\" % (image_file, classes[label]))\n    plt.imshow((0.3 * heatmap + 0.5 * np.array(img)).astype(np.uint8)) # superimpose heat map on img\n    plt.show()\n\n\nvisualize_cam('cat.jpg', 743)\n\n\nvisualize_cam('cat.jpg', 383)\n\n\nvisualize_cam('boat.jpg', 536)"
  },
  {
    "objectID": "labs/lab09.html#just-for-fun",
    "href": "labs/lab09.html#just-for-fun",
    "title": "CSC413 Lab 9: GradCAM and Input Gradients",
    "section": "Just For Fun",
    "text": "Just For Fun\nAs you might have seen in the video, Grad-CAM can be applied to text-generating models. For example, in image-captioning tasks, a text is generated describing the given image. Some methods first feed the image to a convolutional neural network to extract features, and then feed the extracted features to an RNN, to generate the text. Neuraltalk2 was one of the earliest models using this approach. Similar to the classification task, it is enough to compute the gradient of the score (what is the score in an image-captioning task?) with respect to the last convolutional layer.\nIf you are interested in how neuraltalk2 functions you can check this project. Moreover, if you are looking for more hands-on experience, this repo has implemented many image-captioning methods, and you can easily apply Grad-CAM on them (especially show and tell).\nHint: There is a file which re-implements the forward pipeline of ResNet101, where you can store the features."
  },
  {
    "objectID": "labs/lab11.html",
    "href": "labs/lab11.html",
    "title": "CSC413 Lab 11: Text Generation with Transformers",
    "section": "",
    "text": "In this lab, we will build a generative transformer to generate new lines that emulates the TV show Friends. In order to do so, we will leverage Andrej Karpathy’s implementation of GPT2 called the nanoGPT. This particular implementation uses a small number of components and focuses on the essential ideas behind the GPT2 model.\nInstead of training a GPT model from scratch, we will fine-tune a pre-trained model. This reduces training time necessary to achieve a reasonable measure of performance.\nBy the end of this lab, you will be able to:\nAcknowledgements:\nPlease work in groups of 1-2 during the lab."
  },
  {
    "objectID": "labs/lab11.html#submission",
    "href": "labs/lab11.html#submission",
    "title": "CSC413 Lab 11: Text Generation with Transformers",
    "section": "Submission",
    "text": "Submission\nIf you are working with a partner, start by creating a group on Markus. If you are working alone, click “Working Alone”.\nSubmit the ipynb file lab11.ipynb on Markus containing all your solutions to the Graded Tasks. Your notebook file must contain your code and outputs where applicable, including printed lines and images. Your TA will not run your code for the purpose of grading.\nFor this lab, you should submit the following:\n\nPart 1. Your code to generate the list uts. (1 point)\nPart 1. Your explanation for why the tokenization method needs to be consistent. (1 point)\nPart 1. Your explanation of why the target tensor is an “offset” of the input tensor. (1 point)\nPart 2. Your explanation of the relationship between lm_head and wte. (1 point)\nPart 2. Your result for the shape of x in the GPT.forward() method. (1 point)\nPart 2. Your computation of the shape of (q @ k.transpose(-2, -1)) in the causal self attention module (1 point)\nPart 2. Your explantion of why masking makes sense intuitively for causal self attention. (1 point)\nPart 2. Your computation of the number of parameters in a GPT2 model. (3 points)"
  },
  {
    "objectID": "labs/lab11.html#part-1.-data",
    "href": "labs/lab11.html#part-1.-data",
    "title": "CSC413 Lab 11: Text Generation with Transformers",
    "section": "Part 1. Data",
    "text": "Part 1. Data\nThe “Friends Corpus” can be downloaded through the ConvKit package on Python. You can read more about the data here.\nLet’s install this package, and explore the data set.\n\n%pip install convokit\n\n\nimport convokit\n\n\ncorpus = convokit.Corpus(convokit.download('friends-corpus'))\n\nTask: Run the code below, which iterates through the first 10 utterances of the show. How is each utterance formatted? What do the speaker and text field mean?\n\nfor i, utterance in enumerate(corpus.iter_utterances()):\n    print(utterance)\n    if (i &gt;= 10): \n        break\n\nGraded Task: Create a list of strings called uts that contains all utterances made by your favourite (of the 6) main character of the show.\n\ncharacter = 'Monica Geller' # OR 'Chandler Bing' OR 'Phoebe Buffay' OR ...\n\nuts = []\n\nfor utterance in corpus.iter_utterances():\n    pass # TODO\n\nPlease include the output of this next line in your solution\n\nprint(len(uts)) \n\nTask: Run the below code. This code combines these lines into a large string for training, and a large string for validation. We will index ranges in this large string use in a minibatch—i.e. a minibatch of data will consist of a substring in this large string. This substring may start in the middle of an utterance and may contain multiple utterances—and that turns out to be okay! Our neural network still manages to learn what an utterance looks like and emulate it.\nSince this approach simpler to implement than the batching approach seen in the previous lab, it is more often seen\nWe will use 90% of the data for training, and 10% of the data for validation.\n\ntrain_split = 0.9\nn = len(uts)\n\ntrain_data_str = '\\n'.join(uts[:int(n*train_split)])\nval_data_str = '\\n'.join(uts[int(n*train_split):])\n\nTask: Notice that we split the utterances so that the earlier utterances are in the training set, and the later utterances (i.e., later in the TV show) is in the validation set. Why is this method preferable to randomly splitting the utterances into training and validation?\n\n# Include your explanation here\n\nTask: Why do we not set aside a test set?\n\n# Include your explanation here\n\nJust like in the previous lab, we will tokenize our text. Modern models use a tokenization strategy called Byte Pair Encoding, which tokenize text into common into sub-word tokens. Sub-word tokens split words into commonly occuring (and thus meaningful) parts. For example, the word “utterance” could be split into “utter” and “ance”. The model would learn about these constinuent tokens in different contexts, helping the model generalize better.\nWe will use the Byte Pair Encoding (BPE) tokenizer from the tiktoken library. Let’s install and import this library.\n\n%pip install tiktoken\n\n\nimport tiktoken\n\nGraded Task: We will be fine-tuning a pre-trained GPT2 model. Explain why it is important for us to use the same tokenization method as is used in the original GPT2 model whose weights we will be using.\n\n# Your explanation goes here\n\nNow, let’s retireve the original GPT2 model.\n\nenc = tiktoken.get_encoding(\"gpt2\")\n\ntrain_ids = enc.encode_ordinary(train_data_str)\nval_ids = enc.encode_ordinary(val_data_str)\n\nTask: How many tokens are in the training and validation sets? How does this compare with the number of words in each data set (computed using the str.split() method)? What about the number of character?\n\n# TODO\n\nTask: Run the below code, which will save the above numpy arrays in a file. This way, we can use np.memmap function, which creates a memory-map to an array stored in a binary file on disk. This approach is useful for accessing segments of a large file on disk, which we will be doing.\n\nimport numpy as np\nimport os\n\ndata_dir = 'friends_gpt2'\nos.makedirs(data_dir, exist_ok=True)\n\n# export to bin files\ntrain_ids = np.array(train_ids, dtype=np.uint16)\nval_ids = np.array(val_ids, dtype=np.uint16)\ntrain_ids.tofile(os.path.join(data_dir, 'train.bin'))\nval_ids.tofile(os.path.join(data_dir, 'val.bin'))\n\n\n# create a memory map\ntrain_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\nval_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n\nTask: Use the get_batch function below to extract a sample input/output from this data set. Here, we will be using the approach shown in the generative RNN lecture, where the model generates the next token given the previous context.\n\nimport torch\n\ndef get_batch(data, block_size, batch_size, device):\n    \"\"\"\n    Return a minibatch of data. This function is not deterministic.\n    Calling this function multiple times will result in multiple different\n    return values.\n\n    Parameters:\n        `data` - a numpy array (e.g., created via a call to np.memmap)\n        `block_size` - the length of each sequence\n        `batch_size` - the number of sequences in the batch\n        `device` - the device to place the returned PyTorch tensor\n\n    Returns: A tuple of PyTorch tensors (x, t), where\n        `x` - represents the input tokens, with shape (batch_size, block_size)\n        `y` - represents the target output tokens, with shape (batch_size, block_size)\n    \"\"\"\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n    t = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n    if 'cuda' in device:\n        # pin arrays x,t, which allows us to move them to GPU asynchronously\n        #  (non_blocking=True)\n        x, t = x.pin_memory().to(device, non_blocking=True), t.pin_memory().to(device, non_blocking=True)\n    else:\n        x, t = x.to(device), t.to(device)\n    return x, t\n\n\n# TODO: get and print a single batch from the training set\ndevice = 'cuda' if torch.cuda.is_available()  else 'cpu'\n\nGraded Task: Once again, we will be using the approach shown in the generative RNN lecture, where the model’s goal is to generate the next token given the previous context. With that in mind, explain why the target output tokens is very similar to the input tokens, just offset by 1 along the block_size dimension.\n\n# TODO: Your explanation goes here."
  },
  {
    "objectID": "labs/lab11.html#part-2.-model",
    "href": "labs/lab11.html#part-2.-model",
    "title": "CSC413 Lab 11: Text Generation with Transformers",
    "section": "Part 2. Model",
    "text": "Part 2. Model\nNow that we have our data set in mind, it is time to set up our GPT2 model. We will use the code provided in the nanoGPT repository, slightly modified here for succinctness. Thus, we will not re-implement the GPT2 model. Instead, let’s use the nanoGPT implementation to understand, step-by-step, what happens in a GPT model.\nLISATODO add a blurb about GPT2 model history, paper, and changes since GPT1.\nWe will explore the components of the GPT2 model first in a top-down manner, to get an intuition as to how the pieces connect. Then, we will explore the same components in a bottom-up manner, so that we can fully understand the role of each component.\n\nfrom dataclasses import dataclass\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport inspect\n\nWe begin with the GPTConfig class, which contains model architecture settings for our GPT2 model. The settings specify:\n\nblock_size: the input sequence length. Shorter sequences can be padded (with a padding token as seen in the previous lab), and longer sequences must be cut shorter. During training, we will generate batches with sequences that are exactly the block size\nvocab_size: the number of unique tokens in our vocabulary. This affects the size of the initial embedding layer.\nn_layer: the number of transformer layers.\nn_head: the number of attention heads to use in the causal self-attention layer.\nn_embd: the embedding size used throughout the model. You can think of each token position as being represented as a vector of length n_embd.\ndropout: for dropout.\nbias: whether to use a bias parameter in certain layers.\n\n\n@dataclass\nclass GPTConfig:\n    block_size: int = 1024\n    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n    n_layer: int = 12\n    n_head: int = 12\n    n_embd: int = 768\n    dropout: float = 0.0\n    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n\nTask: Which of these settings do you think would affect the total number of trainable parameters in a GPT model? Which of them do you think have the largest impact on the number of trainable parameters? Please write down your guess before continuing, and we will check back here later.\n\n# TODO: Write down your thoughts here.\n\nWith the setting in mind, we can set up a GPT model. A GPT model will take a GPTConfig object as a parameter. Pay particular attention to the __init__() and forward() methods. These are the methods that we will study in more detail.\nThe code uses a more PyTorch features that we have not discussed, but these features are mostly cosmetic and do not provide significantly different functionality: the use of nn.ModuleDict allows us to access modules in the GPT class in a straightforward way, and nn.ModuleList allows us to create a list of modules. We have not yet defined the PyTorch neural network modules Block and LayerNorm, but we will do so soon.\nIf you see a PyTorch feature used that you don’t understand, you can always look it up in the PyTorch documentation. However, you don’t try to understand everything at one go. It is normal to read code in multiple “passes”, and focus on the big picture in the first pass.\nTask: Begin with a first pass read of the __init__() and forward() methods of the GPT module.\n\nclass GPT(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        assert config.vocab_size is not None\n        assert config.block_size is not None\n        self.config = config\n\n        self.transformer = nn.ModuleDict(dict(\n            wte = nn.Embedding(config.vocab_size, config.n_embd),\n            wpe = nn.Embedding(config.block_size, config.n_embd),\n            drop = nn.Dropout(config.dropout),\n            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n        ))\n        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n        # with weight tying when using torch.compile() some warnings get generated:\n        # \"UserWarning: functional_call was passed multiple values for tied weights.\n        # This behavior is deprecated and will be an error in future versions\"\n        # not 100% sure what this is, so far seems to be harmless. TODO investigate\n        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n\n        # init all weights\n        self.apply(self._init_weights)\n        # apply special scaled init to the residual projections, per GPT-2 paper\n        for pn, p in self.named_parameters():\n            if pn.endswith('c_proj.weight'):\n                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n\n        # report number of parameters\n        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n\n    def get_num_params(self, non_embedding=True):\n        \"\"\"\n        Return the number of parameters in the model.\n        For non-embedding count (default), the position embeddings get subtracted.\n        The token embeddings would too, except due to the parameter sharing these\n        params are actually used as weights in the final layer, so we include them.\n        \"\"\"\n        n_params = sum(p.numel() for p in self.parameters())\n        if non_embedding:\n            n_params -= self.transformer.wpe.weight.numel()\n        return n_params\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\n    def forward(self, idx, targets=None):\n        device = idx.device\n        b, t = idx.size()\n        assert t &lt;= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n\n        # forward the GPT model itself\n        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n        x = self.transformer.drop(tok_emb + pos_emb)\n        for block in self.transformer.h:\n            x = block(x)\n        x = self.transformer.ln_f(x)\n\n        if targets is not None:\n            # if we are given some desired targets also calculate the loss\n            logits = self.lm_head(x)\n            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n        else:\n            # inference-time mini-optimization: only forward the lm_head on the very last position\n            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n            loss = None\n\n        return logits, loss\n\n    def crop_block_size(self, block_size):\n        # model surgery to decrease the block size if necessary\n        # e.g. we may load the GPT2 pretrained model checkpoint (block size 1024)\n        # but want to use a smaller block size for some smaller, simpler model\n        assert block_size &lt;= self.config.block_size\n        self.config.block_size = block_size\n        self.transformer.wpe.weight = nn.Parameter(self.transformer.wpe.weight[:block_size])\n        for block in self.transformer.h:\n            if hasattr(block.attn, 'bias'):\n                block.attn.bias = block.attn.bias[:,:,:block_size,:block_size]\n\n    @classmethod\n    def from_pretrained(cls, model_type, override_args=None):\n        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n        override_args = override_args or {} # default to empty dict\n        # only dropout can be overridden see more notes below\n        assert all(k == 'dropout' for k in override_args)\n        from transformers import GPT2LMHeadModel\n        print(\"loading weights from pretrained gpt: %s\" % model_type)\n\n        # n_layer, n_head and n_embd are determined from model_type\n        config_args = {\n            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n        }[model_type]\n        print(\"forcing vocab_size=50257, block_size=1024, bias=True\")\n        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n        config_args['bias'] = True # always True for GPT model checkpoints\n        # we can override the dropout rate, if desired\n        if 'dropout' in override_args:\n            print(f\"overriding dropout rate to {override_args['dropout']}\")\n            config_args['dropout'] = override_args['dropout']\n        # create a from-scratch initialized minGPT model\n        config = GPTConfig(**config_args)\n        model = GPT(config)\n        sd = model.state_dict()\n        sd_keys = sd.keys()\n        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n\n        # init a huggingface/transformers model\n        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n        sd_hf = model_hf.state_dict()\n\n        # copy while ensuring all of the parameters are aligned and match in names and shapes\n        sd_keys_hf = sd_hf.keys()\n        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n        # this means that we have to transpose these weights when we import them\n        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n        for k in sd_keys_hf:\n            if any(k.endswith(w) for w in transposed):\n                # special treatment for the Conv1D weights we need to transpose\n                assert sd_hf[k].shape[::-1] == sd[k].shape\n                with torch.no_grad():\n                    sd[k].copy_(sd_hf[k].t())\n            else:\n                # vanilla copy over the other parameters\n                assert sd_hf[k].shape == sd[k].shape\n                with torch.no_grad():\n                    sd[k].copy_(sd_hf[k])\n\n        return model\n\n    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n        # start with all of the candidate parameters\n        param_dict = {pn: p for pn, p in self.named_parameters()}\n        # filter out those that do not require grad\n        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n        decay_params = [p for n, p in param_dict.items() if p.dim() &gt;= 2]\n        nodecay_params = [p for n, p in param_dict.items() if p.dim() &lt; 2]\n        optim_groups = [\n            {'params': decay_params, 'weight_decay': weight_decay},\n            {'params': nodecay_params, 'weight_decay': 0.0}\n        ]\n        num_decay_params = sum(p.numel() for p in decay_params)\n        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n        # Create AdamW optimizer and use the fused version if it is available\n        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n        use_fused = fused_available and device_type == 'cuda'\n        extra_args = dict(fused=True) if use_fused else dict()\n        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n        print(f\"using fused AdamW: {use_fused}\")\n\n        return optimizer\n\n    def estimate_mfu(self, fwdbwd_per_iter, dt):\n        \"\"\" estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS \"\"\"\n        # first estimate the number of flops we do per iteration.\n        # see PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311\n        N = self.get_num_params()\n        cfg = self.config\n        L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embd//cfg.n_head, cfg.block_size\n        flops_per_token = 6*N + 12*L*H*Q*T\n        flops_per_fwdbwd = flops_per_token * T\n        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n        # express our flops throughput as ratio of A100 bfloat16 peak flops\n        flops_achieved = flops_per_iter * (1.0/dt) # per second\n        flops_promised = 312e12 # A100 GPU bfloat16 peak flops is 312 TFLOPS\n        mfu = flops_achieved / flops_promised\n        return mfu\n\n    @torch.no_grad()\n    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n        \"\"\"\n        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n        \"\"\"\n        for _ in range(max_new_tokens):\n            # if the sequence context is growing too long we must crop it at block_size\n            idx_cond = idx if idx.size(1) &lt;= self.config.block_size else idx[:, -self.config.block_size:]\n            # forward the model to get the logits for the index in the sequence\n            logits, _ = self(idx_cond)\n            # pluck the logits at the final step and scale by desired temperature\n            logits = logits[:, -1, :] / temperature\n            # optionally crop the logits to only the top k options\n            if top_k is not None:\n                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n                logits[logits &lt; v[:, [-1]]] = -float('Inf')\n            # apply softmax to convert logits to (normalized) probabilities\n            probs = F.softmax(logits, dim=-1)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1)\n            # append sampled index to the running sequence and continue\n            idx = torch.cat((idx, idx_next), dim=1)\n\n        return idx\n\nDoing a first-pass read on both the __init__() and forward() methods, you should see that the GPT model has the following components (ignoring dropout):\n\ntransformer.wte, which is an embedding layer that maps tokens to a vector embedding.\ntransformer.wtp, is also an embedding layer, but this one maps token position indices to a vector embedding. This index is required to inject position information into the embedding—otherwise transformer computation would be invariant to the reordering of input tokens (i.e., the computation would not change if the order of the input tokens change). Since the length of a sequence is at most block_size, so there are at most block_size indices to embed.\nA sequence of Blocks — to be defined. The output from the previous block is taken as the input of the next block.\nA final LayerNorm layer after the last block. This layer is also yet to be defined, and the name suggests that this is a normalization layer (similar to batch normalization) that does not change the shape of the features (i.e., the output shape is the same as the input shape).\nlm_head, which is a linear layer that maps embeddings back to a distribution over the possible otuput tokens.\n\nTask: Compute the number of parameters in the wte embedding layer of the GPT2 model. (For these and other questions that specifically mention GPT2 model, please use the config settings above and provide an actual numbers.)\n\n# TODO: Perform this computation by hand.\n\nTask: Compute the number of parameters in the wtp embedding layer of the GPT2 model.\n\n# TODO: Perform this computation by hand.\n\nGraded Task: Explain why the linear layer lm_head has the same number of parameters as the embedding layer wte. Provide an intuitive explanation for why weight tying—i.e., using the same set of weights for both layers, just transposed—would be reasonable. The weight tying is done to reduce the total number of parameters in the GPT2 model.\n\n# TODO: Include your explanations here\n\nTask: Explain why it is that in the forward() method, the tensor tok_emb has the shape (b, t, n_embd), where b is the batch size, t is the sequence length (max block_size), and n_embd is the embedding size.\n\n# TODO: Include your explanations here\n\nTask: Notice that in the forward() method, the tensor pos_emb has the shape (t, n_embd). In other words, we embed the position only once for each batch, and then rely on PyTorch tensor broadcasting to perform the addition tok_emb + pos_emb. Why is this ok?\n\n# TODO: Include your explanations here\n\nTask: What is the shape of tok_emb + pos_emb in the forward() method in a GPT2 model? This question is not trivial because the two addend tensors are not of the same shape. Thus, the addition uses broadcasting. PyTorch broadcasting works similarly to that of Numpy’s. You can look up “PyTorch broadcasting” to find resources related to how broadcasting works.\n\n# TODO: Perform this computation by hand.\n\nGraded Task: What is the shape of x in the forward() method? This is an important shape to remember, since it is the shape of the feature map consistent in most of the transformer network.\n\n# TODO: Perform this computation by hand.\n\nTask: What is the shape of logits in the forward() method?\n\n# TODO: Perform this computation by hand.\n\nThese questions above should give you a clear idea of the main components of the transformer model, the expected input and output tensor shapes, and the shapes of intermediate tensors. With this in mind, let’s explore the two modules referenced by GPT.\nWe’ll start with the simple one. The LayerNorm layer is intended to be similar to PyTorch’s LayerNorm layer.\n\nclass LayerNorm(nn.Module):\n    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n\n    def __init__(self, ndim, bias):\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(ndim))\n        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n\n    def forward(self, input):\n        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n\nTask: How many parameters are in a LayerNorm layer?\n\n# TODO: Perform this computation by hand.\n\nTask: Read the description of the LayerNorm layer in PyTorch at https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html#torch.nn.LayerNorm. Then, explain how layer normalization differs from batch normalization.\n\n# TODO: Perform this computation by hand.\n\nLet’s move on to the Block module. Recall that here are several Block modules in a GPT model, and the output of one module is the input of the next.\n\nclass Block(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n        self.attn = CausalSelfAttention(config)\n        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n        self.mlp = MLP(config)\n\n    def forward(self, x):\n        x = x + self.attn(self.ln_1(x))\n        x = x + self.mlp(self.ln_2(x))\n        return x\n\nThis module is actually quite succinct, but it also refers to modules that are yet to be defined. It consists of:\n\nA layer normalization layer.\nA causal self attention layer (to be defined). This is the heart of the GPT model.\nAnother layer normalization layer.\nAn MLP layer (to be defined).\n\nTask: Judging by the Block.forward() method above, why must the CausalSelfAttention and the MLP layers preserve the shape of the features?\n\n# TODO: Include your explanation here.\n\nTask: How might the skip-connections in the Block.forward() method help with gradient flow? An intuitive explanation is sufficient here.\n\n# TODO: Include your explanation here.\n\nWith the GPT2 Block in mind, we will define the MLP module next.\n\nclass MLP(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n        self.gelu    = nn.GELU()\n        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n        self.dropout = nn.Dropout(config.dropout)\n\n    def forward(self, x):\n        x = self.c_fc(x)\n        x = self.gelu(x)\n        x = self.c_proj(x)\n        x = self.dropout(x)\n        return x\n\nImmediately, we see that this MLP consists of two linear layers. The activation function used between these two layers is the Gaussian Error Linear Units function. You can read more about it in this paper.\nTask: Compute the number of parameters in a MLP layer in a GPT2 model.\n\n# TODO: Perform this computation by hand\n\nTask: Recall that the input of the MLP layer is a tensor with the usual dimension computed earlier. What is the shape of self.c_fc(x) in the MLP.forward() method? What about the shape of the return value in this method?\n\n# TODO: Perform this computation by hand\n\nTask: Explain why this MLP layer is also called the “pointwise feed forward” layer. (Hint: a “point” here refers to a single token or position in the input sequence)\n\n# TODO: Include your explanation here.\n\nFinally, let’s study the definition of the CausalSelfAttention layer. This is the heart of the GPT model and is also the most complex module.\nTask: Begin with a first pass read of the __init__() and forward() methods of CausalSelfAttention module. We will then trace through the case where self.flash is False, since the code provides more detailed explanation for the computation steps.\n\nclass CausalSelfAttention(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        assert config.n_embd % config.n_head == 0\n        # key, query, value projections for all heads, but in a batch\n        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n        # output projection\n        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n        # regularization\n        self.attn_dropout = nn.Dropout(config.dropout)\n        self.resid_dropout = nn.Dropout(config.dropout)\n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n        self.dropout = config.dropout\n        # flash attention make GPU go brrrrr but support is only in PyTorch &gt;= 2.0\n        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n        if not self.flash:\n            print(\"WARNING: using slow attention. Flash Attention requires PyTorch &gt;= 2.0\")\n            # causal mask to ensure that attention is only applied to the left in the input sequence\n            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n                                        .view(1, 1, config.block_size, config.block_size))\n\n    def forward(self, x):\n        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n\n        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n\n        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -&gt; (B, nh, T, T)\n        if self.flash:\n            # efficient attention using Flash Attention CUDA kernels\n            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n        else:\n            # manual implementation of attention\n            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n            att = F.softmax(att, dim=-1)\n            att = self.attn_dropout(att)\n            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -&gt; (B, nh, T, hs)\n        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n\n        # output projection\n        y = self.resid_dropout(self.c_proj(y))\n        return y\n\nTask: Compute the number of parameters in the c_attn layer in a GPT2 model.\n\n# TODO: Perform this computation by hand\n\nTask: Like the comment in the __init__() method suggests, we can think of the c_attn layer as a combination of three nn.Linear(config.n_embd, config.n_embd, bias=config.bias) modules. These three networks projects the input embedding into three parts: q (for query), k (for key), and v (for *value).\nWhat is the shape of self.c_attn(x) in the forward() method? Use this answer to show that self.c_attn(x).split(self.n_embd, dim=2) gives us the same q, k, v values had we used three separate networks.\n\n# TODO: Perform the computation by hand, then include your explaination.\n\nTask: Explain why config.n_head must be a factor of config.n_embd.\n\n# TODO: Your explanation goes here.\n\nWe will explore the manual implementation of attention in the next few questions. For this part, it helps to first consider the case where the batch size B=1, and n_head=1. For a larger batch size and number of heads, the attention computation is repeated for every sequence in the batch and every attention head. Thus, the shapes of the three important tensors are:\n\nq: (1, 1, T, n_embd)\nk: (1, 1, T, n_embd)\nv: (1, 1, T, n_embd)\n\nLike discussed in the lectures, you can think of the attention mechanism as a “soft” dictionary lookup. Instead of obtaining a single key/values for a given query, attention gives us a probability distribution over the possible keys/values. We can then use this probability distribution to obtain a weighted sum (akin to an expected value) of the lookup value. Moreover, instead of having strings, numbers, or other objects as keys/values, a key is a vector (of shape n_embd), and a value is also a vector (of shape n_embd). This is consistent with what we have seen in neural networks—everything is represented using a vector! The tensors k and v contains these keys and values, and there is one vector at every token position. The tensor q contains the queries— analogues to the item (a possible key) that we are searching for in a regular dictionary lookup. There is also one query vector for each token position: for each token position, we want to look up a corresponding (weighted sum of) values that contains information pertinent to understanding the meaning of the token in this position.\nWith that in mind, let’s go through the mathematical computations.\nGraded Task: What is the shape of (q @ k.transpose(-2, -1))? For this and the following questions, assume that q, k, v have the shape above, where we have assumed that batch size and num heads are both 1.\n\n# TODO: Perform this computation by hand\n\nTask: What is the value of math.sqrt(k.size(-1))?\n\n# TODO: Perform this computation by hand\n\nTask: Argue that the line att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1))) is computing a “distance” or “similarity” metric between the query at each token position and the key at each token position.\n\n# TODO: Your explanation goes here\n\nThe following line of code references a self.bias parameter, which is defined in the last line of the __init__() method. Since block_size is quite large, we can understand what self.bias looks like by running a similar piece of code below with a smaller block_size value.\n\nbias = torch.tril(torch.ones(5, 5)).view(1, 1, 5, 5)\nbias\n\nTask: Explain what the above code returns. Explain how PyTorch broadcasting may be useful for computations involving this tensor—i.e., why is it okay that the first two dimensions of this tensor are 1, thus assuming that batch size = 1 and num heads = 1?\n\n# TODO: Your explanation goes here\n\nTask: We will use a similar technique of running a modified version of the next two lines of code in the forward() method to better understand what it does. Run the below code, and explain what the masked_fill function does.\n\nattn = torch.rand(1, 1, 5, 5)\nattn\n\n\nmasked = attn.masked_fill(bias[:,:,:5, :5] == 0, float('-inf'))\nmasked\n\n\nout = F.softmax(masked, dim=-1)\nout\n\n\n# Your explanation goes here\n\nGraded Task: This masking is in place so that query tokens cannot “look up” key/values that at a position with a larger index. Explain why this limitation means our GPT model cannot use information in subsequent/later tokens to form an understand of what what is in the current token. (Note: this masking is the “Causal” part of Causal Self-Attention!)\n\n# Your explanation goes here\n\nTask: Your answer above explains which positions in the out tensor need to be set to zero. Explain why setting the corresponding value of pre-softmax tensor masked to -inf is necessary. Why can’t we set the value of masked to 0 in these positions?\n\n# Your argument goes here\n\nTask: Argue that out[0,0,0,0] must always be 1.\n\n# Your argument goes here\n\nTask: Now, out in our example is akin to the final value of att in the CausalSelfAttention.forward() method. Explain why the operation y = att @ v computes a weighted sum of values at each token position, where the weights are defined by att.\n\n# Your explanation goes here\n\nTask: The above explanation pertain to a single attention head. Explain why using multiple attention heads allows a token position to consider information from various other positions. Alternatively, explain why using multiple heads might help the network learn different ways in which the meaning at one token could depend on other tokens.\n\n# Your explanation goes here\n\nGraded Task: Compute the total number of parameters in a GPT2 model by computing the following. Please use actual numbers in each case, assuming the GPT2 configuration from above.\n\nThe number of parameters in a CausalSelfAttention model.\nThe number of parameters in a MLP module.\nThe number of parameters in a Block module.\nThe number of parameters in all Block modules in a GPT2 model.\nThe number of parameters in the wte embedding layer in a GPT2 model.\nThe total number of parameters in a GPT2 model.\n\nPlease perform the computation either by hand (and show your work), or with a function that clearly shows the computations.\nYou should see that approximately 30% of the GPT2 weight comes from the wte embedding layer. This is why weight tying is used in the GPT module!\n\n# TODO: Your work goes here"
  },
  {
    "objectID": "labs/lab11.html#part-3.-training",
    "href": "labs/lab11.html#part-3.-training",
    "title": "CSC413 Lab 11: Text Generation with Transformers",
    "section": "Part 3. Training",
    "text": "Part 3. Training\nWe are ready to finetune our GPT2 model on the “Friends” data set! There is no graded task in this section since training this model can take some time to achieve reasonable performance.\nTo run this part of the lab, you will need to use a GPU. On Google Colab, you can select a session with a GPU by navigating to the “Runtime” menu, selecting “Change runtime type”, and then selecting the “T4 GPU” option.\nWe will set up a config object to make it easier to store and use configs.\n\nimport easydict\nimport math\nimport time\n\nfinetune_config_dict = {\n  'gradient_accumulation_steps': 32,\n  'block_size': 256,\n  'dropout': 0.2,\n  'bias': False,\n  'learning_rate': 3e-5,\n  'weight_decay': 0.1,\n  'beta1': 0.9,\n  'beta2': 0.99,\n  'grad_clip': 1.0,\n  'decay_lr': False,\n  'warmup_iters': 100,\n  'lr_decay_iters': 5000,\n  'min_lr': 0.0001}\nconfig = easydict.EasyDict(finetune_config_dict)\n\nFirst, we need to load the GPT2 weights.\n\n# initialize from OpenAI GPT-2 weights\noverride_args = dict(dropout=config.dropout)\nmodel = GPT.from_pretrained('gpt2', override_args)\n\n# crop down the model block size using model surgery\nif config.block_size &lt; model.config.block_size:\n    model.crop_block_size(config.block_size)\n\ndevice = 'cuda' if torch.cuda.is_available()  else 'cpu'\nmodel.to(device)\n\nTask: Explain why reducing the block_size do not significantly reduce the number of parameters, but does significantly reduce memory usage.\n\n# TODO: Include your explanation here\n\nThere are some additional helpers to improve training.\n\n# initialize a GradScaler. If enabled=False scaler is a no-op\ndtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\nptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\nctx = nullcontext() if device == 'cpu' else torch.amp.autocast(device_type=device, dtype=ptdtype)\nscaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n\n# learning rate decay scheduler (cosine with warmup)\ndef get_lr(config, it):\n    # 1) linear warmup for warmup_iters steps\n    if it &lt; config.warmup_iters:\n        return config.learning_rate * it / config.warmup_iters\n    # 2) if it &gt; lr_decay_iters, return min learning rate\n    if it &gt; config.lr_decay_iters:\n        return config.min_lr\n    # 3) in between, use cosine decay down to min learning rate\n    decay_ratio = (it - config.warmup_iters) / (config.lr_decay_iters - config.warmup_iters)\n    assert 0 &lt;= decay_ratio &lt;= 1\n    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n    return config.min_lr + coeff * (config.learning_rate - config.min_lr)\n\n# helps estimate an arbitrarily accurate loss over either split using many batches\n@torch.no_grad()\ndef estimate_loss(model, train_dataset, val_dataset, block_size):\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        dataset = train_dataset if split == 'train' else val_dataset\n        for k in range(eval_iters):\n            X, Y = get_batch(dataset, block_size, batch_size, device)\n            with ctx:\n                logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out\n\nNow we can begin the training loop. You may need to increase max_iter to obtain good results.\n\niter_num = 0\nbest_val_loss = 1e9\neval_interval = 10\nlog_interval = 10\neval_iters = 40\nmax_iters = 500\nbatch_size = 1\n\n# optimizer\noptimizer = model.configure_optimizers(config.weight_decay, config.learning_rate,\n   (config.beta1, config.beta2), device)\n\n# training loop\nX, Y = get_batch(train_data, config.block_size, batch_size, device) # fetch the very first batch\nt0 = time.time()\nlocal_iter_num = 0 # number of iterations in the lifetime of this process\nraw_model = model # unwrap DDP container if needed\nrunning_mfu = -1.0\nwhile True:\n    # determine and set the learning rate for this iteration\n    lr = get_lr(config, iter_num) if config.decay_lr else config.learning_rate\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\n\n    # evaluate the loss on train/val sets and write checkpoints\n    if iter_num % eval_interval == 0:\n        losses = estimate_loss(model, train_data, val_data, config.block_size)\n        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n    # forward backward update, with optional gradient accumulation to simulate larger batch size\n    # and using the GradScaler if data type is float16\n    for micro_step in range(config.gradient_accumulation_steps):\n        with ctx:\n            logits, loss = model(X, Y)\n            loss = loss / config.gradient_accumulation_steps # scale the loss to account for gradient accumulation\n        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n        X, Y = get_batch(train_data, config.block_size, batch_size, device)\n        # backward pass, with gradient scaling if training in fp16\n        scaler.scale(loss).backward()\n\n    # clip the gradient\n    if config.grad_clip != 0.0:\n        scaler.unscale_(optimizer)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)\n\n    # step the optimizer and scaler if training in fp16\n    scaler.step(optimizer)\n    scaler.update()\n\n    # flush the gradients as soon as we can, no need for this memory anymore\n    optimizer.zero_grad(set_to_none=True)\n\n    # timing and logging\n    t1 = time.time()\n    dt = t1 - t0\n    t0 = t1\n    if iter_num % log_interval == 0:\n        # get loss as float. note: this is a CPU-GPU sync point\n        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n        lossf = loss.item() * config.gradient_accumulation_steps\n        if local_iter_num &gt;= 5: # let the training loop settle a bit\n            mfu = raw_model.estimate_mfu(batch_size * config.gradient_accumulation_steps, dt)\n            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n\n    iter_num += 1\n    local_iter_num += 1\n\n    # termination conditions\n    if iter_num &gt; max_iters:\n        break\n\nHere is some code you can use to generate a sequence using the fine-tuned GPT2 model.\n\ninit_from = 'resume' # either 'resume' (from an out_dir) or a gpt2 variant (e.g. 'gpt2-xl')\nout_dir = 'out' # ignored if init_from is not 'resume'\nstart = \"\\n\" # or \"&lt;|endoftext|&gt;\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\nnum_samples = 10 # number of samples to draw\nmax_new_tokens = 500 # number of tokens generated in each sample\ntemperature = 0.8 # 1.0 = no change, &lt; 1.0 = less random, &gt; 1.0 = more random, in predictions\ntop_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n\n\nenc = tiktoken.get_encoding(\"gpt2\")\nencode = lambda s: enc.encode(s, allowed_special={\"&lt;|endoftext|&gt;\"})\ndecode = lambda l: enc.decode(l)\n\nstart_ids = encode(start)\nx = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n\n#model = finetuned_model\n# run generation\nwith torch.no_grad():\n    with ctx:\n        for k in range(num_samples):\n            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n            print(decode(y[0].tolist()))\n            print('---------------')"
  },
  {
    "objectID": "labs/lab05.html",
    "href": "labs/lab05.html",
    "title": "CSC413 Lab 5: Differential Privacy",
    "section": "",
    "text": "In this lab, we will explore how training a neural network with some of the optimization methods discussed in the lecture can cause models to capture more information about the training data than we might intend. We will discuss why this may be problematic from a privacy perspective, and introduce the idea of differential privacy.\nFinally, this lab introduces an optimization strategy called DP-SGD or differentially private stochastic gradient descent. This strategy has some provable properties about the amount of information captured.\nThis lab also serves as an introductory guide to implementing optimization models that are presented in research papers. We hope that the techniques used in this lab build skills so that you can implement new techniques and ideas presented in other papers.\nBy the end of this lab, you will be able to:\nPlease work in groups of 1-2 during the lab.\nAcknowledgements:\nPlease work in groups of 1-2 during the lab."
  },
  {
    "objectID": "labs/lab05.html#submission",
    "href": "labs/lab05.html#submission",
    "title": "CSC413 Lab 5: Differential Privacy",
    "section": "Submission",
    "text": "Submission\nIf you are working with a partner, start by creating a group on Markus. If you are working alone, click “Working Alone”.\nSubmit the ipynb file lab05.ipynb on Markus containing all your solutions to the Graded Tasks. Your notebook file must contain your code and outputs where applicable, including printed lines and images. Your TA will not run your code for the purpose of grading.\nFor this lab, you should submit the following:\n\nPart 2: Description of the difference between the non-dp model predictions over data in/out of training (1 point)\nPart 3: Explanation of why the “average height” model is not \\(\\epsilon\\)-DP (1 point)\nPart 4: Explanation of T_max in CosineAnnealingLR (1 point)\nPart 4: Explanation why max_grad_norm &gt;= 7.49 causes gradient clipping to remain unchanged in the example (1 point)\nPart 4: Implementation of dp_grads function (4 points)\nPart 4: Explanation of the difference in the histogram of the dp and non-dp models (1 point)\nPart 4: Analysis of the impact of privacy breach on a vulnerable individual (1 point)"
  },
  {
    "objectID": "labs/lab05.html#google-colab-setup",
    "href": "labs/lab05.html#google-colab-setup",
    "title": "CSC413 Lab 5: Differential Privacy",
    "section": "Google Colab Setup",
    "text": "Google Colab Setup\nLike last week, we will be using the medmnist data set, which is available as a Python package. We will also be using opacus, which is a differential privacy library.\nRecall that on Google Colab, we use “!” to run shell commands. Below, we use such commands to install the Python packages.\n\n!pip install medmnist\n!pip install opacus"
  },
  {
    "objectID": "labs/lab05.html#part-1.-data-and-model",
    "href": "labs/lab05.html#part-1.-data-and-model",
    "title": "CSC413 Lab 5: Differential Privacy",
    "section": "Part 1. Data and Model",
    "text": "Part 1. Data and Model\nWe will be using the same data and model as in lab 3, with modifications in the way that the training, validation, and test sets are split. These modifications are necessary to be able to showcase differential privacy issues using a small model and limited data set size to ensure that models do not take an overwhelming amount of time to train.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport opacus\nimport medmnist\nfrom medmnist import PneumoniaMNIST\nimport torchvision.transforms as transforms\n\nimport torch.utils.data as data_utils\n\nmedmnist.INFO['pneumoniamnist']\n\nTask Use code from lab 3 to re-acquaint yourself with the training data. What do the inputs look like? What about the targets? What is the distribution of the targets? Intuitively, how difficult is the classification problem?\n\n# TODO: Run/revise the data exploration code from lab 3 so\n# that you can describe the dataset and the difference between\n# the two classes\n\nTask: Using the standard train/validation/test split provided by the dataset, what percentage of the training set had the label 0? What about the validation set?\n\n# TODO: Write code to compute the figures here\n\nThese statistics differ significantly between the training, validation and test sets for our DP demonstration to work well with our small MLP model. Thus, we will perform our own split of the training, validation, and test sets.\nIn addition, we will split the data into four sets: training, validation, test, and a memorization assessment set. This data set will be the same size as our training set. Practitioners sometimes call this set a second or unused training set, even though this data set is not used for training. The idea is that we want to see if there is a difference between data that we actually used for training, vs another data that we could have used for training.\nTask: Run the following code to obtain the four datasets.\n\n# Load the training, validation, and test sets\n# We will normalize each data set to mean 0.5 and std 0.5: this\n# improves training speed\ndata_transform = transforms.Compose([transforms.ToTensor(),\n                                     transforms.Normalize(mean=[.5], std=[.5])])\ntrain_dataset = PneumoniaMNIST(split='train', transform=data_transform, download=True)\nvalid_dataset = PneumoniaMNIST(split='val', transform=data_transform, download=True)\ntest_dataset = PneumoniaMNIST(split='test', transform=data_transform, download=True)\n\n# Combine the training and validation\ncombined_data = train_dataset + valid_dataset\n\n# Re-split the data into training,  memory assessment\ntrain_dataset, mem_asses_dataset, valid_dataset = torch.utils.data.random_split(combined_data, [0.4, 0.4, 0.2])\n\nTask: What percentage of the training set had the label 0? What about the memorization assessment set? What about the validation set?\n\n# TODO: Run code to complete your solution here.\n\nNow that our data is ready, we can set up the model and training code similar to lab 3.\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass MLPModel(nn.Module):\n    \"\"\"A three-layer MLP model for binary classification\"\"\"\n    def __init__(self, input_dim=28*28, num_hidden=600):\n        super(MLPModel, self).__init__()\n        self.fc1 = nn.Linear(input_dim, num_hidden)\n        self.fc2 = nn.Linear(num_hidden, num_hidden)\n        self.fc3 = nn.Linear(num_hidden, 1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.sigmoid(out)\n        out = self.fc2(out)\n        out = self.sigmoid(out)\n        out = self.fc3(out)\n        return out\n\nTo explore the distribution of predicted logits, the following code is written for you: it produces both the predictions and ground-truth labels across a dataset. There is also another utility function that can be used to measure the accuracy.\n\ndef get_predictions(model, data):\n    \"\"\"\n    Return the ground truth and predicted value across a dataset.\n    Unlike the get_prediction function in lab 3, this dataset\n    will produce the predictions for the *entire* dataset (no sampling)\n\n    Parameters:\n        `model` - A PyTorch model\n        `data` - A PyTorch dataset of MedMNIST images\n\n    Returns: A tuple `(ys, ts)` where:\n        `ys` is a list of prediction probabilities, same length as `data`\n        `ts` is a list of ground-truth labels, same length as `data`\n    \"\"\"\n    ys, ts = [], []\n    loader = torch.utils.data.DataLoader(data, batch_size=100)\n    for X, t in loader:\n        z = model(X.reshape(-1, 784))\n        ys += [float(y) for y in torch.sigmoid(z)]\n        ts += [float(t_) for t_ in t]\n    return ys, ts\n\ndef accuracy(model, dataset):\n    \"\"\"\n    Compute the accuracy of `model` over the `dataset`.\n    We will take the **most probable class**\n    as the class predicted by the model.\n\n    Parameters:\n        `model` - A PyTorch model\n        `dataset` - A PyTorch dataset of MedMNIST images\n\n    Returns: a floating-point value between 0 and 1.\n    \"\"\"\n\n    ys, ts = get_predictions(model, dataset)\n    predicted = np.ndarray.round(np.array(ys))\n    return np.mean(predicted == ts)\n\nThe training code below is analogues to the training code used in lab 3, with some differences. One difference is that we use the Adam optimizer rather than SGD. The Adam optimizer combines ideas from momentum and RMSProp and is able to train our model to a suitable accuracy with much fewer iterations.\nTask: Run the code below to train our (non-private) model.\n\ndef train_model(model,                # a PyTorch model\n                train_data,           # training data\n                val_data,             # validation data\n                learning_rate=1e-2,\n                batch_size=100,\n                num_epochs=45,\n                plot_every=20,        # how often (in # iterations) to track metrics\n                plot=True):           # whether to plot the training curve\n    train_loader = torch.utils.data.DataLoader(train_data,\n                                               batch_size=batch_size,\n                                               shuffle=True) # reshuffle minibatches every epoch\n    criterion = nn.BCEWithLogitsLoss()\n    optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n\n    # these lists will be used to track the training progress\n    # and to plot the training curve\n    iters, train_loss, train_acc, val_acc = [], [], [], []\n    iter_count = 0 # count the number of iterations that has passed\n\n    for e in range(num_epochs):\n        for i, (images, labels) in enumerate(train_loader):\n              z = model(images.reshape(-1, 784))\n              loss = criterion(z, labels.to(torch.float))\n              loss.backward()\n              optimizer.step()\n              optimizer.zero_grad()\n\n              iter_count += 1\n              if iter_count % plot_every == 0:\n                  iters.append(iter_count)\n                  ta = accuracy(model, train_data)\n                  va = accuracy(model, val_data)\n                  train_loss.append(float(loss))\n                  train_acc.append(ta)\n                  val_acc.append(va)\n                  print(iter_count, \"Loss:\", float(loss), \"Train Acc:\", ta, \"Val Acc:\", va)\n\n    if plot:\n        plt.figure()\n        plt.plot(iters[:len(train_loss)], train_loss)\n        plt.title(\"Loss over iterations\")\n        plt.xlabel(\"Iterations\")\n        plt.ylabel(\"Loss\")\n \n        plt.figure()\n        plt.plot(iters[:len(train_acc)], train_acc)\n        plt.plot(iters[:len(val_acc)], val_acc)\n        plt.title(\"Accuracy over iterations\")\n        plt.xlabel(\"Iterations\")\n        plt.ylabel(\"Loss\")\n        plt.legend([\"Train\", \"Validation\"])\n\nmodel_np = MLPModel()\ntrain_model(model_np, train_dataset, valid_dataset)"
  },
  {
    "objectID": "labs/lab05.html#part-2.-privacy-issues-in-our-model",
    "href": "labs/lab05.html#part-2.-privacy-issues-in-our-model",
    "title": "CSC413 Lab 5: Differential Privacy",
    "section": "Part 2. Privacy issues in our model",
    "text": "Part 2. Privacy issues in our model\nIn this part of the lab, we will show that our trained model captures more information about the training data than we might intend. In particular, we show that the model predictions have different patterns for images used in training (compared to images that are not used in training). Specifically, the prediction logits follow a different distribution for training images and images not used for training.\nIn more general applications, the patterns in the logit distributions can be used to build classifiers that can predict whether an image was used in training a neural network.\nTask: Suppose that you are a patient in a medical study, who consented for their data to be used to train a machine learning model to detect the strains of certain diseases (i.e., strain A or B of the same disease). Explain why you might not want it be known that your data was used to build the model.\n\n# TODO: Your explanation goes here.\n\nRecall that we used train_dataset to train our model, but did not use the mem_asses_dataset. To show that our model behaves differently for data used in training (vs not), we will plot the histogram of prediction probabilities across these two datasets.\nTask: Run the code below, which produces cumulative histogram plots showing showing the log model predictions of negative and positive samples (truth label=0 vs true label=1). The predictions for data point in the training set is shown in blue. The predictions for data points not in the training set (in the memorization assessment set) is in red.\n\ndef plot_hist(model, in_dataset, out_dataset):\n    \"\"\"\n    Plots the histogram (cumulative, in the log space) of the predicted\n    probabilities for datasets that is in the training set vs out. The\n    histograms are separated by the true labels.\n\n    Parameters:\n        `model` - A PyTorch model\n        `in_dataset` - A PyTorch dataset used for training \n                       (i.e. *in* the training set)\n        `out_dataset` - A PyTorch dataset not used for training \n                       (i.e. *out* the training set)\n    \"\"\"\n    # Obtain the prediction for data points in both data sets\n    ys_in, ts_in  = get_predictions(model, in_dataset)\n    ys_out, ts_out = get_predictions(model, out_dataset)\n\n    # Compute the negative log() of these predictions, separated by the\n    # ground truth labels. An epsilon is added to the prediction for\n    # numerical stability\n    epsilon = 1e-10\n    conf_in_0 = [-np.log(y + epsilon) for t, y in zip(ts_in, ys_in) if t == 0]\n    conf_in_1 = [-np.log(1 - y + epsilon) for t, y in zip(ts_in, ys_in) if t == 1]\n    conf_out_0 = [-np.log(y + epsilon) for t, y in zip(ts_out, ys_out) if t == 0]\n    conf_out_1 = [-np.log(1 - y + epsilon) for t, y in zip(ts_out, ys_out) if t == 1]\n\n    # Bins used for the density/histogram\n    bins_0 = np.linspace(0,max(max(conf_in_0),max(conf_out_0)),500)\n    bins_1 = np.linspace(0,max(max(conf_in_1),max(conf_out_1)),500)\n\n    # Plot the histogram for the predicted probabilities for true label = 0\n    plt.subplot(2, 1, 1)\n    plt.hist(conf_out_0, bins_0, color='r', label='out', alpha=0.5,cumulative=True, density=True)\n    plt.hist(conf_in_0, bins_0, color='b', label='in', alpha=0.5,cumulative=True, density=True)\n    plt.legend()\n    plt.ylabel('CDF')\n    plt.xlabel('-log(Pr(pred=1))')\n    plt.title(\"True label=0\")\n\n    # Plot the histogram for the predicted probabilities for true label = 1\n    plt.subplot(2, 1, 2)\n    plt.hist(conf_out_1, bins_1, color='r', label='out', alpha=0.5,cumulative=True, density=True)\n    plt.hist(conf_in_1, bins_1, color='b', label='in', alpha=0.5,cumulative=True, density=True)\n    plt.legend()\n    plt.title(\"True label=1\")\n    plt.ylabel('CDF')\n    plt.xlabel('-log(Pr(pred=0))')\n\n    plt.subplots_adjust(hspace=1)\n    plt.show()\n\nplot_hist(model_np, train_dataset, mem_asses_dataset)\n\nGraded Task: What difference do you notice between the histograms of the data points in the training set, vs those not in the training set? Explain how this difference is indicative of overfitting.\n\n# TODO: Your answer goes here"
  },
  {
    "objectID": "labs/lab05.html#part-3.-differential-privacy",
    "href": "labs/lab05.html#part-3.-differential-privacy",
    "title": "CSC413 Lab 5: Differential Privacy",
    "section": "Part 3. Differential Privacy",
    "text": "Part 3. Differential Privacy\nIn the previous section, we observed that a model’s prediction confidence for the samples inside its training set can be different from those outside the training set. We already know that this disparity has a negative impact on the model’s performance during test time. This phenomenon is known as overfitting and we know several techniques on how to measure and reduce the overfitting. In this part, we want to argue that this disparity has a negative impact on privacy of the training samples.\nAssume the designed model in the previous section is published for the public as a classification model for Pneumonia. Assume that the training set consists of individuals’ X-ray. Even though the participants have consented to participate in the research, their privacy should still be protected. If an attacker can determine whether a specific individual’s data is part of the research dataset, it might lead to unintended privacy breaches. Participants might not want their involvement in such a study to be public knowledge due to the stigma associated with certain medical conditions. This disparity may also help an attacker to reconstruct a participant’s data. This example shows that the disparity between the model’s confidence lets the adversary infer the membership of a sample from the predictions. This is alarming! In the next part, we discuss how to mitigate this risk by introducing the fundamental concept of Differential Privacy.\nDifferential privacy (DP) is a data privacy framework that aims to provide strong privacy guarantees when analyzing or sharing sensitive data. We say an ML algorithm satisfies Differential Privacy if changing one of the training samples does not change the output of the algorithm significantly. DP is an interesting property: assume you want to give a hospital access to your X-ray. If the hospital uses a DP ML algorithm, then, you are guaranteed that your presence does not affect the output significantly. This is promising and motivates people to give access to their data for the purpose of data analysis.\nNext, we discuss how to formalize DP.\nAssume we have a dataset \\(S=\\{(x_1,y_1),\\dots,(x_n,y_n)\\}\\) which consists of \\(n\\) individuals data. Consider the neighboring dataset \\(S'=\\{(x_1,y_1),\\dots,(x'_n, y'_n)\\}\\) which differs from \\(S\\) in only one sample. Then, we say a randomized algorithm \\(\\mathcal{A}\\) satisfies \\(\\epsilon\\)-DP if for all the output \\(y\\) in the range of \\(\\mathcal{A}\\) it satisfies\n\\[\n\\mathbb{Pr}\\left(\\mathcal{A}(S) =y \\right) \\leq \\exp(\\epsilon)  \\mathbb{Pr}\\left(\\mathcal{A}(S’) =y \\right).\n\\]\nBut what is the intuition behind this equation?\n\\(\\epsilon\\) is called the privacy budget. Privacy budget captures how strong our privacy guarantees are, by showing that the outcome is indistinguishable in two neighboring datasets. This can be shown by setting \\(\\epsilon = 0\\), the probability the analysis having an outcome is the same with or without you in the database. So if we set \\(\\epsilon\\) to some small value, we can get good guarantees that the output will not differ much.\nA common property of privacy-preserving algorithms is randomness. To see why it is the case assume we are interested in the average height of the students enrolled in CSC413while preserving their privacy. Consider a deterministic algorithm that reports the average height of the students. We argue that there exists no finite \\(\\epsilon\\) for which this algorithm is DP. For this example, it can be shown that by adding a Gaussian noise to the average height we can preserve privacy of the individuals.\nGraded Task: Explain why the algorithm that reports the average height of the students is not \\(\\epsilon\\)-DP for any finite \\(\\epsilon&gt;0\\).\n\n# TODO: Include your explanation here\n\nWe hope that the basic intuition behind differential privacy is now clear. DP is now a widely-used method to preserve privacy. It has been used in companies like Google and Apple to gather the user’s data. It is also recently used for the US Census. See the following [video]{https://www.youtube.com/watch?v=nVPE1dbA394} to get more information."
  },
  {
    "objectID": "labs/lab05.html#part-4.-differentially-private-sgd",
    "href": "labs/lab05.html#part-4.-differentially-private-sgd",
    "title": "CSC413 Lab 5: Differential Privacy",
    "section": "Part 4. Differentially-Private SGD",
    "text": "Part 4. Differentially-Private SGD\nIn this section, we discuss how we can make stochastic gradient descent differentially private and implement it. We will follow the algorithmic description of DP-SGD forom https://arxiv.org/pdf/1607.00133.pdf that we reproduce below. Start by reading the words/headings in the description, then we will discuss the algorithm line by line.\nAlgorithm Outline for Differentially Private SGD (DP-SGD)\nInput Examples \\(\\left\\{x_1, \\ldots, x_N\\right\\}\\), loss function \\(\\mathcal{L}(\\theta)=\\) \\(\\frac{1}{N} \\sum_i \\mathcal{L}\\left(\\theta, x_i\\right)\\).\nParameters: learning rate \\(\\eta_t\\), noise scale, \\(\\sigma\\), group size \\(L\\), gradient norm bound \\(C\\). \\\nInitialize \\(\\theta_0\\) randomly\nfor \\(t \\in[T]\\) do * Take a random sample \\(L_t\\) with sampling probability \\(L / N\\) * Compute gradient \\(\\quad\\) For each \\(i \\in L_t\\), compute \\(\\mathbf{g}_t\\left(x_i\\right) \\leftarrow \\nabla_{\\theta_t} \\mathcal{L}\\left(\\theta_t, x_i\\right)\\) * Clip gradient \\(\\overline{\\mathbf{g}}_t\\left(x_i\\right) \\leftarrow \\mathbf{g}_t\\left(x_i\\right) / \\max \\left(1, \\frac{\\left\\|\\mathbf{g}_t\\left(x_i\\right)\\right\\|_2}{C}\\right)\\) * Add noise \\(\\tilde{\\mathbf{g}}_t \\leftarrow \\frac{1}{L}\\left(\\sum_i \\overline{\\mathbf{g}}_t\\left(x_i\\right)+\\mathcal{N}\\left(0, \\sigma^2 C^2 \\mathbf{I}\\right)\\right)\\) \\ * Descent \\(\\theta_{t+1} \\leftarrow \\theta_t-\\eta_t \\tilde{\\mathbf{g}}_t\\)\nOutput \\(\\theta_T\\) and compute the overall privacy cost \\((\\varepsilon, \\delta)\\) using a privacy accounting method.\nTask: Read the algorithm above. Write down, for each symbol/notation used in the algorithm, what it represents. Pay particular attention to the symbol \\(\\mathbf{g}_t\\left(x_i\\right)\\) and its various modifications.\n\n# TODO: Make sure you understand the notation before moving on to the\n# detailed descriptions.\n\nNow, let’s discuss the algorithm line by line.\n\nSampling: The sampling mechanism used in DP-SGD is different from SGD. In non-private SGD, we choose a random permutation at the beginning of each epoch. However, in DP-SGD at each iteration we select a sample with probability (batchsize/number of samples) to be a member of the batch at the current iteration. This sampling mechanism is also called Poisson subsampling. We will not implement this sampling ourselves; we will use Opacus software package implement of it.\nGradient Computation: This step is analogous to gradient computation in SGD. However, the gradients of each sample in the batch is computed separately.\nClipping Gradients: You should be able to show that, mathematically, that clipping ensures that for each data point, the gradient vector of that data point has a maximum norm of \\(C\\). Why is this useful? Assume there is an outlier for which the gradient is very large. Without clipping, the impact of the outlier on the algorithm will be unbounded. DP-SGD performs clipping to each individual gradient point separately, which limits the contribution of each data point to the parameter update.\nAdding Noise: In order to achieve a specific level of privacy determined by \\(\\epsilon\\), we need to select the minimum amount of noise to be added in each iteration (\\(\\sigma\\) in the algorithm description). Since determining the exact amount requires a very technical calculation, there are software packages which can be used. Here, we use the Opacus software package from Meta research. It provides a function which takes as input the privacy level \\(\\epsilon\\), batchsize and the number of training points, and outputs the variance of the noise. There is another input, i.e., \\(\\delta\\). Do not make any changes to it. If you are interested to know what it means please read the following lecture note.\n\nTask: Notice that the scale of the noise added to the gradient is related to the clipping parameter. Does the amount of the noise added increase or decrease if we allow a larger maximum norm \\(C\\) during clipping? (Reasoning about these differences is one way to make sense of mathematical equations like these.)\n\n# TODO: Your answer goes here\n\nIn the next few tasks, we will describe the pieces of code that we will need to implement DP-SGD.\nTask: Run the following code to compare the batches produced by the usual Dataloader vs. via Poisson Sampling. What do you noitce?\n\n# Create a dataset with 20 numbers\nx = torch.arange(20)\nprint(x)\ndataset = torch.utils.data.TensorDataset(x)\n\nprint('PyTorch DataLoader')\ndata_loader = torch.utils.data.DataLoader(dataset, batch_size=4,shuffle=True)\nfor _ in range(2): # run for 2 epochs\n    for x_b in data_loader:\n        print(x_b)\n    print('-------------------')\n\nprint('Poisson Sampling')\ndp_data_loader = opacus.data_loader.DPDataLoader(dataset, sample_rate = 5/20)\nfor _ in range(2): # run for 2 epochs\n    for x_b in dp_data_loader:\n        print(x_b)\n    print('-------------------')\n\nIn addition to using a different sampling method, we will use CosineAnnealingLR learning rate scheduler in pytorch. You need to understand how this learning rate scheduler works and how it can be updated.\nGraded Task Read the PyTorch documentation on CosineAnnealingLR and explain what the parameter T_max represent.\n\n# TODO: Write your explanation here.\n\nThe most challenging part of implementing DP-SGD is that we will need to implement our own optimization process to modify the default gradient descent behaviour. However, Pytorch has a nice feature that we saw in lab 1: each parameter in a model stores its own gradient as an attribute. In particular, consider the following snippet which can be used to print the name and gradient of the parameters in a model.\n\nfor name, param in model_np.named_parameters(): \n    print(name)\n    print(param.grad)\n\nWhat we didn’t see in lab 1 is that we can anually change the gradient of each parameter!\nThis is powerful, because the optimizers in PyTorch uses the .grad attributes of each parameter to perform model updates. Thus, changing the .grad attributes provides a way to override the default gradient descent behaviour.\nTask: Run the below code, which demonstrates how the .grad attribute can be modified.\n\nmodel = nn.Linear(5, 1) # linear model with input dim = 5, and a single output\nprint(list(model.parameters())) # print the current parameters\n\n# manually set the optimizers\noptimizer = torch.optim.SGD(model.parameters(), 0.1)\nmodel.weight.grad = torch.nn.parameter.Parameter(torch.Tensor([[1, 2, 3, 4, 5.]]))\nmodel.bias.grad = torch.nn.parameter.Parameter(torch.Tensor([1.]))\noptimizer.step()\n\n# what would you expect the output to be?\nprint(list(model.parameters()))\n\nTo implement DP-SGD, We will need to manually modify the .grad attribute in a few ways. One of the steps to DP-SGD is gradient clipping. Fortunately, PyTorch actually comes with an implementation of gradient clipping through the function torch.nn.utils.clip_grad_norm_.\nTask: Run this code to see how gradient clipping works. Notice that the gradient direction is unchanged, only the magnitude.\n\nmodel = nn.Linear(5, 1) # linear model with input dim = 5, and a single output\nmodel.weight.grad = torch.Tensor([[1, 2, 3, 4, 5.]])\nmodel.bias.grad = torch.Tensor([1.])\nmax_grad_norm = 0.5\ntorch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\nprint(model.weight.grad, model.bias.grad)\n\nGraded Task: Explain why if we set max_grad_norm &gt;= 7.49 above, the gradient will be unchanged. Your explanation should demonstrate the calculation of where the number 7.49 comes from.\n\n# TODO: Include your explanation and calculation here.\n\nGraded Task: Now that we have the pieces we need to implement our DP-SGD gradient computation, Complete the code below, which performs one iteration of DP-SGD update for a batch of data. You may wish to look ahead to see how this function will be used in DP-SGD training.\n\ndef dp_grads(model, batch_data, criterion, max_grad_norm, noise_multiplier):\n    \"\"\"\n    Compute gradients for an iteration of DP-SGD training by setting the\n    .grad attribute of each parameter in model.named_parameters()\n    according to the DP-SGD algorithm.\n\n    Parameters:\n        - `model` - A PyTorch model\n        - `batch_data` - A list of tuples (x, t) representing a batch of data\n        - `criterion` - A PyTorch loss function\n        - `max_grad_norm` - The maximum gradient norm, used for gradient clipping\n                            (C in the algorithm description)\n        - `noise_multiplier` - The noise multiplier, used for adding noise\n                               (sigma in the algorithm description)\n\n    Returns: A dictionary `clipped_noisy_grads` that maps the names of each\n             parameter in `model.named_parameters()` to its modified gradient\n             computed according to DP-SGD\n    \"\"\"\n    # Create the mapping of each parameter in our model to\n    # what will evetually be the noisy gradients\n    clipped_noisy_grads = {name: torch.zeros_like(param) for name, param in model.named_parameters()}\n\n    # Iterate over the data points in each batch. This is unfortunately\n    # necessary so that we can perform gradient clipping separtely for each\n    # data point\n    for xi, ti in batch_data:\n        zi = None # TODO: compute the model prediction (logit)\n        lossi = None # TODO: compute the loss for this data point\n\n        # TODO: perform the backward pass\n\n        # TODO: perform gradient clipping\n\n        # accumulate the clipped gradients in `clipped_noisy_grads`\n        for name, param in model.named_parameters():\n            clipped_noisy_grads[name] += param.grad\n\n        # TODO: clear the gradients in the model's computation graph\n\n    # Now, we iterate over the name parameters to add noise\n    for name, param in model.named_parameters():\n        # TODO: Read the equation in the \"Add Noise\" section of the\n        #      algorithm description, and implement it. You may find\n        #      the function `torch.normal` helpful.\n        clipped_noisy_grads[name] += 0 # TODO\n\n\n    return clipped_noisy_grads\n\nPlease include the output of the tests below in your submission. (What should the output of the test be?)\n\nmodel = nn.Linear(5, 1)\nmodel.weight = nn.Parameter(torch.Tensor([[1, 1, 0, 0, 0.]]))\nmodel.bias = nn.Parameter(torch.Tensor([0.]))\nbatch_data = [(torch.Tensor([[1, 1, 1, 0, 0.]]), torch.Tensor([1.])),\n              (torch.Tensor([[1, 0, 1, 0, 0.]]), torch.Tensor([0.]))]\ncriterion = nn.BCEWithLogitsLoss()\n\n# no noise and a large max_grad_norm\nprint(dp_grads(model, batch_data, criterion, max_grad_norm=1000, noise_multiplier=0))\nprint('-----------')\n\n# no noise and a small max_grad_norm\nprint(dp_grads(model, batch_data, criterion, max_grad_norm=0.5, noise_multiplier=3))\nprint('-----------')\n\n# small max_grad_norm and some noise (STD should be ~0.5x3/2)\nfor i in range(10):\n    print(dp_grads(model, batch_data, criterion, max_grad_norm=0.5, noise_multiplier=3))\n\nTask Now that we have DP-SGD in place, run the below code to train a differentially private model.\n\ndef train_model_private(model, traindata, valdata, learning_rate=2e-1,\n                        batch_size=500, num_epochs=25, plot_every=20,\n                        epsilon=0.5, max_grad_norm=6):\n    # Compute the noise multiplier\n    N = len(traindata)\n    noise_multiplier = opacus.accountants.utils.get_noise_multiplier(\n        target_epsilon=epsilon,\n        target_delta=1/N,\n        sample_rate=batch_size/N,\n        epochs=num_epochs)\n\n    # Use the differentially private data loader\n    train_loader = opacus.data_loader.DPDataLoader(\n        dataset=traindata,\n        sample_rate=batch_size/N)\n\n    criterion = nn.BCEWithLogitsLoss()\n    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=N//batch_size * num_epochs)\n\n    # these lists will be used to track the training progress\n    # and to plot the training curve\n    iters, train_loss, train_acc, val_acc = [], [], [], []\n    iter_count = 0 # count the number of iterations that has passed\n \n    for e in range(num_epochs):\n        for i, (images, labels) in enumerate(train_loader):\n            images = images.reshape(-1, 28*28)\n            # get the clipped noisy gradients from the function you wrote\n            clipped_noisy_grads = dp_grads(model,\n                                           batch_data=list(zip(images,labels)),\n                                           criterion=criterion,\n                                           max_grad_norm=max_grad_norm,\n                                           noise_multiplier=noise_multiplier)\n            # manually update the gradients\n            for name, param in model.named_parameters():\n                param.grad = clipped_noisy_grads[name]\n            optimizer.step() # update the parameters\n            scheduler.step() # update the learning rate scheduler\n            optimizer.zero_grad() # clean up accumualted gradients\n\n            iter_count += 1\n            if iter_count % plot_every == 0:\n                # forward pass to compute the loss\n                z = model(images.reshape(-1, 784))\n                loss = criterion(z, labels.to(torch.float))\n                optimizer.zero_grad()\n\n                iters.append(iter_count)\n                ta = accuracy(model, traindata)\n                va = accuracy(model, valdata)\n                train_loss.append(float(loss))\n                train_acc.append(ta)\n                val_acc.append(va)\n                print(iter_count, \"Loss:\", float(loss), \"Train Acc:\", ta, \"Val Acc:\", va)\n   \n    plt.figure()\n    plt.plot(iters[:len(train_loss)], train_loss)\n    plt.title(\"Loss over iterations\")\n    plt.xlabel(\"Iterations\")\n    plt.ylabel(\"Loss\")\n\n    plt.figure()\n    plt.plot(iters[:len(train_acc)], train_acc)\n    plt.plot(iters[:len(val_acc)], val_acc)\n    plt.title(\"Accuracy over iterations\")\n    plt.xlabel(\"Iterations\")\n    plt.ylabel(\"Loss\")\n    plt.legend([\"Train\", \"Validation\"])\n\n\nmodel_priv = MLPModel()\ntrain_model_private(model_priv, train_dataset, valid_dataset)\n\nGraded Task: Plot the histogram of the model prediction for this differentially private model. How does this histogram differ from that of model_np from above?\n\nplot_hist(model_priv, train_dataset, mem_asses_dataset)\n\n\n# TODO: Explain how the histogram differs from that of model_np\n\nTask How does the accuracy differ from that of model_no mentioned above? Explain what you observe.\n\n# TODO: Your answer goes here\n\nGraded Task: Suppose that an attacker recognizes that your friend Taylor is in this data set, means that their X-ray was taken at some point during a hospitalization, and that Taylor provided researchers consent to be included in the study dataset. If this information is sold to a third-party (e.g., a credit reporting agency, an employer, or a landlord), how might this affect Taylor?\n\n# TODO: Your answer goes here\n\nIf you are interested in DP, we suggest performing hyperparameter tuning over batch size and max grad norm. In, DP-SGD usually larger batch size would help. So, for instance for two values of \\(\\varepsilon \\in \\{0.5,1,5\\}\\), try to find the best model for \\(\\text{batchsize}\\in \\{100,500\\}\\) and \\(\\text{gradnorm}\\in\\{4,8,16\\}\\)."
  },
  {
    "objectID": "labs/lab05.html#appendix",
    "href": "labs/lab05.html#appendix",
    "title": "CSC413 Lab 5: Differential Privacy",
    "section": "Appendix",
    "text": "Appendix\n\n[Differential Privacy and the Census] https://www.youtube.com/watch?v=nVPE1dbA394\nMain DP-SGD Paper\nCS 860 - Algorithms for Private Data Analysis at UWaterloo"
  },
  {
    "objectID": "labs/lab03.html",
    "href": "labs/lab03.html",
    "title": "CSC413 Lab 3: Multi-Layer Perceptrons with MedMNIST",
    "section": "",
    "text": "MedMNIST’s PneumoniaMNIST data set. We will now transition fully to using PyTorch for our labs going forward.\nBy the end of this lab, you will be able to:\nAcknowledgements:\nPlease work in groups of 1-2 during the lab."
  },
  {
    "objectID": "labs/lab03.html#submission",
    "href": "labs/lab03.html#submission",
    "title": "CSC413 Lab 3: Multi-Layer Perceptrons with MedMNIST",
    "section": "Submission",
    "text": "Submission\nIf you are working with a partner, start by creating a group on Markus. If you are working alone, click “Working Alone”.\nSubmit the ipynb file lab03.ipynb on Markus containing all your solutions to the Graded Tasks. Your notebook file must contain your code and outputs where applicable, including printed lines and images. Your TA will not run your code for the purpose of grading.\nFor this lab, you should submit the following:\n\nPart 2. Your expression that computes the number of trainable parameters in the MLPModel (1 point)\nPart 2. Your implementation of accuracy. (1 point)\nPart 2. Your implementation of train_model. (2 points)\nPart 3. Your implementation of precision and recall. (2 points)\nPart 3. Your interpretation of the confusion matrix for m_once (1 point)\nPart 4. Your completion of the grid search, along with the output (2 point)\nPart 4. Your description of why a model with high AUC may still perform poorly for some groups (1 point)\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "labs/lab03.html#google-colab-setup",
    "href": "labs/lab03.html#google-colab-setup",
    "title": "CSC413 Lab 3: Multi-Layer Perceptrons with MedMNIST",
    "section": "Google Colab Setup",
    "text": "Google Colab Setup\nWe will be using the medmnist data set, which is available as a Python package. Recall that on Google Colab, we use “!” to run shell commands.\n\n!pip install medmnist"
  },
  {
    "objectID": "labs/lab03.html#part-1.-data",
    "href": "labs/lab03.html#part-1.-data",
    "title": "CSC413 Lab 3: Multi-Layer Perceptrons with MedMNIST",
    "section": "Part 1. Data",
    "text": "Part 1. Data\nWe will use the MedMNIST data set, which is described here: https://medmnist.com/. We will use the PneumoniaMNIST images, which are greyscale chest X-ray images that has been resized to 28x28. The task is to predict, given one of these X-ray images, whether the patient has pneumonia or not—a binary classification task. We chose this dataset both because it is lightweight, and because it allows us to discuss the sensitive nature of biomedical images.\nLet’s begin by printing some information about the PneumoniaMNIST data set:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport medmnist\nfrom medmnist import PneumoniaMNIST\n\nmedmnist.INFO['pneumoniamnist']\n\nTask: The dataset providers already split the data into training, validation, and test sets. How many samples are there in the training, validation, and test sets?\n\n# TODO: Write your answer here.\n\nLet’s visually inspect the first element of the training data:\n\ntrain_data_imgs = PneumoniaMNIST(split='train', download=True)\n\nfor img, target in train_data_imgs:\n    plt.imshow(img, cmap='gray')\n    print(np.array(img)) # img is a numpy array of shape 28x28 , with integer values between 0-255\n    print(target)        # the target\n    break\n\nTask: Based on the code above, what is the type of the data structure train_data?\n\n# TODO: Write your answer here.\n\nTask: The code below plots 5 images from each class: normal and pneumonia. Do you notice qualitative differences between these two sets of images? It is always important to qualitatively assess your data prior to training, so that you can develop intuition as to what features may or may not be important for your model. Understanding your data also helps to estimate how challenging the classification problem may be and identify incorrect implementations (e.g., a surprisingly high model accuracy could indicate issues with training set leakage into the test set).\n\n# normal images\nplt.figure()\nn = 0\nfor img, target in train_data_imgs:\n    if int(target) == 0:\n      plt.subplot(1, 5, n+1)\n      plt.title(\"normal\")\n      plt.imshow(img, cmap='gray')\n      n += 1\n    if n &gt;= 5:\n      break\n# pneumonia images\nplt.figure()\nn = 0\nfor img, target in train_data_imgs:\n    if int(target) == 1:\n      plt.subplot(1, 5, n+1)\n      plt.title(\"pneumonia\")\n      plt.imshow(img, cmap='gray')\n      n += 1\n    if n &gt;= 5:\n      break\n\n\n# TODO: Write your explanation here.\n\nPyTorch makes it easy to apply pre-processing transformations to the data, for example to normalize the data prior to using for training. We will use the standard preprocessing functions to transform the images into tensors for PyTorch to be able to use. This transformation also changes the values to be floating-point numbers between 0 and 1.\n\nimport torchvision.transforms as transforms # contains a collection of transformations\n\ntrain_data = PneumoniaMNIST(split='train', download=True, transform=transforms.ToTensor())\nval_data = PneumoniaMNIST(split='val', download=True, transform=transforms.ToTensor())\ntest_data = PneumoniaMNIST(split='test', download=True, transform=transforms.ToTensor())\n\nfor img, target in train_data:\n    print(img)    # img is a PyTorch tensor fo shape 1x28x28\n    print(target) # the target\n    break\n\nTask: How many X-ray images are in the training set with pneumonia? What about without pneumonia? What about the validation/test sets? What does your answer say about the data balance?\n\n# TODO: Write code to find the answer here."
  },
  {
    "objectID": "labs/lab03.html#part-2.-model-and-training",
    "href": "labs/lab03.html#part-2.-model-and-training",
    "title": "CSC413 Lab 3: Multi-Layer Perceptrons with MedMNIST",
    "section": "Part 2. Model and Training",
    "text": "Part 2. Model and Training\nWe will build our own PyTorch model, which will be a subclass of nn.Module. This subclass provides the important methods that we used in the training loop in lab 1, including the methods that allow us to compute the forward pass by calling the model object, and other methods used under the hood to compute the backward pass.\nOur model will be a three-layer MLP with the following architecture: ACTUALTODO—the model architecture may change!\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass MLPModel(nn.Module):\n    \"\"\"A three-layer MLP model for binary classification\"\"\"\n    def __init__(self, input_dim=28*28, num_hidden=100):\n        super(MLPModel, self).__init__()\n        self.fc1 = nn.Linear(input_dim, num_hidden)\n        self.fc2 = nn.Linear(num_hidden, num_hidden)\n        self.fc3 = nn.Linear(num_hidden, 1)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.relu(out)\n        out = self.fc2(out)\n        out = self.relu(out)\n        out = self.fc3(out)\n        return out\n\nGraded Task: How many trainable parameters are in this model? Express your answer in terms of input_dim and num_hidden.\n\n# TODO: Compute the number of trainable parameters in MLPModel\n\nIn order to assess model performance, we will begin by implementing the accuracy function, which computes the accuracy of the model across a dataset.\nGraded Task: Complete the accuracy function. Keep in mind that this function will be slightly different from the accuracy function in lab 1, since we are working on a binary classification problem and prediction here is a single logit value (rather than a vector).\n\ndef accuracy(model, dataset):\n    \"\"\"\n    Compute the accuracy of `model` over the `dataset`.\n    We will take the **most probable class**\n    as the class predicted by the model.\n\n    Parameters:\n        `model` - A PyTorch MLPModel\n        `dataset` - A data structure that acts like a list of 2-tuples of\n                  the form (x, t), where `x` is a PyTorch tensor of shape\n                  [1, 28, 28] representing an MedMNIST image,\n                  and `t` is the corresponding binary target label\n\n    Returns: a floating-point value between 0 and 1.\n    \"\"\"\n\n    correct, total = 0, 0\n    loader = torch.utils.data.DataLoader(dataset, batch_size=100)\n    for img, t in loader:\n        X = img.reshape(-1, 784)\n        z = model(X)\n\n        y = None # TODO: pred should be a [N, 1] tensor with binary \n                    # predictions, (0 or 1 in each entry)\n\n        correct += int(torch.sum(t == y))\n        total   += t.shape[0]\n    return correct / total\n\nBecause we are working with binary classification, we will be using a different implementation of the cross-entropy loss function, implemented via PyTorch in a class called BCEWithLogitsLoss (short for Binary Cross Entropy with Logits loss).\n\ncriterion = nn.BCEWithLogitsLoss()\n\nThis loss function takes a predicted logit (pre-softmax activation) and the ground-truth label. The use of pre-softmax logits rather than prediction probabilities is due to numerical stability reasons.\n\nprint(criterion(torch.tensor([2.5]),  # predicted\n                torch.tensor([1.])))  # actual\n\nprint(criterion(torch.tensor([-2.5]), # predicted\n                torch.tensor([1.])))  # actual\n\nTask: Explain why the second printed value above is larger than the first. In other words, why does it make sense that we think of the second prediction (logit of z=-2.5) as “worse” than the first (logit of z=2.5)?\n\n# TODO: Your explanation goes here.\n\nGraded Task: Complete the following code to be used for training.\n\ndef train_model(model,                # an instance of MLPModel\n                train_data,           # training data\n                val_data,             # validation data\n                learning_rate=0.1,\n                batch_size=100,\n                num_epochs=10,\n                plot_every=50,        # how often (in # iterations) to track metrics\n                plot=True):           # whether to plot the training curve\n    train_loader = torch.utils.data.DataLoader(train_data,\n                                               batch_size=batch_size,\n                                               shuffle=True) # reshuffle minibatches every epoch\n    criterion = nn.BCEWithLogitsLoss()\n    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n\n    # these lists will be used to track the training progress\n    # and to plot the training curve\n    iters, train_loss, train_acc, val_acc = [], [], [], []\n    iter_count = 0 # count the number of iterations that has passed\n\n    try:\n        for e in range(num_epochs):\n            for i, (images, labels) in enumerate(train_loader):\n                z = None # TODO\n  \n                loss = None # TODO\n  \n                loss.backward() # propagate the gradients\n                optimizer.step() # update the parameters\n                optimizer.zero_grad() # clean up accumualted gradients\n  \n                iter_count += 1\n                if iter_count % plot_every == 0:\n                    iters.append(iter_count)\n                    ta = accuracy(model, train_data)\n                    va = accuracy(model, val_data)\n                    train_loss.append(float(loss))\n                    train_acc.append(ta)\n                    val_acc.append(va)\n                    print(iter_count, \"Loss:\", float(loss), \"Train Acc:\", ta, \"Val Acc:\", va)\n    finally:\n        # This try/finally block is to display the training curve\n        # even if training is interrupted\n        if plot:\n            plt.figure()\n            plt.plot(iters[:len(train_loss)], train_loss)\n            plt.title(\"Loss over iterations\")\n            plt.xlabel(\"Iterations\")\n            plt.ylabel(\"Loss\")\n\n            plt.figure()\n            plt.plot(iters[:len(train_acc)], train_acc)\n            plt.plot(iters[:len(val_acc)], val_acc)\n            plt.title(\"Accuracy over iterations\")\n            plt.xlabel(\"Iterations\")\n            plt.ylabel(\"Accuracy\")\n            plt.legend([\"Train\", \"Validation\"])\n\n# Please include the output of this cell for grading\nmodel = MLPModel()\ntrain_model(model, train_data, val_data)\n\nTask: Suppose that a model has a validation accuracy of 74% for this binary classification task. Why would this model be considered a very bad model? Your answer should illustrate why accuracy may not be an excellent tool to use.\n\n# TODO: Write your explanation here"
  },
  {
    "objectID": "labs/lab03.html#part-3.-performance-metrics",
    "href": "labs/lab03.html#part-3.-performance-metrics",
    "title": "CSC413 Lab 3: Multi-Layer Perceptrons with MedMNIST",
    "section": "Part 3. Performance Metrics",
    "text": "Part 3. Performance Metrics\nWe often use accuracy as a go-to metric when evaluating the performance of a classification model. However, the accuracy measure weighs all errors equally. A deeper look into the types of errors made can provide a more complete picture of model performance, especially when there is data imbalance and—when applying models in real situations—when some errors may be associated with more serious impacts to users than others.\nTo start our explorations, we’ll look at the decisions we made well, i.e. the:\n\nTrue Positives (TP), or positive outcomes that were correctly predicted as positive.\nTrue Negatives (TN), or negative outcomes that were correctly predicted as negative.\n\nThen we will look at our mistakes, i.e. the:\n\nFalse Positives (FP, or Type I errors), or negative outcomes that were predicted as positive. In our case, this occurs when our model predicts that a person has heart disease, but they do not.\nFalse Negatives (FN, or Type II errors), or positive outcomes that were predicted as negative. In our case, this occurs when our model predicts that a person does not have heart disease, but they do.\n\nWe can then use the metrics above to calculate:\n\nPrecision (or True Positive Rate, or Positive Predicive Value): \\(\\frac{TP}{TP + FP}\\). The answers the question: out of all the examples that we predicted as positive, how many are really positive?\nRecall (or Sensitivity): \\(\\frac{TP}{TP + FN}\\). The answers the question: out of all the positive examples in the data set, how many did we predict as positive?\nFalse Positive Rate (or Negative Predicive Value): \\(\\frac{TN}{TN + FN}\\). The answers the question: out of all the examples that we predicted as negative, how many are really negative?\n\nGraded Task: Complete the functions precision and recall:\n\ndef precision(model, dataset):\n    \"\"\"\n    Compute the precision of `model` over the `dataset`.  We will take the\n    **most probable class** as the class predicted by the model.\n\n    Parameters:\n        `model` - A PyTorch MLPModel\n        `dataset` - A data structure that acts like a list of 2-tuples of\n                  the form (x, t), where `x` is a PyTorch tensor of shape\n                  [1, 28, 28] representing an MedMNIST image,\n                  and `t` is the corresponding binary target label\n\n    Returns: a floating-point value between 0 and 1.\n    \"\"\"\n    true_pos, total_pred_pos = 0, 0\n    loader = torch.utils.data.DataLoader(dataset, batch_size=100)\n    for img, t in loader:\n        X = img.reshape(-1, 784)\n        z = model(X)\n\n        y = None # TODO: pred should be a [N, 1] tensor with binary \n                    # predictions, (0 or 1 in each entry)\n\n        # TODO: update total_pred_pos and true_pos\n    return true_pos / total_pred_pos\n\n\ndef recall(model, dataset):\n    \"\"\"\n    Compute the recall (or sensitivity) of `model` over the `dataset`.  We will\n    take the **most probable class** as the class predicted by the model.\n\n    Parameters:\n        `model` - A PyTorch MLPModel\n        `dataset` - A data structure that acts like a list of 2-tuples of\n                  the form (x, t), where `x` is a PyTorch tensor of shape\n                  [1, 28, 28] representing an MedMNIST image,\n                  and `t` is the corresponding binary target label\n\n    Returns: a floating-point value between 0 and 1.\n    \"\"\"\n    true_pos, total_actual_pos = 0, 0 # track the true and false positive\n    loader = torch.utils.data.DataLoader(dataset, batch_size=100)\n    for img, t in loader:\n        X = img.reshape(-1, 784)\n        z = model(X)\n\n        y = None # TODO: pred should be a [N, 1] tensor with binary \n                    # predictions, (0 or 1 in each entry)\n\n        # TODO: update total_pos and true_pos\n    return true_pos / total_actual_pos\n\nprint(\"Precision(Training)\", precision(model, train_data))\nprint(\"Recall(Training)\", recall(model, train_data))\nprint(\"Precision(Validation)\", precision(model, val_data))\nprint(\"Recall(Validation)\", recall(model, val_data))\n\nA confusion matrix is a table that shows the number of TP, TN, FP, and FN. A confusion matrix can be a valuable tool in understanding why a model makes the mistake that it makes.\nTask Run the code below to display the confusion matrix for your model for the validation data.\n\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\ndef get_prediction(model, data, sample=1000):\n    loader = torch.utils.data.DataLoader(data, batch_size=sample, shuffle=True)\n    for X, t in loader:\n        z = model(X.view(-1, 784))\n        y = torch.sigmoid(z)\n        break\n    y = y.detach().numpy()\n    t = t.detach().numpy()\n    return y, t\n\ny, t = get_prediction(model, val_data)\ny = y &gt; 0.5\ncm = confusion_matrix(t, y)\ncmp = ConfusionMatrixDisplay(cm, display_labels=[\"0\", \"1\"])\ncmp.plot()\nplt.title(\"Confusion Matrix (Val Data)\")\n\nTask: The code below trains a MLPModel for a very few number of iterations. You should see that this model achieves a 74% accuracy. Display the confusion matrix for this model by running the code below.\n\nm_once = MLPModel()\ntrain_model(m_once, train_data, val_data, learning_rate=0.5, batch_size=500, num_epochs=1)\nprint(\"Training Accuracy:\", accuracy(m_once, train_data))\nprint(\"Validation Accuracy:\", accuracy(m_once, val_data))\n\ny, t = get_prediction(m_once, val_data)\ny = y &gt; 0.5\nConfusionMatrixDisplay(confusion_matrix(t, y), display_labels=[\"0\", \"1\"]).plot()\nplt.title(\"Confusion Matrix (Val Data)\")\n\nGraded Task: What does the confusion matrix tell you about how the m_once model is achieving 74% accuracy?\n\n# TODO: Your explanation goes here.\n\nWe have been choosing a threshold of 0.5 for turning our continuous predicted probabilities into a discrete prediction. However, this can be an arbitrary choice.\nTask: Explain why, in practical application, it may be reasonable to use a different threshold value. In what situation might you want the threshold to be set very high in order to make a positive prediction? What about a negative prediction?\n\n# TODO: Your explanation goes here.\n\nA receiver operating characteristic curve (or ROC) shows how the True Positive Rate and False Positive Rate vary based on our choice of the decision making threshold used to binarize predictions. By default, this threshold is 0.5, but it can be changed to any value between 0 and 1. Different thresholds will result in different TP and FP rates, all of which are illustrated on our graph. we can calculate the area underneath this curve in order to get a sense as to how our classifiers might work across a wide range of different thresholds. This calcution of area can also be used as a metric of our model’s “goodness”, and it is called AUC (or “Area Under Curve”).\nThe AUC metric is particularly useful for machine learning practitioners because it does not depend on the choice of the threshold value used for making discrete predicions. The metric is also resistant to measurement.\nTask: Is it better for the AUC to be larger or smaller? Explain why.\n\n# TODO: Your explanation goes here\n\nThe code below plots the ROC curve for a model.\n\nfrom sklearn.metrics import roc_curve, RocCurveDisplay, auc\n\ny, t = get_prediction(model, val_data)\n\nfpr, tpr, thresholds = roc_curve(t, y)\nroc_auc = auc(fpr, tpr)\nrocp = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc)\nrocp.plot()\nplt.title(\"Validation ROC Curve\")\n\n\ny, t = get_prediction(model, train_data)\n\nfpr, tpr, thresholds = roc_curve(t, y)\nroc_auc = auc(fpr, tpr)\nrocp = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc)\nrocp.plot()\nplt.title(\"Training ROC Curve\")\n\nHere is a function you can use to estimate the auc:\n\ndef get_auc(model, data):\n    y, t = get_prediction(model, data)\n    fpr, tpr, thresholds = roc_curve(t, y)\n    return auc(fpr, tpr)"
  },
  {
    "objectID": "labs/lab03.html#part-4.-hyperparameter-tuning-via-grid-search",
    "href": "labs/lab03.html#part-4.-hyperparameter-tuning-via-grid-search",
    "title": "CSC413 Lab 3: Multi-Layer Perceptrons with MedMNIST",
    "section": "Part 4. Hyperparameter Tuning via Grid Search",
    "text": "Part 4. Hyperparameter Tuning via Grid Search\nAs we mentioned in lab 1, hyperparameter choices matter significantly, and these hyperparameter choices interact with one another. Practitioners use a strategy called grid search to try all variations of hyperparameters from a set of hyperparameters.\nOne very important hyperparameter is the number of hidden units in our MLPModel. This setting affects the number of parameters (weights/biases) used in our model.\nThe use of ReLU vs sigmoid activation function is another hyperparameter that we will explore.\nFinally, optimization parameters like the batch size and the learning rate can also significantly affect the learning process.\n\nclass MLPModelSigmoid(nn.Module):\n    \"\"\"A three-layer MLP model for binary classification\"\"\"\n    def __init__(self, input_dim=28*28, num_hidden=100):\n        super(MLPModelSigmoid, self).__init__()\n        self.fc1 = nn.Linear(input_dim, num_hidden)\n        self.fc2 = nn.Linear(num_hidden, num_hidden)\n        self.fc3 = nn.Linear(num_hidden, 1)\n        self.sig = nn.Sigmoid()\n\n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.sig(out)\n        out = self.fc2(out)\n        out = self.sig(out)\n        out = self.fc3(out)\n        return out\n\nGraded Task: Complete the code below, which performs grid search over the following hyperparameter values of the:\n\nhidden size\nactivation function (ReLu vs sigmoid activation)\nbatch size\nlearning rate\n\nDo so by creating a new model and train it with the appropriate settings, then assessing the final training/validation accuracy, precision, recall, and AUC score. You may use to use the flag plot=False when calling train_model. You might also set plot_every to a large value and visualize the training curve as a separate step for hyperparameter values that you’re interested in.\nPlease include all your output in your submission.\n(There is one more graded task below that you can complete while the hyperparameter tuning is running.)\n\ngridsearch = {}\nfor num_hidden in [25, 100, 250]:\n    for act in [\"relu\", \"sigmoid\"]:\n        for bs in [10, 100, 500]:\n            for lr in [0.01, 0.1]:\n                # Adjust num_epoch based on the batch size, so that we \n                # train for the same number of iterations irrespective\n                # of batch size\n                ne = int(20 * (bs/100))\n\n                modelname = f\"num_hidden: {num_hidden}, activation: {act}, batch_size: {bs}, learning_rate: {lr}\"\n                print(f\"========={modelname}\")\n\n                # TODO: create and train the model with the appropriate settings\n\n                # Update and display metrics. This part is done for you.\n                metrics = {\n                    \"acc_train\": accuracy(m, train_data),\n                    \"acc_val\": accuracy(m, val_data),\n                    \"precision_train\": precision(m, train_data),\n                    \"precision_val\": precision(m, val_data),\n                    \"recall_train\": recall(m, train_data),\n                    \"recall_val\": recall(m, val_data),\n                    \"auc_train\": get_auc(m, train_data),\n                    \"auc_val\": get_auc(m, val_data),\n                }\n                gridsearch[modelname] = metrics\n                print(f'Accuracy (train):{metrics[\"acc_train\"]} (val):{metrics[\"acc_val\"]}')\n                print(f'Precision (train):{metrics[\"precision_train\"]} (val):{metrics[\"precision_val\"]}')\n                print(f'Recall (train):{metrics[\"recall_train\"]} (val):{metrics[\"recall_val\"]}')\n                print(f'AUC (train):{metrics[\"auc_train\"]} (val):{metrics[\"auc_val\"]}')\n\nPlease include the below output in your submission\n\nprint(gridsearch)\n\nTask: Which hyperparameter choice is the “best”? You should base this answer on the validation AUC. Use the other metrics as a guide to understand the kinds of predictions and mistakes that your model is likely make. Train a final model with those hyperparameter values.\n\n# TODO\n\nTask: Report the test accuracy and AUC for this model, and plot the confusion matrix over the test set.\n\n# TODO\n\nGraded Task: Explain why a model with high AUC may still produce consistently poor predictions for a subset of the population. You might find this article interesting: Gender imbalance in medical imaging datasets produces biased classifiers for computer-aided diagnosis; in particular, Figure 1 shows how test AUC differs male/female patients depending on the training set used.\n\n# TODO"
  },
  {
    "objectID": "labs/lab01.html",
    "href": "labs/lab01.html",
    "title": "CSC413 Lab 1: Linear Models",
    "section": "",
    "text": "In this lab, we will review linear models for classification, which was discussed in depth in CSC311. We will use this as an opportunity to review key ideas, including the splitting of the dataset into train/validation/test sets, optimization methods using Stochastic Gradient Descent, and so on. This lab reviews Python libraries that you have used in CSC311, including numpy, matplotlib and others.\nBut the main aim of this lab is to introduce a new Python library that we will be using throughout this course: PyTorch. PyTorch provides automatic differentiation capabilities and other neural network tools. This means that we do not need to compute gradients ourselves! Instead, we rely on PyTorch to build the computation graph and compute gradients. PyTorch can do this because it knows how to compute gradients for simple operations like addition, multiplication, ReLU activation, and common functions like exponentials, logarithms, and so on. The neural networks we build require computation that are combinations of these simple operations.\nFor now, we will we solve a multi-class classification problem in two ways: first with numpy, and then with PyTorch.\nBy the end of this lab, you will be able to:\nPlease work in groups of 1-2 during the lab, but submit your own solution individually."
  },
  {
    "objectID": "labs/lab01.html#submission",
    "href": "labs/lab01.html#submission",
    "title": "CSC413 Lab 1: Linear Models",
    "section": "Submission",
    "text": "Submission\nIf you are working with a partner, start by creating a group on Markus. If you are working alone, click “Working Alone”.\nSubmit the ipynb file lab01.ipynb on Markus containing all your solutions to the Graded Tasks. Your notebook file must contain your code and outputs where applicable, including printed lines and images. Your TA will not run your code for the purpose of grading.\nFor this lab, you should submit the following:\n\nPart 1. Your explanation of the purpose of the training and validation sets. (1 point)\nPart 2. Your implementation of accuracy_basic. (2 point)\nPart 2. Your implementation of accuracy_vectorized. (1 point)\nPart 2. Your implementation of accuracy. (1 point)\nPart 3. Your computation of model_bias_grad. (2 points)\nPart 4. Your completion of train_model. (1 point)\nPart 5. Your list of hyperparameters. (1 point)\nPart 5. Your explanation of what happens if the learning rate is too large. (1 point)"
  },
  {
    "objectID": "labs/lab01.html#google-colab-setup",
    "href": "labs/lab01.html#google-colab-setup",
    "title": "CSC413 Lab 1: Linear Models",
    "section": "Google Colab Setup",
    "text": "Google Colab Setup\nWe will use Google Colab to open IPython Notebook (ipynb) file. This tool allows us to write and execute Python code through our browser, without any environmental setup.\nHere are the steps to open ipynb file on Google Colab.\n\nDownload lab01.ipynb, available from the Quercus course website.\nClick on the following link to open Google Colab: https://colab.research.google.com/\nClick “Upload”, then choose the file which has been downloaded in step 1.\n\nAnd that’s it! Now we can start writing the codes, creating the new code or text cell, etc.\nHere are some basic functionalities and features that you might find useful.\n\nRunning a cell\nClick the run button on the left side of the code cell (looks like a “play” button with a triangle in a circle)\nor\npress SHIFT + ENTER.\nInstalling libraries using Bash Commands\nAlthough most of the commonly used libraries (e.g. NumPy, Pandas, Matplotlib) are pre-installed, we may occasionally ask you to install new libraries or run other bash commands. Bash commands can be run by prefixing instructions in a code cell with ‘!’ in Google Colab (One exception: ‘cd’ command can be run by prefixing with ‘%’), e.g. !pip install [package name]\nMounting Google Drive\nYou may optionally mount Google Drive. Click the files button on the left pane, then click on ‘mount drive’ button (looks like a file icon with a google drive logo).\nor\nRun the following code snippet: from google.colab import drive     drive.mount('/content/drive') By mounting the drive, we can use any files or folders in our drive by using the path as follows: /content/drive/MyDrive/[folder name] For example, we can read the csv file uploaded in the drive using Pandas library as follows: pd.read_csv('/content/drive/MyDrive/myfolder/myfile.csv')\n\nNow, we are ready to import the necessary packages and begin our lab.\n\nimport torch\nimport matplotlib.pyplot as plt\nimport numpy as np"
  },
  {
    "objectID": "labs/lab01.html#part-1.-data",
    "href": "labs/lab01.html#part-1.-data",
    "title": "CSC413 Lab 1: Linear Models",
    "section": "Part 1. Data",
    "text": "Part 1. Data\nWe will use the MNIST data set, which consists of hand-written digits. This dataset is available within the torchvision.datasets library. The dataset creators divided the MNIST imgages into a training and test set, so that different researchers report test accuracy on a consistent set of images. (Recall that the test set is to be set aside and not used during training or to make any model decisions, and that it is used to estimate how well your models generalize to new data that it has never seen before.)\n\nfrom torchvision.datasets import MNIST\n\nmnist_train = MNIST(root=\".\",      # where on the disk to store the data\n                    download=True, # download the data if it does not already exist\n                    train=True)    # use the training set (rather than the test set)\n\nTask: If different practitioners are exploring machine learning models for the same task and data set, why is it important that they use these practitioners report their test performance (e.g., accuracy) on the same test set?\n\n# TODO: Write your answer here\n\nIt is always a good idea to visually inspect our data before working with it. First, let’s take a look at the first element of the training set:\n\nprint(mnist_train[0]) # a tuple consisting of the image, and the label (5)\n\nThe image can be displayed on Google Colab using matplotlib:\n\nplt.imshow(mnist_train[0][0], cmap='gray') # display the image\n\nIt is important to note that images are represented using numbers on your machine. Converting this image into a numpy array shows a representation of the image using 28x28 numbers, each representing a pixel value.\n\nnp.array(mnist_train[0][0])\n\nTask: What does the numerical value 0 (smallest possible value) mean in the image? What about the largest possible value, 255?\n\n# TODO: Write your answer here\n\nFor our purposes, we will only use the first 5000 elements of the training set. This is to make training faster.\nPyTorch also makes it easy to apply pre-processing transformations to the data, for example to normalize the data prior to using for training. We will use the standard preprocessing functions to transform the images into tensors for PyTorch to be able to use. This transformation also changes the values to be floating-point numbers between 0 and 1. Performing this transformation to PyTorch tensors now makes it easier to use PyTorch functionalities to help us create minibatches with this data.\n\nimport torchvision.transforms as transforms\n\nmnist_data = MNIST(root=\".\",      # where on the disk to store the data\n                   download=True, # download the data if it does not already exist\n                   train=True,    # use the canonical training set (rather than the test set)\n                   transform=transforms.ToTensor()) # transforms the images into PyTorch tensors\nmnist_data = list(mnist_data)[:5000]\n\nprint(mnist_data[0]) # a tuple consisting of a PyTorch tensor of shape (1, 28, 28) and an integer target label\n\nWe will split the data set into 3000 for training, 1000 for validation, and 1000 for test:\n\ntrain_data = mnist_data[:3000]\nval_data   = mnist_data[3000:4000]\ntest_data  = mnist_data[4000:]\n\nGraded Task: We described, above, that the purpose of the test set is to estimate how well our models would generalize to new data that it has never seen before. What are the purposes of the training and validation sets?\n\n# TODO: Write your answer here"
  },
  {
    "objectID": "labs/lab01.html#part-2.-a-linear-model-in-pytorch",
    "href": "labs/lab01.html#part-2.-a-linear-model-in-pytorch",
    "title": "CSC413 Lab 1: Linear Models",
    "section": "Part 2. A Linear Model in PyTorch",
    "text": "Part 2. A Linear Model in PyTorch\nYou may recall from CSC311 that in machine learning, we often describe a model by first describing how to make predictions with a model (e.g., multi-class classification), and then describe how to find appropriate weights (e.g., an optimization method like gradient descent). We will follow that process here.\nRecall that, mathematically, the multi-class classification model can be written as follows:\n\\[{\\bf y} = \\textrm{softmax}({\\bf W} {\\bf x} + {\\bf b})\\]\nWhere the \\[{\\bf x}\\] vector represents the input (i.e., the vector representation of the MNIST image), and the \\[{\\bf y}\\] vector contains the predicted probably of the image being in each class (i.e., predicted probability of the image being of each digit 0-9).\nTask: In the MNIST image classification tas, what are the shapes of the quantities \\[{\\bf x}\\], \\[{\\bf W}\\], \\[{\\bf b}\\] and \\[{\\bf y}\\]?\n\n# TODO: Write your answer here\n\nThe matrix \\[{\\bf W}\\] and \\[{\\bf b}\\] are the parameters of the model. The matrix \\[{\\bf W}\\] is sometimes called the weight matrix and the vector \\[{\\bf b}\\] the bias vector, but these parameters taken together are often referred to collectively as the weights. “Good” values of the parameters \\[{\\bf W}\\] and \\[{\\bf b}\\] are those that would produce values of the vector \\[{\\bf y}\\] that more accurately match the actual target label. We will need to discuss what “good” means and how to measure “good”ness and optimize it. For now, let’s begin by applying and analyzing a “bad” model: a model where the parameters \\[{\\bf W}\\] and \\[{\\bf b}\\] are chosen randomly.\nNotice that there are two parts to the computation \\[\\textrm{softmax}({\\bf W} {\\bf x} + {\\bf b})\\]: there is a linear transformation on \\[{\\bf X}\\], and then there is a softmax operation. PyTorch models these two components separately.\nThe linear transformation is modeled as a Python class in PyTorch. Since the parameters \\[{\\bf W}\\] and \\[{\\bf b}\\] are parameters of the linear transformation portion of the computation, the weights and biases are class attributes. The output of the linear transformation step is typically denoted using the symbol \\[{\\bf z}\\], and is called the logit (or unnormalized logits).\n\nimport torch.nn as nn\n\nmodel = nn.Linear(in_features=784,\n                  out_features=10)\n\nprint(model.weight) # the weight parameter, initialized to random values\nprint(model.bias)   # the bias parameter, initialized to random values\n\nThis model object can be called like a function to perform the computation \\[{\\bf W}{\\bf x} + {\\bf b}\\]:\n\n# reshape the input image into the shape [1, 784]\n# PyTorch always expects inputs of shape [batch_size, num_features]\nx = train_data[0][0].reshape(1, 784)\n\n# Computes z = Wx + b, also called the *logit*\nz = model(x)\nprint(z)\n\nThe softmax operation has no trainable parameters (i.e., no numbers that we tune that would effect the predictions of our model). Torch ahs a function torch.softmax that performs this operation, which normalizes the prediction so that the prediction represents a probability distribution. The dim parameter to the function tells PyTorch which dimension represent the different label classes (as opposed to, say, the dimension that represents different images in a batch).\n\ny = torch.softmax(z, dim=1)\nprint(y) # notice that this represents a probability distribution!\n\nIf we are looking for a discrete/point prediction rather than a probability distribution, we will typically choose the label that the model believes to be most probable:\n\npred = torch.argmax(y, axis=1)\nprint(pred) # a prediction of which class/digit the image belong to\n\nNote that if all we wanted was a discrete prediction, we need not compute the softmax! (Why is that? How can we prove this property mathematically, using the definition of the softmax operation?)\n\npred = torch.argmax(z, axis=1)\nprint(pred)\n\nGraded Task: Complete the function below, which computes the accuracy of a PyTorch model over a dataset. The accuracy metric is the proportion of predictions made that is correct, or:\n\\[\\frac{\\textrm{the number of correct predictions}}{\\textrm{total number of predictions made}}\\]\n\ndef accuracy_basic(model, dataset):\n    \"\"\"\n    Compute the accuracy of `model` over the `dataset`.\n    We will take the **most probable class**\n    as the class predicted by the model.\n\n    Parameters:\n        `model` - A torch.nn model. We will only be passing `nn.Linear` models.\n                  However, to make your code more generally useful, do not access\n                  `model.weight` and `model.bias` parameters directly. These\n                  class attributes may not exist for other kinds of models.\n        `dataset` - A list of 2-tuples of the form (x, t), where `x` is a PyTorch\n                  tensor of shape [1, 28, 28] representing an MNIST image,\n                  and `t` is the corresponding target label\n\n    Returns: a floating-point value between 0 and 1.\n    \"\"\"\n    total = 0      # count the total number of predictions made\n    correct = 0    # count the number of correct predictions made\n    for img, t in dataset:\n        x = None # TODO - what should the input be? Recall that\n        z = None # TODO - how can we compute z = Wx + b using `model`?\n        pred = None # TODO - how can we obtain a point prediction?\n        if t == pred:\n            correct += 1\n        total += 1\n    return correct / total\n\n\nprint(\"Accuracy over the training set:\")\nprint(accuracy_basic(model, train_data))\n\nTask: Explain why we would expect the training accuracy above to be poor.\n\n# TODO: Your explanation goes here.\n\nOne other nice thing about our nn.Linear model is that PyTorch vectorizes computations for us: we can make predictions for many images at the same time. Here is a rudimentary example where we make predictions for the first three images of our training set.\n\nx1 = train_data[0][0].reshape(1, 784)\nx2 = train_data[1][0].reshape(1, 784)\nx3 = train_data[2][0].reshape(1, 784)\n\nX = torch.cat([x1, x2, x3]).reshape(-1, 784) # note the -1 value here, explained below\nprint(X.shape) # Pytorch figures out that the shape of this tensor needs to be [3, 784]\n\nThe above code uses a features of PyTorch’s reshape() method that allows you to use the size -1 as a placeholder value and let PyTorch figure out what the correct size should be.\nNow, we can make predictions for all 3 images simultaneously\n\nz = model(X)\ny = torch.softmax(z, dim=1)\nprint(y)\n\nTask Complete the function accuracy_vectorized that outputs the same result as accuracy_basic, but uses vectorization to compute predictions for all inputs in the dataset simultaneously.\n\ndef accuracy_vectorized(model, dataset):\n    \"\"\"\n    Same signature as the `accuracy_basic()` function, but the call to model() is vectorized\n    \"\"\"\n    X = torch.concat([x for x, t in dataset])\n    t = torch.Tensor([t for x, t in dataset])\n    z = None # TODO: use a single call to model() to compute the prediction for all inputs\n    pred = None # TODO: `pred` should have the same shape as `t`\n    correct = int(torch.sum(t == pred)) # count the number of correct predictions\n    total = t.shape[0]                  # count the total number of predictions made\n    return correct / total\n\nTask: Compare the runtime of accuracy_basic and accuracy_vectorized by running the two cells below. The line %%time prints the amount of time that Colab takes to run the code in the cell. The function call is repeated 100 times so that we can more clearly see the difference in runtime. Using what you learned from CSC311, explain why accuracy_vectorized is faster.\n\nimport time\n\nstart_time = time.time()\nfor i in range(100):\n    accuracy_basic(model, train_data)\nend_time = time.time()\nelapsed_time = end_time - start_time\nprint(f\"Elapsed time: {elapsed_time:.4f} seconds\")\n\n\nimport time\n\nstart_time = time.time()\nfor i in range(100):\n    accuracy_vectorized(model, train_data)\nend_time = time.time()\nelapsed_time = end_time - start_time\n\nprint(f\"Elapsed time: {elapsed_time:.4f} seconds\")\n\n\n# TODO: Your explanation goes here\n\nIf our data set is large, feeding all inputs into the model at the same time may result in an out of memory error. Thus, we may find PyTorch’s DataLoader useful. This class takes our data set and the desired batch size, and splits the data into mini-batches of that size. We can use a loop to iterate over the minibatches:\n\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size=100)\nfor X, t in train_loader:\n    print(X.shape)\n    print(t.shape)\n    break\n\nTask: Complete the definition of the accuracy function below:\n\ndef accuracy(model, dataset):\n    \"\"\"\n    Same signature as the `accuracy_basic()` function, but we will use a DataLoader and process\n    100 images at a time\n    \"\"\"\n    correct, total = 0, 0\n    loader = torch.utils.data.DataLoader(dataset, batch_size=100)\n    for X, t in loader:\n        z = None # TODO: use a single call to `model()` here as before\n        pred = None # TODO: `pred` should have the same shape as `t`\n        # TODO: update `correct` and `total`\n    return correct / total\n\naccuracy(model, train_data)"
  },
  {
    "objectID": "labs/lab01.html#part-3.-cross-entropy-loss-and-automatic-gradient-computation",
    "href": "labs/lab01.html#part-3.-cross-entropy-loss-and-automatic-gradient-computation",
    "title": "CSC413 Lab 1: Linear Models",
    "section": "Part 3. Cross Entropy Loss and Automatic Gradient Computation",
    "text": "Part 3. Cross Entropy Loss and Automatic Gradient Computation\nNow that we understand how a linear model makes predictions, we can explore how to modify its parameters to produce a model that makes “better” predictions. To do this, we will define a measure of “good”ness (or rather, “bad”ness) of a model that is differentiable with respect to the parameters. Differentiability is important, because it allows us to compute derivatives with respect to these parameters, which tells us how to tune the parameters to decrease “bad”ness.\nThis “badness” metric is called a loss function. A loss function compares the model prediction against the ground-truth target and produces a value representing how different the prediction is from the target. Like in CSC311, we will use the Cross-Entropy Loss for multi-class classification. In statistics courses you may learn about the theoretical reasons why the cross-entropy loss is appropriate for the multi-class classification task.\n\\[\\mathcal{L}(y, t) = - t \\log(y) - (1-t) \\log(1-y)\\]\nYou can read more about PyTorch’s implementation of the Cross-entropy loss here: https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html One key thing to note is that the cross entropy loss takes as input the unnormalized logits (the z’s) and not the post-softmax, normalized probabilities (the y’s).\n\ncriterion = nn.CrossEntropyLoss()\n\nBefore we go further, let’s get some more intuition about the cross-entropy loss. We first demonstrate the cross entropy loss in action by computing hte “badness”\n\nx = train_data[0][0].reshape(1, 784)        # input\nt = torch.Tensor([train_data[0][1]]).long() # target label, represented as an integer index\nz = model(x.reshape(-1, 784))               # prediction logit\n\nloss = criterion(z, t)\n\nprint(loss)\n\nGraded Task: Consider the code below. What value of label would produce the lowest cross-entropy loss? Why? To answer this question, start by exploring the arguments passed to the call to criterion, and form an understanding of what those arugments represent. (Why are there 3 possible values of label? What do the 3 floating-point values in the first argument to criterion represent?)\n\nlabel = None # 0, 1, or 2?\n\nloss = criterion(torch.Tensor([[1.5, 2.2, 3.2]]),\n                 torch.Tensor([label]).long())\nprint(loss)\n\n\n# TODO: Explain why the label choice produces the lowest loss.\n\nNow that we have an understanding of the cross-entropy loss, we can begin to understand how PyTorch computes gradients. Notice that when we print the variable loss above, the value printed is not only a numerical value, but also has other information attached. These information help PyTorch compute gradients, and these gradients can be propagated backwards using the loss.backward() method.\nUnder the hood, PyTorch performs backpropagation, which we discussed in CSC311 and will review again in the coming weeks. After loss.backward() is computed, tensors like model.bias and model.weights will have gradients.\n\n# recreate the model to clean up some hidden variables\nmodel = nn.Linear(in_features=784, out_features=10)\n\nx = train_data[0][0].reshape(1, 784)        # input\nt = torch.Tensor([train_data[0][1]]).long() # target label, represented as an integer index\nz = model(x.reshape(-1, 784))               # prediction logit\n\nloss = criterion(z, t)\nprint(loss)\n\nloss.backward() # propagate the gradients with respect to the loss\n\n\nprint(model.bias.grad) # the gradient of the loss with respect to the bias vector\n\n\nprint(model.weight.grad) # the gradient of the loss with respect to the weight matrix\n\nGraded Task: Verify that model.bias.grad is correct by computing this gradient explicitly in PyTorch. You may wish to begin by reviewing your past CSC311 notes, or by using calculus to find an expression to represent this quantity.\n\nt_onehot = torch.eye(10)[t] # You may find this useful. (Why? What does this quantity represent)\nmodel_bias_grad = None      # TODO\n\nprint(model_bias_grad) # should be the same as model.bias.grad\n\nThe gradient computation works in a vectorized setting as well.\n\n# create a data set with 3 inputs, 3 targets\n\nx1 = train_data[0][0].reshape(1, 784)\nx2 = train_data[1][0].reshape(1, 784)\nx3 = train_data[1][0].reshape(1, 784)\nX = torch.cat([x1, x2, x3]).reshape(-1, 784)\nt = torch.Tensor([train_data[0][1], train_data[1][1], train_data[2][1]]).long()\n\nprint(X.shape)\nprint(t)\n\nThe average loss (mean) across the data points are shown:\n\nmodel = nn.Linear(in_features=784, out_features=10)\nz = model(X)\nloss = criterion(z, t)\nprint(loss)\n\nloss.backward() # propagate the gradients with respect to the loss\n\nGraded Task: Verify that model.bias.grad is correct when using vectorized input by computing this gradient explicitly in PyTorch. Again, we recommend working this out by hand first. After you have done so, you may find the function torch.sum() or torch.mean() helpful.\n\nt_onehot = torch.eye(10)[t] # TODO: understand the shape of this quantity before using it\nmodel_bias_grad = None      # TODO\n\nprint(model_bias_grad)\nprint(model.bias.grad) # should be the same as above"
  },
  {
    "objectID": "labs/lab01.html#part-4.-neural-network-training-via-stochastic-gradient-descent.",
    "href": "labs/lab01.html#part-4.-neural-network-training-via-stochastic-gradient-descent.",
    "title": "CSC413 Lab 1: Linear Models",
    "section": "Part 4. Neural Network Training via Stochastic Gradient Descent.",
    "text": "Part 4. Neural Network Training via Stochastic Gradient Descent.\nWe are almost ready to use PyTorch to train the model. One last piece that we need is an optimizer that updates the model parameter based on the gradient. This update can be done in different ways, and the most basic approach discussed in CSC311 was using gradient descent.\n\\[{\\bf W} \\leftarrow {\\bf W} - \\lambda \\frac{\\partial \\mathcal{L}}{\\partial {\\bf W}}\\]\nTODO: description of gradient descent here!!\nWhen we use the entire training data set to compute the gradient of the mean loss (with respect to each parameter), we call the approach full batch gradient descent. However, this approach is expensive if we have a large training set. Typically, we approximate this mean loss using a minibatch, or a small sample of the training set. Using gradient descent with this approximate gradient is called stochastic gradient descent.\nPyTorch has built-in classes inside the package torch.optim to perform these gradient update steps. We will use the SGD class. Initializing this class requires a few things, including the list of model parameters that we want to optimize. For us, the list of parameters to optimize include model.weight and model.bias, which we can obtain by calling model.parameters().\n\nimport torch.optim as optim\n\noptimizer = optim.SGD(model.parameters(), # the parameters to optimize\n                      lr=0.005)           # the learning rate\n\nThere are two important optimizer methods that we will use. First is the optimizer.zero_grad() method, which clears the .grad attribute of the parameters. Let’s see how it works:\n\nprint(model.bias.grad) # should be a nonzero value from above\n\noptimizer.zero_grad()\n\nprint(model.bias.grad) # should be cleared\n\nThe other method is the step() method, which performs the actual gradient descent update.\n\nmodel = nn.Linear(in_features=784, out_features=10)\noptimizer = optim.SGD(model.parameters(), lr=0.005)\n\nprint(model.bias)\n\nz = model(X)\nloss = criterion(z, t)\nloss.backward()\n\nprint(model.bias.grad) # gradient\nprint(model.bias - 0.005 * model.bias.grad) # this should be the updated value of model.bias\n\noptimizer.step()\n\nprint(model.bias) # should be different compared to above\n\nNow that we have everything in place, we are ready to write the training loop:\nGraded Task: Complete the function below, which trains the model.\n\ndef train_model(model,\n                train_data,\n                val_data,\n                learning_rate=0.005,\n                batch_size=100,\n                num_epochs=10,\n                plot_every=10):\n    \"\"\"\n    Train the PyTorch model `model` using the training data `train_data` and the\n    corresponding hyperparameters. Report training loss, training accuracy, and\n    validation accuracy every `plot_every` iterations.\n    \"\"\"\n    train_loader = torch.utils.data.DataLoader(train_data,\n                                               batch_size=batch_size,\n                                               shuffle=True) # reshuffle minibatches every epoch\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n\n    # these lists will be used to track the training progress\n    # and to plot the training curve\n    iters, train_loss, train_acc, val_acc = [], [], [], []\n    iter_count = 0 # count the number of iterations that has passed\n\n    try:\n        for e in range(num_epochs):\n            for i, (images, labels) in enumerate(train_loader):\n                z = None # TODO\n\n                loss = None # TODO\n\n                loss.backward() # propagate the gradients\n                optimizer.step() # update the parameters\n                optimizer.zero_grad() # clean up accumualted gradients\n\n                iter_count += 1\n                if iter_count % plot_every == 0:\n                    iters.append(iter_count)\n                    train_loss.append(float(loss))\n                    train_acc.append(accuracy(model, train_data))\n                    val_acc.append(accuracy(model, val_data))\n    finally:\n        # This try/finally block is to display the training curve\n        # even if training is interrupted\n        plt.figure()\n        plt.plot(iters[:len(train_loss)], train_loss)\n        plt.title(\"Loss over iterations\")\n        plt.xlabel(\"Iterations\")\n        plt.ylabel(\"Loss\")\n\n        plt.figure()\n        plt.plot(iters[:len(train_acc)], train_acc)\n        plt.plot(iters[:len(val_acc)], val_acc)\n        plt.title(\"Accuracy over iterations\")\n        plt.xlabel(\"Iterations\")\n        plt.ylabel(\"Accuracy\")\n        plt.legend([\"Train\", \"Validation\"])\n\nmodel = nn.Linear(784, 10)\ntrain_model(model, train_data, val_data)"
  },
  {
    "objectID": "labs/lab01.html#part-5.-hyperparameter-tuning",
    "href": "labs/lab01.html#part-5.-hyperparameter-tuning",
    "title": "CSC413 Lab 1: Linear Models",
    "section": "Part 5. Hyperparameter Tuning",
    "text": "Part 5. Hyperparameter Tuning\nOur training process is not yet complete. In general, the performance of machine learning models depend heavily on the hyperparameter settings used. Hyperparameters are settings that cannot be tuned via gradient descent in a straightforward way. These settings can affect the model architecture, but may also affect the optimization process.\nGraded Task: What are some examples of hyperparameters that affect the model architecture of a model? You may consider some hyperparameters that you learned about in CSC311. List at least 3 examples.\n\n# TODO: List at least 3 examples\n\nTask: What are some examples of hyperparameters that affect the optimization process?\n\n# TODO: List at least 2 examples\n\nModel architecture related hyperparameters are important to tune and should not be neglected in practical application. However, since we are working with a linear model right now, we are limited in this lab to exploring the optimization hyperparameters.\nTask: What happens if the learning rate is too small? Provide an example training curve by calling train_model with a low learning rate, and describe the features of the training curve that you see.\n\n# TODO\n\nGraded Task: What happens if the learning rate is too large? Provide an example training curve by calling train_model with a large learning rate, and describe the features of the training curve that you see.\n\n# TODO\n\nHyperparameter choices interact with one another. Thus, practitioners use a strategy called grid search to try all variations of hyperparameters from a set of hyperparameters. We will not do that here since linear models don’t yet have many hyperparameters to work with.\nTask: Choose the best model that you have trained. Typically we make this choice using the validation accuracy. To understand how well the model you choose would generalize to unseen data, we use the test data. Compute the test accuracy for this model by calling the function accuracy() on the model and the test data.\n\n# TODO: Compute the test accuracy"
  },
  {
    "objectID": "labs/lab07.html",
    "href": "labs/lab07.html",
    "title": "CSC413 Lab 7: Transfer Learning and Descent",
    "section": "",
    "text": "Transfer learning is a technique where we use neural network weights trained to complete one task to complet a different task. In this tutorial, we will go through an example of transfer learning to detect American Sign Language (ASL) gestures letters A-I. Although we could train a CNN from scratch, you will see that using CNN weights that are pretrained on a larger dataset and more complex task provides much better results, all with less training.\nAmerican Sign Language (ASL) is a complete, complex language that employs signs made by moving the hands combined with facial expressions and postures of the body. It is the primary language of many North Americans who are deaf and is one of several communication options used by people who are deaf or hard-of-hearing.\nThe hand gestures representing English alphabets are shown below. This lab focuses on classifying a subset of these hand gesture images using convolutional neural networks. Specifically, given an image of a hand showing one of the letters A-I, we want to detect which letter is being represented.\nBy the end of this lab, you will be able to:\nAcknowledgements:\nPlease work in groups of 1-2 during the lab."
  },
  {
    "objectID": "labs/lab07.html#submission",
    "href": "labs/lab07.html#submission",
    "title": "CSC413 Lab 7: Transfer Learning and Descent",
    "section": "Submission",
    "text": "Submission\nIf you are working with a partner, start by creating a group on Markus. If you are working alone, click “Working Alone”.\nSubmit the ipynb file lab07.ipynb on Markus containing all your solutions to the Graded Tasks. Your notebook file must contain your code and outputs where applicable, including printed lines and images. Your TA will not run your code for the purpose of grading.\nFor this lab, you should submit the following:\n\nPart 1. Your answer to the question about the splitting of the data into train/validation/test sets. (1 point)\nPart 2. Your comparison of the CNN model with and without batch normalization. (1 point)\nPart 2. Your comparison of BatchNorm1d vs BatchNorm2d. (1 point)\nPart 2. Your analysis of the effect of varying the CNN model width. (1 point)\nPart 2. Your analysis of the effect of varying weight decay parameter. (1 point)\nPart 2. Your analysis of the training curve that illustrates double descent. (1 point)\nPart 3. Your implementation of LinearModel for transfer learning. (1 point)\nPart 3. Your comparison of transfer learning vs the CNN model. (1 point)\nPart 4. Your analysis of the confusion matrix. (1 point)\nPart 4. Your explanation for how to mitigate an issue we notice by visually inspecting misclassified images. (1 point)\n\n\nimport matplotlib\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.models, torchvision.datasets\n\n%matplotlib inline"
  },
  {
    "objectID": "labs/lab07.html#part-1.-data",
    "href": "labs/lab07.html#part-1.-data",
    "title": "CSC413 Lab 7: Transfer Learning and Descent",
    "section": "Part 1. Data",
    "text": "Part 1. Data\nWe will begin by downloading the data onto Google Colab.\n\n# Download lab data file\n!wget https://www.cs.toronto.edu/~lczhang/413/asl_data.zip\n!unzip asl_data.zip\n\nThe file structure we use is intentional, so that we can use torchvision.datasets.ImageFolder to help load our data and create labels.\nYou can read what torchvision.datasets.ImageFolder does for us here https://pytorch.org/docs/stable/torchvision/datasets.html#imagefolder\n\ntrain_path = \"asl_data/train/\" # edit me\nvalid_path = \"asl_data/valid/\" # edit me\ntest_path = \"asl_data/test/\"   # edit me\n\ntrain_data = torchvision.datasets.ImageFolder(train_path, transform=torchvision.transforms.ToTensor())\nvalid_data = torchvision.datasets.ImageFolder(valid_path, transform=torchvision.transforms.ToTensor())\ntest_data = torchvision.datasets.ImageFolder(test_path, transform=torchvision.transforms.ToTensor())\n\nAs in previous labs, we can iterate through the one training data point at a time like this:\n\nfor x, t in train_data:\n    print(x, t)\n    plt.imshow(x.transpose(2, 0).transpose(0, 1).numpy()) # display an image\n    break # uncomment if you'd like\n\nTask: What do the variables x and t contain? What is the shape of our images? What are our labels? Based on what you learned in Part (a), how were the labels generated from the folder structure?\n\n# Your explanation goes here\n\nWe saw in the earlier tutorials that PyTorch has a utility to help us creat minibatches with our data. We can use the same DataLoader helper here:\n\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size=10, shuffle=True)\n\nfor x, t in train_loader:\n    print(x, t)\n    break # uncomment if you'd like\n\nTask: What do the variables x and t contain? What are their shapes? What data do they contain?\n\n# Your explanation goes here\n\nTask: How many images are there in the training, validation, and test sets?\n\n# Your explanation goes here\n\nNotice that there are fewer images in the training set, compared to the validation and test sets. This is so that we can explore the effect of having a limited training set.\nGraded Task: The data set is generated by students taking pictures of their hand while making the corresponding gestures. We therefor split the training, validation, and test sets were split so that images generated by a student all belongs in a single data set. In other words, we avoid cases where some students’ images are in the training set and others end up in the test set. Why do you think this important for obtaining a representative test accuracy?\n\n# Your explanation goes here"
  },
  {
    "objectID": "labs/lab07.html#part-2.-training-a-cnn-model",
    "href": "labs/lab07.html#part-2.-training-a-cnn-model",
    "title": "CSC413 Lab 7: Transfer Learning and Descent",
    "section": "Part 2. Training a CNN Model",
    "text": "Part 2. Training a CNN Model\nFor this part, we will be working with this CNN network.\n\nclass CNN(nn.Module):\n    def __init__(self, width=4, bn=True):\n        \"\"\"\n        A 4-layer convolutional neural network. The first layer has\n        `width` number of channels, and with each layer we half the\n        feature width/height and double the number of channels.\n\n        If `bn` is set to False, then batch normalization will not run.\n        \"\"\"\n        super(CNN, self).__init__()\n        self.width = width\n        self.bn = bn\n        # define all the conv layers\n        self.conv1 = nn.Conv2d(in_channels=3,\n                               out_channels=self.width,\n                               kernel_size=3,\n                               padding=1)\n        self.conv2 = nn.Conv2d(in_channels=self.width,\n                               out_channels=self.width*2,\n                               kernel_size=3,\n                               padding=1)\n        self.conv3 = nn.Conv2d(in_channels=self.width*2,\n                               out_channels=self.width*4,\n                               kernel_size=3,\n                               padding=1)\n        self.conv4 = nn.Conv2d(in_channels=self.width*4,\n                               out_channels=self.width*8,\n                               kernel_size=3,\n                               padding=1)\n        # define all the BN layers\n        if bn:\n            self.bn1 = nn.BatchNorm2d(self.width)\n            self.bn2 = nn.BatchNorm2d(self.width*2)\n            self.bn3 = nn.BatchNorm2d(self.width*4)\n            self.bn4 = nn.BatchNorm2d(self.width*8)\n        # pooling layer has no parameter, so one such layer\n        # can be shared across all conv layers\n        self.pool = nn.MaxPool2d(2, 2)\n        # FC layers\n        self.fc1 = nn.Linear(self.width * 8 * 14 * 14, 100)\n        self.fc2 = nn.Linear(100, 9)\n    def forward(self, x):\n        x = self.pool(torch.relu(self.conv1(x)))\n        if self.bn:\n            x = self.bn1(x)\n        x = self.pool(torch.relu(self.conv2(x)))\n        if self.bn:\n            x = self.bn2(x)\n        x = self.pool(torch.relu(self.conv3(x)))\n        if self.bn:\n            x = self.bn3(x)\n        x = self.pool(torch.relu(self.conv4(x)))\n        if self.bn:\n            x = self.bn4(x)\n        x = x.view(-1, self.width * 8 * 14 * 14)\n        x = torch.relu(self.fc1(x))\n        return self.fc2(x)\n\nTask:\nThe training code is written for you. Train the CNN() model for at least 6 epochs, and report on the maximum validation accuracy that you can attain.\nAs your model is training, you might want to move on to the next question.\n\ndef get_accuracy(model, data, device=\"cpu\"):\n    loader = torch.utils.data.DataLoader(data, batch_size=256)\n    model.to(device)\n    model.eval() # annotate model for evaluation (important for batch normalization)\n    correct = 0\n    total = 0\n    for imgs, labels in loader:\n        output = model(imgs.to(device))\n        pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n        correct += pred.eq(labels.view_as(pred)).sum().item()\n        total += imgs.shape[0]\n    return correct / total\n\ndef train_model(model,\n                train_data,\n                valid_data,\n                batch_size=64,\n                weight_decay=0.0,\n                learning_rate=0.001,\n                num_epochs=50,\n                plot_every=20,\n                plot=True,\n                device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")):\n    train_loader = torch.utils.data.DataLoader(train_data,\n                                               batch_size=batch_size,\n                                               shuffle=True)\n    model = model.to(device) # move model to GPU if applicable\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(),\n                           lr=learning_rate,\n                           weight_decay=weight_decay)\n    # for plotting\n    iters, train_loss, train_acc, val_acc = [], [], [], []\n    iter_count = 0 # count the number of iterations that has passed\n\n    try:\n        for epoch in range(num_epochs):\n            for imgs, labels in iter(train_loader):\n                if imgs.size()[0] &lt; batch_size:\n                    continue\n\n                model.train()\n                out = model(imgs)\n                loss = criterion(out, labels)\n                loss.backward()\n                optimizer.step()\n                optimizer.zero_grad()\n\n                iter_count += 1\n                if iter_count % plot_every == 0:\n                    loss = float(loss)\n                    tacc = get_accuracy(model, train_data, device)\n                    vacc = get_accuracy(model, valid_data, device)\n                    print(\"Iter %d; Loss %f; Train Acc %.3f; Val Acc %.3f\" % (iter_count, loss, tacc, vacc))\n\n                    iters.append(iter_count)\n                    train_loss.append(loss)\n                    train_acc.append(tacc)\n                    val_acc.append(vacc)\n    finally:\n        plt.figure()\n        plt.plot(iters[:len(train_loss)], train_loss)\n        plt.title(\"Loss over iterations\")\n        plt.xlabel(\"Iterations\")\n        plt.ylabel(\"Loss\")\n\n        plt.figure()\n        plt.plot(iters[:len(train_acc)], train_acc)\n        plt.plot(iters[:len(val_acc)], val_acc)\n        plt.title(\"Accuracy over iterations\")\n        plt.xlabel(\"Iterations\")\n        plt.ylabel(\"Accuracy\")\n        plt.legend([\"Train\", \"Validation\"])\n\nTask: Run the training code below. What validation accuracy can be achieved by this CNN?\n\ncnn = CNN(width=4)\ntrain_model(cnn, train_data, valid_data, batch_size=64, learning_rate=0.001, num_epochs=50, plot_every=25)"
  },
  {
    "objectID": "labs/lab07.html#part-2.-model-architecture-biasvariance-and-double-descent",
    "href": "labs/lab07.html#part-2.-model-architecture-biasvariance-and-double-descent",
    "title": "CSC413 Lab 7: Transfer Learning and Descent",
    "section": "Part 2. Model Architecture, Bias/Variance and Double Descent",
    "text": "Part 2. Model Architecture, Bias/Variance and Double Descent\nIn this section, we will explore the effect of various aspects of a CNN model architecture. We will pay particluar attention to architecture decisions that affect the bias and variance of the model. Finally, we explore a phenomenon called double descent.\nTo begin, let’s explore the effect of batch normalization.\nTask: Run the training code below to explore the effect of training without batch normalization.\n\ncnn = CNN(bn=False)\ntrain_model(cnn, train_data, valid_data, batch_size=64, learning_rate=0.001, num_epochs=50, plot_every=25)\n\nGraded Task: Compare the two sets of training curves above for the CNN model with and without batch normalization. What is the effect of batch normalization on the training loss and accuracy? What about the validation accuracy?\n\n# TODO: Include your analysis here\n\nGraded Task: We used the layer called BatchNorm2d in our CNN. What do you think is the difference between BatchNorm2d and BatchNorm1d? Why are we using BatchNorm2d in our CNN? Why would we use BatchNorm1d in an MLP? You may wish to consult the PyTorch documentation. (How can you find it?)\n\n# Explain your answer here\n\nTask: Run the training code below to explore the effect of varying the model width for this particular data set.\n\ncnn = CNN(width=2, bn=False)\ntrain_model(cnn, train_data, valid_data, batch_size=64, learning_rate=0.001, num_epochs=50, plot_every=25)\n\n\ncnn = CNN(width=4, bn=False)\ntrain_model(cnn, train_data, valid_data, batch_size=64, learning_rate=0.001, num_epochs=50, plot_every=25)\n\n\ncnn = CNN(width=16, bn=False)\ntrain_model(cnn, train_data, valid_data, batch_size=64, learning_rate=0.001, num_epochs=50, plot_every=25)\n\nGraded Task: What is the effect of varying the model width above for this particular data set? Do you notice an effect on the training loss? What about the training/validation accuracy? The final validation accuracy? (Your answer may or may not match your expectations. Please answer based on the actual results above.)\n\n# TODO: Include your analysis here\n\nTask: Run the training code below to explore the effect of weight decay when training a large model.\n\ncnn = CNN(width=16, bn=False)\ntrain_model(cnn, train_data, valid_data, batch_size=64, learning_rate=0.001, num_epochs=50, plot_every=25, weight_decay=0.001)\n\n\ncnn = CNN(width=16, bn=True) # try with batch norm on\ntrain_model(cnn, train_data, valid_data, batch_size=64, learning_rate=0.001, num_epochs=50, plot_every=25, weight_decay=0.001)\n\n\ncnn = CNN(width=16, bn=True) # try decreasing weight decay parameter\ntrain_model(cnn, train_data, valid_data, batch_size=64, learning_rate=0.001, num_epochs=50, plot_every=25, weight_decay=0.0001)\n\nGraded Task: What is the effect of setting weight decay to the above value? Do you notice an effect on the training loss? What about the training/validation accuracy? The final validation accuracy? (Again, your answer may or may not match your expectations. Please answer based on the actual results above.)\n\n# TODO: Include your analysis here\n\nTask: Note that there is quite a bit of noise in the results that we might obtain above. That is, if you run the same code twice, you may obtain different answers. Why might that be? What are two sources of noise/randomness?\n\n# TODO: Include your explanation here\n\nThese settings that we have been exporting are hyperparameters that should be tuned when you train a neural network. These hyperparameters interact with one another, and thus we should tune them using the grid search strategy mentioned in previous labs.\nYou are not required to perform grid search for this lab, so that we can explore a few other phenomena.\nOne interesting phenomenon is called double descent. In statistical learning theory, we expect validation error to decrease with increase model capacity, and then increase as the model overfits to the number of data points available for training. In practise, in neural networks, we often see that as model capacity increases, validation error first decreases, then increase, and then decrease again—hence the name “double descent”.\nIn fact, the increase in validation error is actually quite subtle. However, what is readily apparent is that in most cases, increasing model capacity does not result in a decrease in validation accuracy.\nOptional Task: To illustrate that validation accuracy is unlikely to decrease with increased model parameter, train the below network.\n\n# Uncomment to run. \n# cnn = CNN(width=40, bn=True)\n# train_model(cnn, train_data, valid_data, batch_size=64, learning_rate=0.001, num_epochs=50, plot_every=50)\n\nDouble descent is actually not that mysterious. It comes from the fact that when capacity is large enough there are many parameter choices that achieves 100% training accuracy, the neural network optimization procedure is effectively choosing a best parameters out of the many that can achieve this perfect training accuracy. This differs from when capacity is low, where the optimization process needs to find a set of parameter choices that best fits the training data—since no choice of parameters fits the training data perfectly. When the capacity is just large enough to be able to find parameters that fit the data, but too small for there be a range of parameter choices available to be able to select a “best” one.\nThis twitter thread written by biostatistics professor Daniela Witten also provides an intuitive explanation, using polynomial curve fitting as an example: https://twitter.com/daniela_witten/status/1292293102103748609\nDouble descent explored in depth in this paper here: https://openreview.net/pdf?id=B1g5sA4twr This paper highlights that the increase in validation/test error occurs when the training accuracy approximates 100%. Moreover, the double descent phenomena is noticable when varying model capacity (e.g. number of parameters) and when varying the number of iterations/epochs of training.\nWe will attempt to explore the latter effect—i.e. we will train a large model, use a small numer of training data points, and explore how each iteration of training impacts validation accuracy. The effect is subtle and, depending on your neural network initialization, you may not see an effect. So, a training curve is also provided for you to analyze.\nOptional Task: Run the code below to try and reproduce the “double descent” phenomena. This code will take a while to run, so you may wish to continue with the remaining questions while it runs.\n\n# use a subset of the training data\n# uncomment to train\n\n# train_data_subset, _ =  random_split(train_data, [50, len(train_data)-50])\n# cnn = CNN(width=20)\n# train_model(cnn,\n#             train_data_subset,\n#             valid_data,\n#             batch_size=50, # set batch_size=len(train_data_subset) to minimize training noise\n#             num_epochs=200,\n#             plot_every=1,  # plot every epoch (this is slow)\n#             learning_rate=0.0001)  # choose a low learning rate\n\nFor reference, here is the our training curve showing the loss and accuracy over 200 iterations:\n \nWe are not able to consistently reproduce this result (e.g., due to initialization), so it is totally reasonable for your figure to look different!\nTask: In the provided training curve, during which iterations do the validation accuracy initially increase (i.e. validation error decrease)?\n\n# TODO: Include your answer here\n\nGraded Task: In the provided training curve, during which iterations do the validation accuracy decrease slightly? Approximately what training accuracy is achieved at this piont?\n\n# TODO: Include your answer here\n\nTask: In the provided training curve, during which iterations do the validation accuracy increase for a second time (i.e. validation error descends for a second time)?\n\n# TODO: Include your answer here"
  },
  {
    "objectID": "labs/lab07.html#part-3.-transfer-learning",
    "href": "labs/lab07.html#part-3.-transfer-learning",
    "title": "CSC413 Lab 7: Transfer Learning and Descent",
    "section": "Part 3. Transfer Learning",
    "text": "Part 3. Transfer Learning\nFor many image classification tasks, it is generally not a good idea to train a very large deep neural network model from scratch due to the enormous compute requirements and lack of sufficient amounts of training data.\nA better option is to try using an existing model that performs a similar task to the one you need to solve. This method of utilizing a pre-trained network for other similar tasks is broadly termed Transfer Learning. In this assignment, we will use Transfer Learning to extract features from the hand gesture images. Then, train a smaller network to use these features as input and classify the hand gestures.\nAs you have learned from the CNN lecture, convolution layers extract various features from the images which get utilized by the fully connected layers for correct classification. AlexNet architecture played a pivotal role in establishing Deep Neural Nets as a go-to tool for image classification problems and we will use an ImageNet pre-trained AlexNet model to extract features in this assignment.\nHere is the code to load the AlexNet network, with pretrained weights. When you first run the code, PyTorch will download the pretrained weights from the internet.\n\nimport torchvision.models\nalexnet = torchvision.models.alexnet(pretrained=True)\n\nprint(alexnet)\n\nAs you can see, the alexnet model is split up into two components: alexnet.features and alexnet.classifier. The first neural network component, alexnet.features, is used to computed convolutional features, which is taken as input in alexnet.classifier.\nThe neural network alexnet.features expects an image tensor of shape Nx3x224x224 as inputs and it will output a tensor of shape Nx256x6x6 . (N = batch size).\nHere is an example code snippet showing how you can compute the AlexNet features for some images (your actual code might be different):\n\nimg, label = train_data[0]\nfeatures = alexnet.features(img.unsqueeze(0)).detach()\n\nprint(features.shape)\n\nNote that the .detach() at the end will be necessary in your code. The reason is that PyTorch automatically builds computation graphs to be able to backpropagate graidents. If we did not explicitly “detach” this tensor from the AlexNet portion of the computation graph, PyTorch might try to backpropagate gradients to the AlexNet weight and tune the AlexNet weights.\nTODO Compute the AlexNet features for each of your training, validation, and test data by completing the function compute_features. The code below creates three new arrays called train_data_fets, valid_data_fets and test_data_fets. Each of these arrays contains tuples of the form (alexnet_features, label).\n\ndef compute_features(data):\n    fets = []\n    for img, t in data:\n        features = None  # TODO\n        fets.append((features, t),)\n    return fets\n\ntrain_data_fets = compute_features(train_data)\nvalid_data_fets = compute_features(valid_data)\ntest_data_fets = compute_features(test_data)\n\nIn the rest of this part of the lab, we will test two models that will take as input these AlexNet features, and make a prediction for which letter the hand gesture represents. The two models are a linear model, a two-layer MLP. We will compare the performance of these two models.\nGraded Task: Complete the definition of the LinearModel class, which is a linear model (e.g., logistic regression). This model should as input these AlexNet features, and make a prediction for which letter the hand gesture represents.\n\nclass LinearModel(nn.Module):\n    def __init__(self):\n        super(LinearModel, self).__init__()\n        # TODO: What layer need to be initialized?\n\n    def forward(self, x):\n        x = x.view(-1, 256 * 6 * 6) # flatten the input\n        z = None # TODO: What computation needs to be performed?\n        return z\n\nm_linear = LinearModel()\nm_linear(train_data_fets[0][0]) # this should produce a(n unnormalized) prediction\n\nTask: Train a LinearModel() for at least 6 epochs, and report on the maximum validation accuracy that you can attain. We should still be able to use the train_model function, but make sure to provide the AlexNet features as input (and not the image features).\n\nm_linear = LinearModel()\n# TODO: Train the linear model. Include your output in your submission\n\nGraded Task: Compare this model with the CNN() models that we trained earlier. How does this model perform in terms of validation accuracy? What about in terms of the time it took to train this model?\n\n# TODO: Your observation goes here\n\nTask: We decide to use AlexNet features as input to our MLP, and avoided tuning AlexNet weights. However, we could have considered AlexNet to be a part of our model, and continue to tune AlexNet weights to improve our model performance. What are the advantages and disadvantages of continuing to tune AlexNet weights?\n\n# TODO"
  },
  {
    "objectID": "labs/lab07.html#part-4.-data",
    "href": "labs/lab07.html#part-4.-data",
    "title": "CSC413 Lab 7: Transfer Learning and Descent",
    "section": "Part 4. Data",
    "text": "Part 4. Data\nTask: Report the test accuracy on this transfer learning model.\n\n# TODO\n\nTask: Use this code below to construct the confusion matrix for this model over the test set.\n\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\nimport sklearn\nlabel = \"ABCDEFGHI\"\ndef plot_confusion(model, data):\n    n = 0\n    ts = []\n    ys = []\n    for x, t in data:\n        z = model(x.unsqueeze(0))\n        y = int(torch.argmax(z))\n        ts.append(t)\n        ys.append(y)\n\n    cm = confusion_matrix(ts, ys)\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label)\n    plt.figure()\n    disp.plot()\n\nplot_confusion(m_linear, test_data_fets)\n\nGraded Task: Which class is most likely mistaken as another? Is this reasonable? (i.e. is that class particularly challenging, or very similar to another class?)\n\n# TODO: Include your analysis here\n\nTask: In order to understand where errors come from, it is crucial that we explore why and how our models fail. A first step is to visually inspect the test data points where failure occurs. That way, we can identify what we can do to prevent/fix errors before our models are deployed.\nRun the below code to display images in the test set that our model misclassifies:\n\nfor i, (x, t) in enumerate(test_data_fets):\n    y = int(torch.argmax(m_linear(x)))\n    if not (y == t):\n        plt.figure()\n        plt.imshow(test_data[i][0].transpose(0,1).transpose(1,2).numpy())\n\nTask: By visually inspecting these misclassified images, we see that there are two main reasons for misclassification. What reason for misclassification is due to a mistake in the formatting of the test set images?\n\n# TODO\n\nGraded Task: We also see a much more serious issue, where gestures made by individuals with darker skin tones may be more frequently misclasified. This result suggests that errors in the model may impact some groups more than others. What steps should we take to mitigate this issue?\n\n# TODO"
  },
  {
    "objectID": "labs/lab02.html",
    "href": "labs/lab02.html",
    "title": "CSC413 Lab 2: Word Embeddings",
    "section": "",
    "text": "In this lab, we will build a neural network that can predict the next word in a sentence given the previous three. We will apply an idea called weight sharing to go beyond the multi-layer perceptrons that we discussed in class.\nWe will also solve this problem problem twice: once in numpy, and once using PyTorch. When using numpy, you’ll implement the backpropagation computation manually.\nThe prediction task is not very interesting on its own, but in learning to predict subsequent words given the previous three, our neural networks will learn about how to represent words. In the last part of the lab, we’ll explore the vector representations of words that our model produces, and analyze these representations.\nAcknowledgements:"
  },
  {
    "objectID": "labs/lab02.html#submission",
    "href": "labs/lab02.html#submission",
    "title": "CSC413 Lab 2: Word Embeddings",
    "section": "Submission",
    "text": "Submission\nClick “Working Alone” on Markus.\nSubmit the ipynb file lab02.ipynb on Markus containing all your solutions to the Graded Tasks. Your notebook file must contain your code and outputs where applicable, including printed lines and images. Your TA will not run your code for the purpose of grading.\nFor this lab, you should submit the following:\n\nPart 1. Your implementation of convert_words_to_indices (1 point)\nPart 2. Your implementation of do_forward_pass (4 points)\nPart 3. Your implementation of do_backward_pass (3 points)\nPart 3. The output of the gradient checking code (1 point)\nPart 4. Your explanation of why each row of model.Ww corresponds to a word representation (1 point)\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "labs/lab02.html#part-1.-data",
    "href": "labs/lab02.html#part-1.-data",
    "title": "CSC413 Lab 2: Word Embeddings",
    "section": "Part 1. Data",
    "text": "Part 1. Data\nWe will begin by downloading the data onto Google Colab.\n\n# Download lab data file\n!wget https://www.cs.toronto.edu/~lczhang/413/raw_sentences.txt\n\nWith any machine learning problem, the first thing that we would want to do is to get an intuitive understanding of what our data looks like. The following code reads the sentences in our file, split each sentence into its individual words, and stores the sentences (list of words) in the variable sentences.\n\nsentences = []\nfor line in open('raw_sentences.txt'):\n    words = line.split()\n    sentence = [word.lower() for word in words]\n    sentences.append(sentence)\n\nThere are 97,162 sentences in total, and these sentences are composed of 250 distinct words.\n\nvocab = set([w for s in sentences for w in s])\nprint(len(sentences)) # 97162\nprint(len(vocab)) # 250\n\nWe will separate our data into training, validation, and test. We will use 10,000 sentences for test, 10,000 for validation, and the remaining for training.\n\n# First, randomly shuffle the sentences in case these sentences are\n# temporally correlated (i.e., so that our train/val/test sets have\n# equal probability of getting the earlier vs later sentences)\nimport random\nrandom.seed(10)\nrandom.shuffle(sentences)\n\ntest, valid, train = sentences[:10000], sentences[10000:20000], sentences[20000:]\n\nTask: To get an understanding of the data set that we are working with, start by printing 10 sentences in the training set. How are punctuated treated in this word representation? What about words with apostrophes?\n\n# TODO: Your code goes here\n\nIt is also a good idea to explore the distributional properties of the data.\nTask: The length of the sentences affects the types of modeling we can perform on the data. If the sentences are too short, then using a model that depends on many previous words would not make sense. Run the below code, which computes the length of the shortest, average, and longest sentences in the training set.\n\nsentence_lengths = [len(s) for s in train]\nprint(\"Shortest Sentence\", np.min(sentence_lengths))\nprint(\"Average Sentence\", np.mean(sentence_lengths))\nprint(\"Longest Sentence\", np.max(sentence_lengths))\n\nTask: How many words are in the training set? In general, there may be words in the validation/test sets that are not in training!\n\n# TODO: Write code to perform the computation\n\nTask: What is the most common word in the training set? How often does this word appear in the training set? This information is useful to know since it helps us understand the difficult of the word prediction problem. In other words, this figure represents the accuracy of a “baseline” model that simply returns the most common word as the prediction for what the next word should be!\n\nfrom collections import Counter\n\ntotal_words = 0        # count number of words in the training set\nword_count = Counter() # count the occurrence of each word\nfor s in train:\n    for w in s:\n        word_count[w] += 1\n        total_words += 1\n\n# TODO: find the most common word\n\nNow that we understand a bit about the distributional properties of our data, we can move on to representing our data numerically in a way that a neural network can use.\nWe will use a one-hot encoding to represent words. Alternatively, you can think of what we’re doing as assigning each word to a unique integer index. We will need some functions that converts sentences into the corresponding word indices.\nGraded Task: Complete the helper functions convert_words_to_indices. The functions generate_4grams and process_data have been written for you. The process_data function will take a list of sentences (i.e. list of list of words), and generate an \\(N \\times 4\\) numpy matrix containing indices of 4 words that appear next to each other. You can use the constants vocab, vocab_itos, and vocab_stoi in your code.\n\n# A list of all the words in the data set. We will assign a unique\n# identifier for each of these words.\nvocab = sorted(list(set([w for s in train for w in s])))\n# A mapping of index =&gt; word (string)\nvocab_itos = dict(enumerate(vocab))\n# A mapping of word =&gt; its index\nvocab_stoi = {word:index for index, word in vocab_itos.items()}\n\ndef convert_words_to_indices(sents):\n    \"\"\"\n    This function takes a list of sentences (list of list of words)\n    and returns a new list with the same structure, but where each word\n    is replaced by its index in `vocab_stoi`.\n\n    Example:\n    &gt;&gt;&gt; convert_words_to_indices([['one', 'in', 'five', 'are', 'over', 'here'],\n                                  ['other', 'one', 'since', 'yesterday'],\n                                  ['you']])\n    [[148, 98, 70, 23, 154, 89], [151, 148, 181, 246], [248]]\n    \"\"\"\n    indices = []\n    # TODO: Write your code here\n    return indices\n\ndef generate_4grams(seqs):\n    \"\"\"\n    This function takes a list of sentences (list of lists) and returns\n    a new list containing the 4-grams (four consecutively occurring words)\n    that appear in the sentences. Note that a unique 4-gram can appear multiple\n    times, one per each time that the 4-gram appears in the data parameter `seqs`.\n\n    Example:\n\n    &gt;&gt;&gt; generate_4grams([[148, 98, 70, 23, 154, 89], [151, 148, 181, 246], [248]])\n    [[148, 98, 70, 23], [98, 70, 23, 154], [70, 23, 154, 89], [151, 148, 181, 246]]\n    &gt;&gt;&gt; generate_4grams([[1, 1, 1, 1, 1]])\n    [[1, 1, 1, 1], [1, 1, 1, 1]]\n    \"\"\"\n    grams = []\n    for seq in seqs:\n        for i in range(len(seq) - 4):\n            grams.append(seq[i:i+4])\n    return grams\n\ndef process_data(sents):\n    \"\"\"\n    This function takes a list of sentences (list of lists), and generates an\n    numpy matrix with shape [N, 4] containing indices of words in 4-grams.\n    \"\"\"\n    indices = convert_words_to_indices(sents)\n    fourgrams = generate_4grams(indices)\n    return np.array(fourgrams)\n\ntrain4grams = process_data(train)\nvalid4grams = process_data(valid)\ntest4grams = process_data(test)\n\nTask: We are almost ready to discuss the model. Review the following helper functions, which has been written for you:\n\nmake_onehot, which converts indices\n\n\ndef make_onehot(indicies, total=250):\n    \"\"\"\n    Convert indicies into one-hot vectors.\n\n    Parameters:\n        `indices` - a numpy array of any shape (e.g. `[N, 3]` where `N`\n                    is the batch size)\n        `total` - an integer describing the total number of possible classes\n                  (maximum possible value in `indicies`)\n\n    Returns: a one-hot representation of the input numpy array\n             (If the input is of shape `[X, Y]`, then the output would\n             be of shape `[X, Y, total]` and consists of 0's and 1's)\n    \"\"\"\n    # create an identity matrix of shape [total, total]\n    I = np.eye(total)\n    # index the appropriate columns of that identity matrix\n    return I[indicies]\n\ndef softmax(x):\n    \"\"\"\n    Compute the softmax of vector x, or row-wise for a matrix x.\n    We subtract x.max(axis=0) from each row for numerical stability.\n\n    Parameters:\n        `x` - a numpy array shape `[N, num_classes]`\n\n    Returns: a numpy array of the same shape as the input.\n    \"\"\"\n    x = x.T\n    exps = np.exp(x - x.max(axis=0))\n    probs = exps / np.sum(exps, axis=0)\n    return probs.T\n\nThere is one more data processing function that we need, which turns the four-grams into inputs (consisting of the one-hot representations of the first 3 words), and the target output (the index of the 4th word).\nSince the one-hot representation is not memory efficient, we will only convert data into this representation when required, and only do so at a minibatch level.\n\ndef get_batch(data, range_min, range_max, onehot=True):\n    \"\"\"\n    Convert one batch of data (specifically, `data[range_min:range_max]`)\n    in the form of 4-grams into input and output data and return the\n    training data.\n\n    Parameters:\n        `data` - a numpy array of shape [N, 4] produced by a call\n                 to the function `process_data`\n        `range_min` - the starting index of the minibatch\n        `range_max` - the ending index of the minibatch, with\n                      range_max &gt; range_min and\n                      batch_size = range_max - range_min\n        `onehot` - boolean value, if `True` the targets are also made\n                   to be one-hot vectors rather than indices\n\n    Returns: a tuple `(x, t)` where\n     - `x` is an numpy array of one-hot vectors of shape [batch_size, 3, 250]\n     - `t` is either\n            - a numpy array of shape [batch_size, 250] if onehot is True,\n            - a numpy array of shape [batch_size] containing indicies otherwise\n    \"\"\"\n    x = data[range_min:range_max, :3]\n    x = make_onehot(x)\n    x = x.reshape(-1, 750)\n    t = data[range_min:range_max, 3]\n    if onehot:\n        t = make_onehot(t).reshape(-1, 250)\n    return x, t\n\n# some testing code for illustrative purposes\nx_, t_ = get_batch(train4grams, 0, 10, onehot=False)\nprint(train4grams[0])\npos_index = train4grams[0][0]\nprint(x_[0, pos_index-1]) # should be 0\nprint(x_[0, pos_index])   # should be 1\nprint(x_[0, pos_index+1]) # should be 0\npos_index = train4grams[1][0]\nprint(x_[0, 250 + pos_index-1]) # should be 0\nprint(x_[0, 250 + pos_index])   # should be 1\nprint(x_[0, 250 + pos_index+1]) # should be 0\npos_index = train4grams[2][0]\nprint(x_[0, 500 + pos_index-1]) # should be 0\nprint(x_[0, 500 + pos_index])   # should be 1\nprint(x_[0, 500 + pos_index+1]) # should be 0\n\nFinally, the estimate_accuracy function has been provided to you as well. This function is similar to the accuracy function from lab 1.\n\ndef estimate_accuracy(model, data, batch_size=5000, max_N=10000):\n    \"\"\"\n    Estimate the accuracy of the model on the data. To reduce\n    computation time, use at least `max_N` elements of `data` to\n    produce the estimate.\n\n    Parameters:\n        `model` - an object (e.g. `NNModel`, see below) with a forward()\n                  method that produces predictions for an input\n        `data` - a dataset of 4grams (produced by `process_data`) over\n                 which we compute accuracy\n        `batch_size` - integer batch size to use to produce predictions\n        `max_N` - integer value describing the minimum number of predictions\n                  to make to produce the accuracy estimate\n\n    Returns: a floating point value between 0 and 1\n    \"\"\"\n    num_correct = 0\n    num_preds = 0\n    for i in range(0, data.shape[0], batch_size):\n        xs, ts = get_batch(data, i, i + batch_size, onehot=False)\n        z = model.forward(xs)\n        pred = np.argmax(z, axis=1)\n        num_correct += np.sum(ts == pred)\n        num_preds += ts.shape[0]\n\n        if num_preds &gt;= max_N: # at least max_N predictions have been made\n            break\n    return num_correct / num_preds"
  },
  {
    "objectID": "labs/lab02.html#part-2.-model-building-forward-pass",
    "href": "labs/lab02.html#part-2.-model-building-forward-pass",
    "title": "CSC413 Lab 2: Word Embeddings",
    "section": "Part 2. Model Building: Forward Pass",
    "text": "Part 2. Model Building: Forward Pass\nIn this section, we will build our deep learning model. As we did in the previous lab, we begin by understanding how to make predictions with this model. So, in this part of the lab, we will write the functions required to perform the forward pass operation. We will write the backward-pass and train the model in Part 3 and 4.\nTask: Consider the following two models architectures:\nModel 1: \nModel 2: \nIn Model 1, the input \\(\\bf{x}\\) consists of three one-hot vectors concatenated together. We can think of \\(\\bf{h}\\) as a representation of those three words (all together). However, the model architecture treat the three one-hot vectors from the three words distinctly. However, \\(\\bf{W^{(1)}}\\) needs to learn about the first word separately from the second and third word. In other words, the deep learning model treats these three sets of one-hot features as if they have no semantic connection in common.\nIn Model 2, we use an idea called weight sharing, where we use the sample set of weights \\(\\bf{W}^{(word)}\\) to map the one-hot vectors into a vector representation. This allows us to learn the weights \\(\\bf{W}^{(word)}\\) from informatino from all three words. This model architecture encodes our knowledge that the three sets of one-hot vectors share something in common.\nWe will use model 2 in the rest of this lab. For clarity, here is the forward-pass computation to be performed. (Note that this is not vectorized!)\n\\[\\begin{align*}\n\\bf{x_a} &= \\textrm{the one-hot vector for word 1} \\\\\n\\bf{x_b} &= \\textrm{the one-hot vector for word 2} \\\\\n\\bf{x_c} &= \\textrm{the one-hot vector for word 3} \\\\\n\\bf{v_a} &= \\bf{W}^{(word)} \\bf{x_a} \\\\\n\\bf{v_b} &= \\bf{W}^{(word)} \\bf{x_b} \\\\\n\\bf{v_c} &= \\bf{W}^{(word)} \\bf{x_c} \\\\\n\\bf{v} &= \\textrm{concatenate}(\\bf{v_a}, \\bf{v_b}, \\bf{v_c})\\\\\n\\bf{m} &= \\bf{W^{(1)}} \\bf{v} + \\bf{b^{(1)}} \\\\\n\\bf{h} &= \\textrm{ReLU}(\\bf{m}) \\\\\n\\bf{z} &= \\bf{W^{(2)}} \\bf{h} + \\bf{b^{(2)}} \\\\\n\\bf{y} &= \\textrm{softmax}(\\bf{z}) \\\\\nL &= \\mathcal{L}_\\textrm{Cross-Entropy}(\\bf{y}, \\bf{t}) \\\\\n\\end{align*}\\]\nThe class NNModel represents this above neural network model. This class stores the weights and biases of our model. Moreover, this class will also have methods that use and modify these weights.\nMost of the class has been implemented for you, including these methods:\n\nThe initializeParams() method, which randomly initializes the weights\nThe loss() method, which computes the average cross-entropy loss\nThe update() method, which performs the gradient updates\nThe cleanup() method, which clears the member variables used in the computation\n\nThe implementation for these methods are incomplete:\n\nThe forward method computes the prediction given a data matrix X. These computations are known as the forward pass. This method also saves some of the intermediate values in the neural network computation, to make gradient computation easier.\nThe backward method computes the gradient of the average loss with respect to various quantities (i.e. the error signals). These computations are known as the backward pass.\n\nYou may assume that during an iteration of gradient descent, the following methods will be called in order:\n\nThe cleanup method to clear information stored from the previous computation\nThe forward method to compute the predictions\nThe backward method to compute the error signals\n(Possibly the loss method to compute the average loss)\nThe update method to move the weights\n\nYou might recognize that the way we set up the class correspond to what PyTorch does.\n\nclass NNModel(object):\n    def __init__(self, vocab_size=250, emb_size=150, num_hidden=100):\n        \"\"\"\n        Initialize the weights and biases of this two-layer MLP.\n        \"\"\"\n        # information about the model architecture\n        self.vocab_size = vocab_size\n        self.emb_size = emb_size\n        self.num_hidden = num_hidden\n\n        # weights for the embedding layer of the model\n        self.Ww = np.zeros([vocab_size, emb_size])\n\n        # weights and biases for the first layer of the MLP\n        self.W1 = np.zeros([emb_size * 3, num_hidden])\n        self.b1 = np.zeros([num_hidden])\n\n        # weights and biases for the second layer of the MLP\n        self.W2 = np.zeros([num_hidden, vocab_size])\n        self.b2 = np.zeros([vocab_size])\n\n        # initialize the weights and biases\n        self.initializeParams()\n\n        # set all values of intermediate variables (to be used in the\n        # forward/backward passes) to None\n        self.cleanup()\n\n    def initializeParams(self):\n        \"\"\"\n        Initialize the weights and biases of this two-layer MLP to be random.\n        This random initialization is necessary to break the symmetry in the\n        gradient descent update for our hidden weights and biases. If all our\n        weights were initialized to the same value, then their gradients will\n        all be the same!\n        \"\"\"\n        self.Ww = np.random.normal(0, 2/self.vocab_size, self.Ww.shape)\n        self.W1 = np.random.normal(0, 2/(3*self.emb_size), self.W1.shape)\n        self.b1 = np.random.normal(0, 2/(3*self.emb_size), self.b1.shape)\n        self.W2 = np.random.normal(0, 2/self.num_hidden, self.W2.shape)\n        self.b2 = np.random.normal(0, 2/self.num_hidden, self.b2.shape)\n\n    def forward(self, X):\n        \"\"\"\n        Compute the forward pass to produce prediction logits.\n\n        Parameters:\n            `X` - A numpy array of shape (N, self.vocab_size * 3)\n\n        Returns: A numpy array of logit predictions of shape\n                 (N, self.vocab_size)\n        \"\"\"\n        return do_forward_pass(self, X) # To be implemented below\n\n    def backward(self, ts):\n        \"\"\"\n        Compute the backward pass, given the ground-truth, one-hot targets.\n\n        You may assume that the `forward()` method has been called for the\n        corresponding input `X`, so that the quantities computed in the\n        `forward()` method is accessible.\n\n        Parameters:\n            `ts` - A numpy array of shape (N, self.vocab_size)\n        \"\"\"\n        return do_backward_pass(self, ts)\n\n    def loss(self, ts):\n        \"\"\"\n        Compute the average cross-entropy loss, given the ground-truth, one-hot targets.\n\n        You may assume that the `forward()` method has been called for the\n        corresponding input `X`, so that the quantities computed in the\n        `forward()` method is accessible.\n\n        Parameters:\n            `ts` - A numpy array of shape (N, self.num_classes)\n        \"\"\"\n        return np.sum(-ts * np.log(self.y)) / ts.shape[0]\n\n    def update(self, alpha):\n        \"\"\"\n        Compute the gradient descent update for the parameters of this model.\n\n        Parameters:\n            `alpha` - A number representing the learning rate\n        \"\"\"\n        self.Ww = self.Ww - alpha * self.Ww_bar\n        self.W1 = self.W1 - alpha * self.W1_bar\n        self.b1 = self.b1 - alpha * self.b1_bar\n        self.W2 = self.W2 - alpha * self.W2_bar\n        self.b2 = self.b2 - alpha * self.b2_bar\n\n    def cleanup(self):\n        \"\"\"\n        Erase the values of the variables that we use in our computation.\n        \"\"\"\n        # To be filled in during the forward pass\n        self.N = None # Number of data points in the batch\n        self.xa = None # word (a)'s one-hot encoding\n        self.xb = None # word (b)'s one-hot encoding\n        self.xc = None # word (c)'s one-hot encoding\n        self.va = None # word (a)'s embedding\n        self.vb = None # word (b)'s embedding\n        self.vc = None # word (c)'s embedding\n        self.v = None  # concatenated embedding\n        self.m = None  # pre-activation hidden state\n        self.h = None  # post-activation hidden state\n        self.z = None  # prediction logit\n        self.y = None  # prediction softmax\n\n        # To be filled in during the backward pass\n        self.z_bar  = None # The error signal for self.z\n        self.W2_bar = None # The error signal for self.W2\n        self.b2_bar = None # The error signal for self.b2\n        self.h_bar  = None # The error signal for self.h\n        self.m_bar  = None # The error signal for self.z1\n        self.W1_bar = None # The error signal for self.W1\n        self.b1_bar = None # The error signal for self.b1\n        self.v_bar  = None # The error signal for self.v\n        self.va_bar = None # The error signal for self.va\n        self.vb_bar = None # The error signal for self.vb\n        self.vc_bar = None # The error signal for self.vc\n        self.Ww_bar = None # The error signal for self.Ww\n\nGraded Task: Complete the implementation of the do_forward_pass method, which computes the predictions given a NNModel and a batch of input data.\nWe recommend that you reason about your approach on paper before writing any numpy code. Track the shapes of your quantities carefully! When you finally write your numpy code, print out the shapes of your quantities as you go along, and reason about whether these shapes match your initial expectations.\n\ndef do_forward_pass(model, X):\n    \"\"\"\n    Compute the forward pass to produce prediction logits.\n\n    This function also keeps some of the intermediate values in\n    the neural network computation, to make computing gradients easier.\n\n    For the ReLU activation, you may find the function `np.maximum` helpful\n\n    Parameters:\n        `model` - An instance of the class NNModel\n        `X` - A numpy array of shape (N, model.vocab_size)\n\n    Returns: A numpy array of logit predictions of shape\n             (N, model.vocab_size)\n    \"\"\"\n    # populate the input attributes necessary for the\n    # backward pass\n    model.N = X.shape[0]\n    model.X = X\n\n    # for xa, xb, xc, we index the appropriate range of X\n    # (recall that the tensor X has shape [batch_size, 3*vocab_size])\n    model.xa = X[:, :model.vocab_size]\n    model.xb = X[:, model.vocab_size:model.vocab_size*2]\n    model.xc = X[:, model.vocab_size*2:]\n\n    # compute the embeddings\n    model.va = None # TODO\n    model.vb = None # TODO\n    model.vc = None # TODO\n    model.v = np.concatenate([model.va, model.vb, model.vc], axis=1)\n\n    # compute the remaining part of the forward pass\n    model.m = None # TODO - the hidden state value (pre-activation)\n    model.h = None # TODO - the hidden state value (post ReLU activation)\n    model.z = None # TODO - the logit scores (pre-activation)\n    model.y = None # TODO - the class probabilities (post-activation)\n    return model.z\n\nTask: One way important way to check your implementation is to run the forward() method to ensure that the shapes of your quantities are correct. Run the below code. If you run into shape mismatch issues, print out the shapes of the quantities that you are working with (e.g. print(model.va.shape)) and ensure that these shapes are what you expect them to be.\n\n# Create a batch of data that we will use for gradient checking\n# we will use a small batch size of 8. This number is chosen\n# because it is small, but also because this shape does not\n# appear elsewhere in our architecture (e.g. vocab size, num hidden)\n# so that shape mismatch issues are easier to identify.\nx_, t_ = get_batch(train4grams, 0, 8)\nmodel = NNModel()\ny = model.forward(x_)\n\n# TODO: Check that these shapes are correct. What should these shapes be?\nprint(model.va.shape, model.vb.shape, model.vc.shape)\nprint(model.v.shape)\nprint(model.m.shape, model.h.shape)\nprint(model.z.shape, model.y.shape)\n\nAt this point, we can work with a pre-trained model by loading weights that are provided to you via the link below. If you would like, you can jump to part 4 first and explore the interesting properties of this model before tackling backpropagation and model training.\n\n!wget https://www.cs.toronto.edu/~lczhang/413/sentence_pretrained.pk\n\n\ndef load_pretrained(model):\n    import pickle\n    assert(model.vocab_size == 250)\n    assert(model.emb_size   == 150)\n    assert(model.num_hidden == 100)\n    Ww, W1, b1, W2, b2 = pickle.load(open(\"sentence_pretrained.pk\", \"rb\"))\n    model.Ww = Ww\n    model.W1 = W1\n    model.b1 = b1\n    model.W2 = W2\n    model.b2 = b2\n    model.cleanup()\n    return model\n\nmodel = load_pretrained(NNModel())"
  },
  {
    "objectID": "labs/lab02.html#part-3.-model-building-backwards-pass",
    "href": "labs/lab02.html#part-3.-model-building-backwards-pass",
    "title": "CSC413 Lab 2: Word Embeddings",
    "section": "Part 3. Model Building: Backwards Pass",
    "text": "Part 3. Model Building: Backwards Pass\nWe are ready to complete the function that computes the backward pass of our model!\nYou should start by reviewing the lecture slides on backpropagation. One difference between the slides and our implementation here is that the slides express the required computations for computing the gradients of the loss for a single data point. However, our implementation of backpropagation is further vectorized to compute gradients of the loss for a batch consisting of multiple data points.\nWe begin with applying the backpropagation algorithm on our forward pass steps from earlier. Recall that our model’s forward pass is as follows:\n\\[\\begin{align*}\n\\bf{x_a} &= \\textrm{the one-hot vector for word 1} \\\\\n\\bf{x_b} &= \\textrm{the one-hot vector for word 2} \\\\\n\\bf{x_c} &= \\textrm{the one-hot vector for word 3} \\\\\n\\bf{v_a} &= \\bf{W}^{(word)} \\bf{x_a} \\\\\n\\bf{v_b} &= \\bf{W}^{(word)} \\bf{x_b} \\\\\n\\bf{v_c} &= \\bf{W}^{(word)} \\bf{x_c} \\\\\n\\bf{v} &= \\textrm{concatenate}(\\bf{v_a}, \\bf{v_b}, \\bf{v_c})\\\\\n\\bf{m} &= \\bf{W^{(1)}} \\bf{v} + \\bf{b^{(1)}} \\\\\n\\bf{h} &= \\textrm{ReLU}(\\bf{m}) \\\\\n\\bf{z} &= \\bf{W^{(2)}} \\bf{h} + \\bf{b^{(2)}} \\\\\n\\bf{y} &= \\textrm{softmax}(\\bf{z}) \\\\\nL &= \\mathcal{L}_\\textrm{Cross-Entropy}(\\bf{y}, \\bf{t}) \\\\\n\\end{align*}\\]\nFollowing the steps discussed in this week’s lecture, we should get the following backward-pass computation (verify this yourself!): \\[\\begin{align*}\n\\overline{{\\bf z}}  &= {\\bf y} - {\\bf t} \\\\\n\\overline{W^{(2)}}  &= \\overline{{\\bf z}}{\\bf h}^T \\\\\n\\overline{{\\bf b^{(2)}}}  &= \\overline{{\\bf z}} \\\\\n\\overline{{\\bf h}}  &= {W^{(2)}}^T\\overline{z} \\\\\n\\overline{W^{(1)}} &= \\overline{{\\bf m}} {\\bf v}^T \\\\\n\\overline{{\\bf b}^{(1)}} &= \\overline{{\\bf m}} \\\\\n\\overline{{\\bf m}}  &= \\overline{{\\bf h}}\\circ \\textrm{ReLU}'({\\bf m}) \\\\\n\\overline{{\\bf v}} &=  {W^{(1)}}^T \\overline{{\\bf m}} \\\\\n\\overline{{\\bf v_a}} &= \\dots \\\\\n\\overline{{\\bf v_b}} &= \\dots \\\\\n\\overline{{\\bf v_c}} &= \\dots \\\\\n\\overline{{\\bf W^{(word)}}} &= \\dots \\\\\n\\end{align*}\\]\nTask: What is the error signal \\(\\overline{{\\bf v_a}}\\)? How does this quantity relate to \\(\\overline{{\\bf v}}\\)? To answer this question, reason about the scalars that make up the elements of \\(\\overline{{\\bf v}}\\). Which of these scalars also appear in \\(\\overline{{\\bf v_a}}\\)?\nExpress your answer by computing va_bar (representing the quantity \\(\\overline{{\\bf v_a}}\\)) given v_bar (representing the quantity \\(\\overline{{\\bf v}}\\)).\n\nN = 10\nemb_size = 100\nv_bar = np.random.rand(N, emb_size * 3)\n\nva_bar = None # TODO\nvb_bar = None # TODO\nvc_bar = None # TODO\n\nTask: What is the derivative \\(\\overline{{\\bf W^{(word)}}}\\)? You may find it helpful to draw a computation graph, and then remember the multivariate chain rule. If \\(\\overline{{\\bf W^{(word)}}}\\) affects the loss in 3 different paths, what do we do with those 3 gradients?\n\n# TODO: Work out the derivative on paper.\n\nWe are still not done: the gradient computation is for a single input \\({\\bf x}\\). We will need to vectorize each of these computations so that they work for an entire batch of inputs \\({\\bf X}\\) of shape \\(N \\times 3\\textrm{vocab_size}\\).\nFor some quantities, vectorizing the backward-pass computation is just as straightforward as the forward-pass computation, requiring the same techniques. For example, each input \\({\\bf x}\\) in a batch will have its own corresponding value of \\({\\bf z}\\) and thus $. (If this sentence is confusing, check that your description of the shape for z_bar from Part 2 has the batch size N in there somewhere.)\nFor other quantities, vectorizing requires the use of the multivariate chain rule. For example, there is a single weight matrix \\(W^{(2)}\\), used for all inputs in a batch. Thus, a change in \\(W^{(2)}\\) will affect the predictions for all inputs. (If this sentence is confusing, check that your description of the shape for W2_bar from Part 2 does not have batch size N in there.)\nThe vectorization for the quantities consistent with those of a MLP is already provided to you in the do_backward_pass function. However, the rest of this function is incomplete.\nGraded Task: Complete the implementation of the do_backward_pass function, which performs backpropagation given a NNModel, given the ground-truth one-hot targets ts. This function assumes that the forward pass method had been called on the input X corresponding to those one-hot targets.\nOnce again, we recommend that you reason about your approach on paper before writing any numpy code! In particular, understand the vectorization strategies discussed in the previous weeks and above before proceeding. Track the shapes of your quantities carefully! When you finally write your numpy code, print out the shapes of your quantities as you go along, and reason about whether these shapes match your initial expectations.\n\ndef do_backward_pass(model, ts):\n    \"\"\"\n    Compute the backward pass, given the ground-truth, one-hot targets.\n\n    You may assume that `model.forward()` has been called for the\n    corresponding input `X`, so that the quantities computed in the\n    `forward()` method is accessible.\n\n    The member variables you store here will be used in the `update()`\n    method. Check that the shapes match what you wrote in Part 2.\n\n    Parameters:\n        `model` - An instance of the class NNModel\n        `ts` - A numpy array of shape (N, model.num_classes)\n    \"\"\"\n    # The gradient signal for the MLP part of this is given\n    # to you (or worked out together from above, TODO)\n    model.z_bar = (model.y - ts) / model.N\n    model.W2_bar = np.dot(model.h.T, model.z_bar)\n    model.b2_bar = np.dot(np.ones(model.N).T, model.z_bar)\n    model.h_bar = np.matmul(model.z_bar, model.W2.T)\n    model.m_bar = model.h_bar * (model.m &gt; 0)\n    model.W1_bar = np.dot(model.v.T, model.m_bar)\n    model.b1_bar = np.dot(np.ones(model.N).T, model.m_bar)\n    model.v_bar = np.matmul(model.m_bar, model.W1.T)\n\n    # Refer to your answer above\n    model.va_bar = None # TODO\n    model.vb_bar = None # TODO\n    model.vc_bar = None # TODO\n\n    # Refer to your answer above\n    model.Ww_bar = None\n\nAs we saw in CSC311, debugging machine learning code can be extremely challenging. It helps to be systematic about testing, and to test every helper function as we write it. It is important to test do_backward_pass before using it for training, so that we can isolates issues related to computing gradients vs. other training issues (e.g. those related to poor hyperparameter choices).\nTask: As in the forward pass, start by making sure that the shapes match. Again, If you run into shape mismatch issues, print out the shapes of the quantities that you are working with.\n\nx_, t_ = get_batch(train4grams, 0, 8)\nmodel = NNModel()\n\nmodel.forward(x_)\nmodel.backward(t_)\nmodel.update(0.001)\n\nThe above step checks that the shapes match. But we also saw, in CSC311, that one way to check the gradient computation is through finite difference. Recall the definition of a derivative. For a function \\(g(w): \\mathbb{R} \\rightarrow \\mathbb{R}\\),\n\\[g'(w) = \\lim_{h \\rightarrow 0} \\frac{g(w+h) - g(w)}{h}\\]\nThis above rule tells us that if we have a way to evaluate g and would like to test our implementation of \\(g'\\), we can choose an \\(h\\) small enough, and check if:\n\\[g'(w) \\approx \\frac{g(w+h) - g(w)}{h}\\]\nIn our case, we have that for any parameter \\(w_j\\) and an \\(h\\) small enough, we should have for our loss \\[\\mathcal{E}\\]:\n\\[\\frac{\\partial \\mathcal{E}}{\\partial w_j} \\approx \\frac{\\mathcal{E}(w_0, w_1, \\dots, w_{j-1}, w_j + h, w_{j+1}, \\dots, w_D) - \\mathcal{E}(w_0, w_1, \\dots, w_D)}{h}\\]\n(A word about notation: here we are enumerating over all scalar weights \\(w_0 \\dots w_D\\) in our model. You will often see this in machine learning textbooks and papers, where we ignore the fact that these scalar weights come from several different weight matrices and bias vectors. This notation might feel strange/imprecise as first, but keep in mind that mathematical notations is a form of language whose purpose is to communicate ideas. Practitioners choose different notations, and even introduce new notation, with the goal of clearly communicating a specific idea. Here, the idea is that we should be able to test the gradient computation or a single scalar weight by computing the loss function twice: once with a slight perturbation on that scalar weight.)\nGraded Task: Run the below code to spot test that the gradients Ww_bar is computed correctly. Include the output of the code in your submission.\n\n# We will opt to use a large batch size to test the gradients `Ww_bar`\n# with a large batch size. Why do you think this is? (Why might we\n# be more likely to have gradients of value 0 if the batch size is\n# small?)\nx_, t_ = get_batch(train4grams, 0, 800)\n\nmodel = NNModel()\nmodel.forward(x_)\n\n# Check the gradient for Ww_bar[3, 10].\n# You should spot check other indices too!\nmodel.backward(t_)\ngradient = model.Ww_bar[3, 10]\n\n# we should have\n# gradient ~= (loss_perturbed - loss_initial) / h\n# where loss_perturbed is the loss if we perturb\n# model.Ww_bar[3, 10] by a small value h\n\nloss_initial = model.loss(t_)\n\nh = 0.01\nmodel.Ww[3, 10] += h\n\nmodel.cleanup()\nmodel.forward(x_)\nloss_perturbed = model.loss(t_)\n\n# These two values should be close\nprint(gradient)\nprint((loss_perturbed - loss_initial) / h)\n\nIf gradient checking succeeds, we are ready to train our model. The function train_model is written for you. Run the code below with the default hyperparameters. Although hyperparameter tuning is an important step in machine learning, we have chosen reasonable hyperparameters to you to keep this lab a reasonable size.\n\ndef train_model(model,\n                train_data=train4grams,\n                validation_data=valid4grams,\n                batch_size=50,\n                learning_rate=0.3,\n                max_iters=20000,\n                plot_every=1000):\n    \"\"\"\n    Use gradient descent to train the numpy model on the dataset train4grams.\n    \"\"\"\n    iters, train_loss, train_acc, val_acc = [], [], [], [] # for the training curve\n    iter_count = 0  # count the number of iterations\n    try:\n        while iter_count &lt; max_iters:\n            # shuffle the training data, and break early if we don't have\n            # enough data to remaining in the batch\n            np.random.shuffle(train_data)\n            for i in range(0, train_data.shape[0], batch_size):\n                if (i + batch_size) &gt; train_data.shape[0]:\n                    break\n\n                # get the input and targets of a minibatch\n                xs, ts = get_batch(train_data, i, i + batch_size, onehot=True)\n\n                # erase any accumulated gradients\n                model.cleanup()\n\n                # forward pass: compute prediction\n                ys = model.forward(xs)\n\n                # backward pass: compute error\n                model.backward(ts)\n                model.update(learning_rate)\n\n                # increment the iteration count\n                iter_count += 1\n\n                # compute and plot the *validation* loss and accuracy\n                if (iter_count % plot_every == 0):\n                    iters.append(iter_count)\n                    train_loss.append(model.loss(ts))\n                    train_acc.append(estimate_accuracy(model, train_data))\n                    val_acc.append(estimate_accuracy(model, validation_data))\n                    model.cleanup()\n                    print(f\"Iter {iter_count}. Acc [val:{val_acc[-1]}, train:{train_acc[-1]}] Loss {train_loss[-1]}]\")\n            #   if iter_count &gt;= max_iters:\n            #       break\n    finally:\n        plt.figure()\n        plt.plot(iters[:len(train_loss)], train_loss)\n        plt.title(\"Loss over iterations\")\n        plt.xlabel(\"Iterations\")\n        plt.ylabel(\"Loss\")\n\n        plt.figure()\n        plt.plot(iters[:len(train_acc)], train_acc)\n        plt.plot(iters[:len(val_acc)], val_acc)\n        plt.title(\"Accuracy over iterations\")\n        plt.xlabel(\"Iterations\")\n        plt.ylabel(\"Loss\")\n        plt.legend([\"Train\", \"Validation\"])\n\nmodel= NNModel()\ntrain_model(model)"
  },
  {
    "objectID": "labs/lab02.html#part-4.-applying-the-model",
    "href": "labs/lab02.html#part-4.-applying-the-model",
    "title": "CSC413 Lab 2: Word Embeddings",
    "section": "Part 4. Applying the Model",
    "text": "Part 4. Applying the Model\nIn this section, we will use apply the model for sentence completion, and to explore model embeddings. If you do not have a trained model, you may use the trained weights provided as part of the assignment.\n\n# model = load_pretrained(NNModel())\n\nTask: The function make_prediction has been written for you. It takes as parameters a NNModel model and sentence (a list of words), and produces a prediction for the next word in the sentence.\nRun the following code to predict what the next word should be in each of the following sentences:\n\ndef make_prediction(model, sentence):\n    \"\"\"\n    Use the model to make a prediction for the next word in the\n    sentence using the last 3 words (sentence[-3:])\n    \"\"\"\n    global vocab_itos\n    indices = convert_words_to_indices([sentence[-3:]])\n    X = make_onehot(indices).reshape(-1, 750)\n    z = model.forward(X)\n    i = np.argmax(z)\n    return vocab_itos[i]\n\nprint(make_prediction(model, ['you', 'are', 'a']))\nprint(make_prediction(model, ['there', 'are', 'no']))\nprint(make_prediction(model, ['yesterday', 'the', 'federal']))\n\nDo your predictions make sense? (If all of your predictions are the same, train your model for more iterations, or change the hyper parameters in your model.\n\n# TODO: Your analysis goes here\n\nWhile training the NNModel, we trained the weight model.Ww, which takes a one-hot representation of a word in our vocabulary, and returns a low-dimensional vector representation of that word. These representations, also called word embeddings have interesting properties.\nGraded Task: Explain why each row of model.Ww contains the vector representing of a word. For example model.Ww[vocab_stoi[\"any\"],:] contains the vector representation of the word “any”.\n\n# TODO: Write your explanation here\n\nOne interesting thing about these word embeddings is that distances in these vector representations of words make some sense! To show this, we have provided code below that computes the cosine similarity of every pair of words in our vocabulary.\n\nnorms = np.linalg.norm(model.Ww, axis=1)\nword_emb_norm = (model.Ww.T / norms).T\nsimilarities = np.matmul(word_emb_norm, word_emb_norm.T)\n\n# Some example distances. The first one should be larger than the second\nprint(similarities[vocab_stoi['any'], vocab_stoi['many']])\nprint(similarities[vocab_stoi['any'], vocab_stoi['government']])\n\nTask: Run the below code, which computes the 5 closest words to each of the following words. Replace these words with words of your choice to explore the distances in the word embeddings.\n\ndef get_closest(word):\n    dst = [(w, similarities[vocab_stoi[word], idx])\n           for w, idx in vocab_stoi.items()]\n    dst = sorted(dst, key=lambda x: x[1], reverse=True)\n    return dst[1:6]\n\nprint(get_closest(\"four\"))\nprint(get_closest(\"go\"))\nprint(get_closest(\"should\"))\nprint(get_closest(\"yesterday\"))\n\nNotice that similar words provided above tend to occur in similar surrounding words in a sentence. Why do you think this might be? Consider the architecture used in this model, and what this model is trained to do. (How would replacing a word with another word with a similar embedding change the neural network prediction?)\nWe can also visualize the word embeddings by reducing the dimensionality of the word vectors to 2D. There are many dimensionality reduction techniques that we could use, and we will use an algorithm called t-SNE. (You don’t need to know what this is for the lab). Nearby points in this 2-D space are meant to correspond to nearby points in the original, high-dimensional space.\nThe following code runs the t-SNE algorithm and plots the result. Look at the plot and find two clusters of related words. What do the words in each cluster have in common?\nNote that there is randomness in the initialization of the t-SNE algorithm. If you re-run this code, you may get a different image.\n\nimport sklearn.manifold\ntsne = sklearn.manifold.TSNE()\nY = tsne.fit_transform(word_emb_norm)\n\nplt.figure(figsize=(10, 10))\nplt.xlim(Y[:,0].min(), Y[:, 0].max())\nplt.ylim(Y[:,1].min(), Y[:, 1].max())\nfor i, w in enumerate(vocab):\n    plt.text(Y[i, 0], Y[i, 1], w)\nplt.show()"
  },
  {
    "objectID": "labs/lab10.html",
    "href": "labs/lab10.html",
    "title": "CSC413 Lab 8: Text Classification using RNNs",
    "section": "",
    "text": "Sentiment Analysis is the problem of identifying the writer’s sentiment given a piece of text. Sentiment Analysis can be applied to movie reviews, feedback of other forms, emails, tweets, course evaluations, and much more.\nIn this lab, we will build an RNN to classify positive vs negative tweets We use the Sentiment140 data set, which contains tweets with either a positive or negative emoticon. Our goal is to determine whether which type of emoticon the tweet (with the emoticon removed) contained. The dataset was actually collected by a group of students, much like you, who are doing their first machine learning projects.\nBy the end of this lab, you will be able to:\nAcknowledgements:\nPlease work in groups of 1-2 during the lab."
  },
  {
    "objectID": "labs/lab10.html#submission",
    "href": "labs/lab10.html#submission",
    "title": "CSC413 Lab 8: Text Classification using RNNs",
    "section": "Submission",
    "text": "Submission\nIf you are working with a partner, start by creating a group on Markus. If you are working alone, click “Working Alone”.\nSubmit the ipynb file lab10.ipynb on Markus containing all your solutions to the Graded Tasks. Your notebook file must contain your code and outputs where applicable, including printed lines and images. Your TA will not run your code for the purpose of grading.\nFor this lab, you should submit the following:\n\nPart 1. Your output showing several positive tweets. (1 point)\nPart 2. Your explanation of the shapes of wordemb. (1 point)\nPart 2. Your explanation of the shapes of h and out. (2 points)\nPart 2. Your explanation of why computing the mean and max of hidden states across all time steps is likely more informative than using the final output state. (1 point)\nPart 3. Your demonstration of the model’s ability to “overfit” on a data set. (1 point)\nPart 3. Your output from training the model on the full data set. (1 point)\nPart 4. Your explanation of why MyGloveRNN requires fewer iteration to obtain “good” accuracy. (1 point)\nPart 4. Your comparison of MyGloveRNN and MyRNN in low data settings.. (1 point)\nPart 4. Your explanation of where the biases in embeddings come from, and whether our model will have the same sorts of baises.. (1 point)"
  },
  {
    "objectID": "labs/lab10.html#part-1.-data",
    "href": "labs/lab10.html#part-1.-data",
    "title": "CSC413 Lab 8: Text Classification using RNNs",
    "section": "Part 1. Data",
    "text": "Part 1. Data\nStart by running these two lines of code to download the data on to Google Colab.\n\n# Download tutorial data files.\n!wget https://www.cs.toronto.edu/~lczhang/413/sample_tweets.csv\n\nAs always, we start by understanding what our data looks like. Notice that the test set has been set aside for us. Both the training and test set files follow the same format. Each line in the csv file contains the tweet text, the string label “4” (positive) or “0” (negative), and some additional information about the tweet.\n\nimport csv\ndatafile = \"sample_tweets.csv\"\n\n# Training/Validation set\ndata = csv.reader(open(datafile))\nfor i, line in enumerate(data):\n    print(line)\n    if i &gt; 10:\n        break\n\nTask: How many positive and negative tweets are in this file?\n\n# TODO\nprint(Counter(x[0] for x in csv.reader(open(datafile))))\n\nGraded Task: We have printed several negative tweets above. Print 10 positive tweets.\n\n# TODO: Please make sure to include both your code and the\n# printed output\n\nWe will now split the dataset into training, validation, and test sets:\n\n# read the data; convert labels into integers\ndata = [(review, int(label=='4'))  # label 1 = positive, 0 = negative\n        for label, _, _, _, _, review in csv.reader(open(datafile))]\n\n# shuffle the data, since the file stores all negative tweets first\nimport random\nrandom.seed(42)\nrandom.shuffle(data)\n\ntrain_data = data[:50000] \nval_data = data[50000:60000] \ntest_data = data[60000:]\n\nIn order to be able to use neural networks to make predictions about these tweets, we need to begin by convert these tweets into sequences of numbers, each representing a words. This is akin to a one-hot encoding: each word will be converted into an a number representing the unique index of that word.\nAlthough we could do this conversion by writing our own python code, torch has a package called torchtext that has utilities useful for text classification and generation tasks. In particular, the Vocab class and build_vocab_from_iterator will be useful for us for building the mapping from words to indices.\n\nimport torchtext\n\nfrom torchtext.data.utils import get_tokenizer\nfrom torchtext.vocab import Vocab, build_vocab_from_iterator\n\n# we will *tokenize* each word by using a tokenzier from \n# https://pytorch.org/text/stable/data_utils.html#get-tokenizer\n\ntokenizer = get_tokenizer(\"basic_english\")\ntrain_data_words = [tokenizer(x) for x, t in train_data]\n\n# build the vocabulary object. the parameters to this function\n# is described below\nvocab = build_vocab_from_iterator(train_data_words,\n                                  specials=['&lt;bos&gt;', '&lt;eos&gt;', '&lt;unk&gt;', '&lt;pad&gt;'],\n                                  min_freq=10)\n\n# set the index of a word not in the vocabulary\nvocab.set_default_index(2) # this is the index of the `&lt;unk&gt;` keyword\n\nNow, vocab is an object of class Vocab (see more here https://pytorch.org/text/stable/vocab.html ) that provides functionalities for converting words into their indices. In addition to words appearing in the training set, ther are four special tokens that we use, akin to placeholder words:\n\n&lt;bos&gt;, to indicate the beginning of a sequence.\n&lt;eos&gt;, to indicate the end of a sequence.\n&lt;unk&gt;, to indicate a word that is not in the vocabulary. This includes words that appear too infrequently to be included in the vocabulary, and any other words in the validation/test sets that are not see in training.\n&lt;pad&gt;, used for padding shorter sequences in a batch: since each tweet may have different length, the shorter tweets in each batch will be padded with the &lt;pad&gt; token so that each sequence (tweet) in a batch has the same length.\n\nThe min_freq parameter identifies the minimum number of times a word must appear in the training set in order to be included in the vocabulary.\nHere you can see the vocab object in action:\n\n# Print the number of words in the vocabulary\nprint(len(vocab))\n\n# Convert a tweet into a sequence of word indices.\ntweet = 'The movie Pneumonoultramicroscopicsilicovolcanoconiosis is a good movie, it is very funny'\ntokens = tokenizer(f'&lt;bos&gt; {tweet} &lt;eos&gt;')\nprint(tokens)\nindices = vocab.forward(tokens)\nprint(indices)\n\nTask: What is the index of the &lt;pad&gt; token?\n\n# TODO: write code to identify the index of the `&lt;pad&gt;` token\n\nNow let’s apply this transformation to the entire set of training, validation, and test data.\n\n\ndef convert_indices(data, vocab):\n    \"\"\"Convert data of form [(tweet, label)...] where tweet is a string\n    into an equivalent list, but where the tweets represented as a list\n    of word indices.\n    \"\"\"\n    return [(vocab.forward(tokenizer(f'&lt;bos&gt; {text} &lt;eos&gt;')), label)\n            for (text, label) in data]\n\ntrain_data_indices = convert_indices(train_data, vocab)\nval_data_indices = convert_indices(val_data, vocab)\ntest_data_indices = convert_indices(test_data, vocab)\n\nWe have seen that PyTorch’s DataLoader provides an easy way to form minibatches when we worked with image data. However, text and sequence data is more challenging to work with since the sequences may not be the same length.\nAlthough we can (and will!) continue to use DataLoader for our text data, we need to provide a function that merges sequences of various lengths into two PyTorch tensors correspondingg to the inputs and targets for that batch.\nTask: Following the instructions below, complete the collate_batch function, which creates the input and target tensors for a batch of data.\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\n\ndef collate_batch(batch):\n    \"\"\"\n    Returns the input and target tensors for a batch of data\n\n    Parameters:\n        `batch` - An iterable data structure of tuples (indices, label),\n                  where `indices` is a sequence of word indices, and \n                  `label` is either 1 or 0.\n\n    Returns: a tuple `(X, t)`, where \n        - `X` is a PyTorch tensor of shape (batch_size, sequence_length)\n        - `t` is a PyTorch tensor of shape (batch_size)\n    where `sequence_length` is the length of the longest sequence in the batch\n    \"\"\"\n\n    text_list = []  # collect each sample's sequence of word indices\n    label_list = [] # collect each sample's target labels\n    for (text_indices, label) in batch:\n        text_list.append(torch.tensor(text_indices))\n        # TODO: what do we need to do with `label`?\n\n    X = pad_sequence(text_list, padding_value=3).transpose(0, 1)\n    t = None # TODO\n    return X, t\n\n\ntrain_dataloader = DataLoader(train_data_indices, batch_size=10, shuffle=True,\n                              collate_fn=collate_batch)\n\nWith the above code in mind, we should be able to extract batches from train_dataloader. Notice that X.shape is different in each batch. You should also see that the index 3 is used to pad shorter sequences in in a batch.\n\nfor i, (X, t) in enumerate(train_dataloader):\n    print(X.shape, t.shape)\n    if i &gt;= 10:\n        break\n\nprint(X)\n\nTask: Why does each sequence begin with the token 0, and end with the token 1 (ignoring the paddings).\n\n# TODO: Your explanation goes here"
  },
  {
    "objectID": "labs/lab10.html#part-2.-model",
    "href": "labs/lab10.html#part-2.-model",
    "title": "CSC413 Lab 8: Text Classification using RNNs",
    "section": "Part 2. Model",
    "text": "Part 2. Model\nWe will use a recurrent neural network model to classify positive vs negative sentiments. Our RNN model will have three components that are typical in a sequence classification model:\n\nAn embedding layer, which will map each word index (akin to a one-hot embedding) into a low-dimensional vector. This layer as having the same functionality as the weights \\(W^{(word)}\\) from lab 2.\nA recurrent layer, which performs the recurrent neural network computation. The input to this layer is the low-dimensional embedding vectors for each word in the sequence.\nA fully connected layer, which computes the final binary classification using features computed from the recurrent layer. In our case, we concatenate the max and mean of the hidden units across the time steps (i.e. across each word).\n\nLet’s define the model that we will use, and then explore it step by step.\n\nimport torch.nn as nn\n\nclass MyRNN(nn.Module):\n    def __init__(self, vocab_size, emb_size, hidden_size, num_classes):\n        super(MyRNN, self).__init__()\n        self.vocab_size = vocab_size\n        self.emb_size = emb_size\n        self.hidden_size = hidden_size\n        self.num_classes = num_classes\n        self.emb = nn.Embedding(vocab_size, emb_size)\n        self.rnn = nn.RNN(emb_size, hidden_size, batch_first=True)\n        self.fc = nn.Linear(hidden_size * 2, num_classes)\n\n    def forward(self, X):\n        # Look up the embedding\n        wordemb = self.emb(X)\n        # Forward propagate the RNN\n        h, out = self.rnn(wordemb)\n        # combine the hidden features computed from *each* time step of\n        # the RNN. we do this by \n        features = torch.cat([torch.amax(h, dim=1),\n                              torch.mean(h, dim=1)], axis=-1)\n        # Compute the final prediction\n        z = self.fc(features)\n        return z\n\nmodel = MyRNN(len(vocab), 128, 64, 2)\n\nTo explore exactly what this model is doing, let’s grab one batch of data from the data loader we created. We will observe, step-by-step, what computation will be performed on the input X to obtain the final prediction. We do this by emulating the forward method of the MyRNN function.\n\nX, t = next(iter(train_dataloader))\n\nprint(X.shape)\n\nGraded Task: Run the code below to check the shape of wordemb. What shape does this tensor have? Explain what each dimension in this shape means.\n\nwordemb = model.emb(X)\n\nprint(wordemb.shape)\n\n# TODO: Include your explanation here\n\nGraded Task: Run the code below, which computes the RNN forward pass, with wordemb as input. What shape do the tensors h and out have? Explain what these tensors correspond to. (See the RNN reference https://pytorch.org/docs/stable/generated/torch.nn.RNN.html on the PyTorch documentation page.)\n\nh, out = model.rnn(wordemb)\n\nprint(h.shape)\nprint(out.shape)\n\n# The tensors `h` and `out` are related. To see the relation,\n# choose an index in the batch and compare the following two\n# vectors in `h` and `out`.\nindex = 2 # choose an index to iterate through the batch\nprint(h[index, -1, :])\nprint(out[0, index, :])\n\n# TODO: Include your explanation here\n\nGraded Task: There is a step in the MyRNN forward pass that combines the features from each time step of the RNN by computing:\n\nthe maximum value of each position in the hidden vector.\nthe mean value of each position in the hidden vector.\nconcatenating the resulting two vectors.\n\n(Note that in the demo below, we are working with a minibatch. Thus, each of out1, out2, and features below are matrices containing the vectors from each minibatch)\nThis method typically performs better than, say, taking the hidden state at the last time step (the value out from above). Explain, intuitively, why you might expect this performance to be the case for a sentiment analysis task.\n\nout1 = torch.amax(h, dim=1)\nout2 = torch.mean(h, dim=1)\nfeatures = torch.cat([out1, out2], axis=-1)\n\n# Compare, for a single input in the batch, the connection between\n# `h`, `out1`, `out2` and `features`:\nprint(h[index, :, :])\nprint(out1[index, :])\nprint(out2[index, :])\nprint(features[index, :])\n\n# TODO: Include your explanation here\n\nTask: Finally, the model uses the features tensor to compute the prediction for each element in the batch. Run the code below to complete this step.\n\nprint(model.fc(features))\n\nThere is one more thing we need to do before training the model, which is to write a function to estimate the accuracy of the model. This is done for you below.\n\ndef accuracy(model, dataset, max=1000):\n    \"\"\"\n    Estimate the accuracy of `model` over the `dataset`.\n    We will take the **most probable class**\n    as the class predicted by the model.\n\n    Parameters:\n        `model`   - An object of class nn.Module\n        `dataset` - A dataset of the same type as `train_data`.\n        `max`     - The max number of samples to use to estimate \n                    model accuracy\n\n    Returns: a floating-point value between 0 and 1.\n    \"\"\"\n\n    correct, total = 0, 0\n    dataloader = DataLoader(dataset,\n                            batch_size=1,  # use batch size 1 to prevent padding\n                            collate_fn=collate_batch)\n    for i, (x, t) in enumerate(dataloader):\n        z = model(x)\n        y = torch.argmax(z, axis=1)\n        correct += int(torch.sum(t == y))\n        total   += 1\n        if i &gt;= max:\n            break\n    return correct / total\n\naccuracy(model, train_data_indices) # should be close to half"
  },
  {
    "objectID": "labs/lab10.html#part-3.-training",
    "href": "labs/lab10.html#part-3.-training",
    "title": "CSC413 Lab 8: Text Classification using RNNs",
    "section": "Part 3. Training",
    "text": "Part 3. Training\nIn this section, we will train the MyRNN model to classify tweets. As the models that we are building begin to increase in complexity, it is important to use good debugging techniques. In this section, we will introduce the technique of checking whether the model and training code is able to overfit on a small training set. This is a way to check for bugs in the implementation.\nTask: Complete the training code below\n\nimport torch.optim as optim \nimport matplotlib.pyplot as plt\n\ndef train_model(model,                # an instance of MLPModel\n                train_data,           # training data\n                val_data,             # validation data\n                learning_rate=0.001,\n                batch_size=100,\n                num_epochs=10,\n                plot_every=50,        # how often (in # iterations) to track metrics\n                plot=True):           # whether to plot the training curve\n    train_loader = torch.utils.data.DataLoader(train_data,\n                                               batch_size=batch_size,\n                                               collate_fn=collate_batch,\n                                               shuffle=True) # reshuffle minibatches every epoch\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n    # these lists will be used to track the training progress\n    # and to plot the training curve\n    iters, train_loss, train_acc, val_acc = [], [], [], []\n    iter_count = 0 # count the number of iterations that has passed\n\n    try:\n        for e in range(num_epochs):\n            for i, (texts, labels) in enumerate(train_loader):\n                z = None # TODO\n\n                loss = None # TODO\n\n                loss.backward() # propagate the gradients\n                optimizer.step() # update the parameters\n                optimizer.zero_grad() # clean up accumualted gradients\n\n                iter_count += 1\n                if iter_count % plot_every == 0:\n                    iters.append(iter_count)\n                    ta = accuracy(model, train_data)\n                    va = accuracy(model, val_data)\n                    train_loss.append(float(loss))\n                    train_acc.append(ta)\n                    val_acc.append(va)\n                    print(iter_count, \"Loss:\", float(loss), \"Train Acc:\", ta, \"Val Acc:\", va)\n    finally:\n        # This try/finally block is to display the training curve\n        # even if training is interrupted\n        if plot:\n            plt.figure()\n            plt.plot(iters[:len(train_loss)], train_loss)\n            plt.title(\"Loss over iterations\")\n            plt.xlabel(\"Iterations\")\n            plt.ylabel(\"Loss\")\n\n            plt.figure()\n            plt.plot(iters[:len(train_acc)], train_acc)\n            plt.plot(iters[:len(val_acc)], val_acc)\n            plt.title(\"Accuracy over iterations\")\n            plt.xlabel(\"Iterations\")\n            plt.ylabel(\"Loss\")\n            plt.legend([\"Train\", \"Validation\"])\n\nGraded Task: As a way to check the model and training code, check if your model can obtain a 100% training accuracy relatively quickly (e.g. within less than a minute of training time), when training on only the first 20 element of the training data.\n\nmodel = MyRNN(vocab_size=len(vocab),\n              emb_size=300,\n              hidden_size=64,\n              num_classes=2)\n# TODO: Include your code and output \n\nTask: Will this model that you trained above have a high accuracy over the validation set? Explain why or why not.\n\n# TODO: Your explanation goes here\n\nGraded Task: Train your model on the full data set. What validation accuracy can you achieve?\n\n# TODO: Include your code here. Try a few hyperparameter choices until you\n# are satisfied that your model performance is reasonable (i.e. no obviously\n# poor hyperparameter choices)\n\nInstead of a (vanilla) RNN model, PyTorch also makes available nn.LSTM and nn.GRU units. They can be used in place of nn.RNN without further changes to the MyRNN code.\nIn general, gated units like LSTM’s are much more frequently used than vanilla RNNs, although transformers are much more popular now as well."
  },
  {
    "objectID": "labs/lab10.html#part-4.-pretrained-embeddings",
    "href": "labs/lab10.html#part-4.-pretrained-embeddings",
    "title": "CSC413 Lab 8: Text Classification using RNNs",
    "section": "Part 4. Pretrained Embeddings",
    "text": "Part 4. Pretrained Embeddings\nAs we saw in the previous lab on images, transfer learning is a useful technique in practical machine learning, especially in low-data settings: instead of training an entire neural network from scratch, we use (part of) a model that is pretrained on large amounts of similar data. We use the intermediate state of this pretrained model as features to our model—i.e. we use the pretrained models to compute features.\nJust like with images, using a pretrained model is an important strategy for working with text. Large language models is an excellent demonstration of how generalizable pretrained features can be.\nIn this part of the lab, we will use a slightly older idea of using pretrained word embeddings. In particular, instead of training our own nn.Embedding layer, we will use GloVe embeddings (2014) https://nlp.stanford.edu/projects/glove/ trained on a large data set containing all of Wikipedia and other webpages.\nNowadays, large language model (LLMs), including those with APIs provided by various organizations, can also be used to map words/sentences into embeddings. However, the basic idea of using pretrained models in low-data settings remains similar. We will also identify some bias issues with pretrained word embeddings. There is evidence that these types of bias issues continues to persist in LLMs as well.\n\nfrom torchtext.vocab import GloVe\n\nglove = torchtext.vocab.GloVe(name=\"6B\", dim=300)\n\nTask: Run the below code to print the GloVe word embedding for the word “cat”.\n\nprint(glove['cat'])\n\nUnfortunately, it is not straightforward to add the &lt;pad&gt;, &lt;unk&gt;, &lt;bos&gt; and &lt;eos&gt; tokens. So we will do without them.\nTask: Run the below code to look up GloVe word indices for the training, validation, and test sets.\n\ndef convert_indices_glove(data, default=len(glove)-1):\n    result = []\n    for text, label in data:\n        words = tokenizer(text) # for simplicity, we wont use &lt;bos&gt; and &lt;eos&gt;\n        indices = []\n        for w in words:\n            if w in glove.stoi:\n                indices.append(glove.stoi[w])\n            else:\n                # this is a bit of a hack, but we will repurpose *last* word\n                # (least common word) appearing in the GloVe vocabluary as our\n                # '&lt;pad&gt;' token\n                indices.append(default)\n        result.append((indices, label),)\n    return result\n\ntrain_data_glove = convert_indices_glove(train_data)\nval_data_glove = convert_indices_glove(val_data)\ntest_data_glove = convert_indices_glove(test_data)\n\nNow, we will modify the MyRNN to use the pretrained GloVe vectors:\n\nclass MyGloveRNN(nn.Module):\n    def __init__(self,  hidden_size, num_classes):\n        super(MyGloveRNN, self).__init__()\n        self.vocab_size, self.emb_size = glove.vectors.shape\n        self.hidden_size = hidden_size\n        self.num_classes = num_classes\n        self.emb = nn.Embedding.from_pretrained(glove.vectors)\n        self.emb.requires_grad=False # do *not* update the glove embeddings\n        self.rnn = nn.RNN(self.emb_size, hidden_size, batch_first=True)\n        self.fc = nn.Linear(hidden_size * 2, num_classes)\n\n    def forward(self, X):\n        # Look up the embedding\n        wordemb = self.emb(X)\n        # Forward propagate the RNN\n        h, out = self.rnn(wordemb)\n        # combine the hidden features computed from *each* time step of\n        # the RNN. we do this by \n        features = torch.cat([torch.amax(h, dim=1),\n                              torch.mean(h, dim=1)], axis=-1)\n        # Compute the final prediction\n        z = self.fc(features)\n        return z\n\n    def parameters(self):\n        # do not return the parameters of self.emb \n        # so the optimizer will not update the parameters of self.emb\n        return (p for p in super(MyGloveRNN, self).parameters() if p.requires_grad)\n\n\nmodel = MyGloveRNN(64, 2)\n\nTask Train this model. Use comparable hyperparameters so that you can compare your result against MyRNN.\n\n# TODO: Train your model here, and include the output\n\nGraded Task: You might notice that a very smaller number of iterations will be required to train this model to a reasonable performance (e.g. &gt;70% validation accuracy). Why might this be?\n\n# TODO: Include your explanation here\n\nGraded Task: Train both MyGloveRNN and MyRNN models using the corresponding embeddings (pretrained vs. not), but only with the first 200 data points in the training set. How do the validation accuracies compare between these two models?\n\n# TODO: Training code for MyGloveRNN.\n# Include outputs and training curves in your submission\n\n\n# TODO: Training code for MyRNN\n# Include outputs and training curves in your submission\n\n\n# TODO: Compare the validation accuaries here\n\nMachine learning models have an air of “fairness” about them, since models make decisions without human intervention. However, models can and do learn whatever bias is present in the training data. GloVe vectors seems innocuous enough: they are just representations of words in some embedding space. Even so, we will show that the structure of the GloVe vectors encodes the everyday biases present in the texts that they are trained on.\nWe start with an example analogy to demonstrate the power of GloVe embeddings that allows us to complete analogies by applying arithmetic operations to the word vectors.\n\\[doctor - man + woman \\approx ??\\]\nTo find the answers to the above analogy, we will compute the following vector, and then find the word whose vector representation is closest to it.\n\nv = glove['doctor'] - glove['man'] + glove['woman']\n\nTask: Run the code below to find the closets word. You should see the word “nurse” fairly high up in that list.\n\ndef print_closest_words(vec, n=5):\n    dists = torch.norm(glove.vectors - vec, dim=1)     # compute distances to all words\n    lst = sorted(enumerate(dists.numpy()), key=lambda x: x[1]) # sort by distance\n    for idx, difference in lst[1:n+1]:                         # take the top n\n        print(glove.itos[idx], difference)\n\nprint_closest_words(v)\n\nTask: To compare, use a similar method to find the answer to this analogy: \\[doctor - woman + man \\approx ??\\]\nIn other words, we go the opposite direction in the “gender” axis to check if similarly concerning analogies exist.\n\nprint_closest_words(glove['doctor'] - glove['woman'] + glove['man'])\n\nTask: Compare the following two outputs.\n\nprint_closest_words(glove['programmer'] - glove['man'] + glove['woman'])\n\n\nprint_closest_words(glove['programmer'] - glove['woman'] + glove['man'])\n\nTask: Compare the following two outputs.\n\nprint_closest_words(glove['professor'] - glove['man'] + glove['woman'])\n\n\nprint_closest_words(glove['professor'] - glove['woman'] + glove['man'])\n\nTask: Compare the following two outputs.\n\nprint_closest_words(glove['engineer'] - glove['man'] + glove['woman'])\n\n\nprint_closest_words(glove['engineer'] - glove['woman'] + glove['man'])\n\nGraded Task: Explain where the bias in these embeddings come from. Would you expect our word embeddings (trained on tweets) to be similarly problematic? Why or why not?\n\n# TODO: Your explanation goes here"
  },
  {
    "objectID": "labs/neural_network_optimization.html",
    "href": "labs/neural_network_optimization.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "This notebook demonstrates the implementation of a three-layer neural network and compares the performance of different optimization methods: Gradient Descent (GD), Momentum, and Adam. The neural network is trained on a two-moon-shaped dataset using various optimization techniques to find the optimal parameters that minimize the cost function and improve the accuracy of the model.\n\n\nThe dataset used in this notebook is generated using the make_moons() function from the sklearn.datasets module. The dataset consists of two classes that form two moon shapes. It is a non-linearly separable dataset, making it a suitable scenario for testing different optimization methods for training a neural network.\n\n\n\nThe neural network uses two activation functions:\n\nReLU (Rectified Linear Unit): The ReLU activation function is used in the hidden layers of the neural network. It allows the model to handle non-linearities efficiently and avoids the vanishing gradient problem, which can occur with sigmoid activation.\nSigmoid: The sigmoid activation function is used in the output layer to compute the final probability of the binary classification task. It maps the input to the range (0, 1), representing the probability of the input belonging to class 1.\n\n\n\n\nThe neural network architecture is defined as a three-layer model with the following layer dimensions: [input size, 5, 2, 1]. The input size corresponds to the features of the dataset, and the output size is set to 1 for binary classification.\n\n\n\n\n\nGradient Descent is a first-order optimization algorithm that updates the model’s parameters in the opposite direction of the gradient of the cost function. The magnitude of the update is controlled by the learning rate. While GD is a simple and intuitive optimization method, it may suffer from slow convergence, especially for large datasets or in cases where the cost function has high curvatures.\n\n\n\nMomentum is an extension of GD that introduces a moving average of the gradients to accelerate convergence. It accumulates the past gradients’ information to continue moving in the same direction even when the gradients change direction frequently. This helps in faster convergence and reduces oscillations in the cost function. The momentum hyperparameter controls the influence of the past gradients.\n\n\n\nAdam is a popular optimization algorithm that combines the ideas of both Momentum and RMSprop. It uses moving averages of the past gradients and squared gradients to adapt the learning rate for each parameter. The algorithm automatically adjusts the learning rate based on the history of the gradients and their magnitudes. This adaptive learning rate makes Adam robust and efficient in practice, requiring minimal hyperparameter tuning.\n\n\n\n\nThe neural network is trained using each of the three optimization methods. For each optimization method, the model’s parameters are updated iteratively over a specified number of epochs using mini-batch gradient descent. At the end of each epoch, the cost function is computed and printed to monitor the training progress. The final trained model’s accuracy on the training data is also calculated and displayed.\n\n\n\nThe notebook provides an insight into the effectiveness of different optimization methods in training neural networks. By comparing the performance of GD, Momentum, and Adam, we can gain a better understanding of how these algorithms handle the optimization process and improve the neural network’s convergence and accuracy on non-linear datasets like the two-moon-shaped dataset used here.\nSure, let’s delve into the details and theory of each optimization method along with their advantages and disadvantages.\n\n\n\nTheory: Gradient Descent is a first-order optimization algorithm used to minimize the cost function of a neural network. It works by iteratively updating the model’s parameters in the opposite direction of the gradient of the cost function with respect to those parameters. The gradient points in the direction of steepest ascent, so taking the opposite direction allows the algorithm to move towards the minimum of the cost function.\nAlgorithm: 1. Initialize the model’s parameters randomly. 2. Compute the gradient of the cost function with respect to each parameter. 3. Update each parameter using the formula: parameter = parameter - learning_rate * gradient, where the learning_rate controls the size of the update step.\nAdvantages: - Simple and easy to implement. - Can be applied to large datasets since it processes one data point at a time. - Can handle non-convex cost functions.\nDisadvantages: - Convergence can be slow, especially for large datasets or complex cost functions. - Sensitive to the learning rate choice; a large learning rate may lead to overshooting the minimum, while a small learning rate may result in slow convergence.\n\n\n\nTheory: Momentum is an extension of GD that aims to accelerate the convergence of the optimization process. It introduces a moving average of the past gradients to continue moving in the same direction even when the gradients change direction frequently. This helps to overcome oscillations in the cost function and speeds up convergence.\nAlgorithm: 1. Initialize the model’s parameters and the velocity (initialized as zeros) for each parameter. 2. Compute the gradient of the cost function with respect to each parameter. 3. Update each parameter using the formula: velocity = beta * velocity + (1 - beta) * gradient    parameter = parameter - learning_rate * velocity where beta is the momentum hyperparameter.\nAdvantages: - Accelerates convergence, especially in areas with high curvature or noisy gradients. - Reduces oscillations and overshooting, leading to more stable updates.\nDisadvantages: - Momentum may accumulate too much velocity in flat regions, making it harder to escape local minima. - May overshoot and oscillate when the learning rate is too large.\n\n\n\nTheory: Adam is an adaptive learning rate optimization algorithm that combines the ideas of both Momentum and RMSprop. It uses moving averages of the past gradients and squared gradients to adapt the learning rate for each parameter. The algorithm automatically adjusts the learning rate based on the history of the gradients and their magnitudes.\nAlgorithm: 1. Initialize the model’s parameters and the first and second moment estimates (initialized as zeros) for each parameter. 2. Compute the gradient of the cost function with respect to each parameter. 3. Update each parameter using the formula: first_moment = beta1 * first_moment + (1 - beta1) * gradient    second_moment = beta2 * second_moment + (1 - beta2) * gradient^2    first_moment_corrected = first_moment / (1 - beta1^t)    second_moment_corrected = second_moment / (1 - beta2^t)    parameter = parameter - learning_rate * first_moment_corrected / sqrt(second_moment_corrected + epsilon) where beta1 and beta2 are the moment hyperparameters, and epsilon is a small constant to prevent division by zero.\nAdvantages: - Adaptive learning rate for each parameter, reducing the need for extensive learning rate tuning. - Efficient and robust in practice, suitable for a wide range of neural network architectures and cost functions. - Fast convergence and good generalization on various datasets.\nDisadvantages: - Adam may exhibit slow convergence on certain non-stationary objectives or saddle points. - Requires more memory to store the additional moving average parameters.\n\n\n\nEach optimization method has its strengths and weaknesses, and the choice of the optimization algorithm may depend on the specific problem, dataset, and neural network architecture. While Gradient Descent is a simple baseline algorithm, Momentum and Adam often offer faster convergence and more stable updates in practice. However, Adam may require more memory due to the additional moving average parameters.\nIt is recommended to experiment with different optimization methods and hyperparameter values to find the optimal combination that yields the best convergence and accuracy for a given neural network task. Additionally, other optimization techniques, such as Adagrad, RMSprop, and Nesterov Accelerated Gradient (NAG), are also widely used in practice and may be worth exploring for specific scenarios.\n\nimport numpy as np \nimport matplotlib.pyplot as plt \nimport h5py \nimport scipy.io \nimport sklearn \nimport sklearn.datasets\n\n\n#creat L-layer neural net with different optimizations \n\n\ndef sigmoid (x) : \n    s = 1/(1+np.exp(-x))\n    return s\n\n\ndef relu(x):\n    s = np.maximum(0,x)\n    return s\n\n\n\ndef load_params_and_grads(seed=1):\n    np.random.seed(seed)\n    W1 = np.random.randn(2,3)\n    b1 = np.random.randn(2,1)\n    W2 = np.random.randn(3,3)\n    b2 = np.random.randn(3,1)\n\n    dW1 = np.random.randn(2,3)\n    db1 = np.random.randn(2,1)\n    dW2 = np.random.randn(3,3)\n    db2 = np.random.randn(3,1)\n    \n    return W1, b1, W2, b2, dW1, db1, dW2, db2\n\n\ndef initialize_parameters(layer_dims) : \n    np.random.seed(3)\n    parameters = {}\n    L = len (layer_dims)\n\n    for i in range (1 , L) : \n        parameters['W' + str (i)] = np.random.rand ( layer_dims[i] ,layer_dims[i-1]) * np.sqrt (2/layer_dims[i-1])\n        parameters['b' + str(i)] = np.zeros(shape=(layer_dims[i] ,1 ))\n\n    return parameters \n\n\ndef compute_cost (a3 , y) : \n    m = max (np.shape(y))\n    cost = 1./m * np.sum(np.multiply (-np.log(a3),y) + np.multiply(-np.log(1 - a3), 1 - y))\n    return cost \n\n\ndef forward_propagation(X , parameters) : \n        \n    # retrieve parameters\n    W1 = parameters[\"W1\"]\n    b1 = parameters[\"b1\"]\n    W2 = parameters[\"W2\"]\n    b2 = parameters[\"b2\"]\n    W3 = parameters[\"W3\"]\n    b3 = parameters[\"b3\"]\n    \n    # LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID\n    z1 = np.dot(W1, X) + b1\n    a1 = relu(z1)\n    z2 = np.dot(W2, a1) + b2\n    a2 = relu(z2)\n    z3 = np.dot(W3, a2) + b3\n    a3 = sigmoid(z3)\n    \n    cache = (z1, a1, W1, b1, z2, a2, W2, b2, z3, a3, W3, b3)\n    \n    return a3, cache \n\n\ndef backward_propagation(X, Y, cache):\n    m = X.shape[1]\n    (z1, a1, W1, b1, z2, a2, W2, b2, z3, a3, W3, b3) = cache\n    \n    dz3 = 1./m * (a3 - Y)\n    dW3 = np.dot(dz3, a2.T)\n    db3 = np.sum(dz3, axis=1, keepdims = True)\n    \n    da2 = np.dot(W3.T, dz3)\n    dz2 = np.multiply(da2, np.int64(a2 &gt; 0))\n    dW2 = np.dot(dz2, a1.T)\n    db2 = np.sum(dz2, axis=1, keepdims = True)\n    \n    da1 = np.dot(W2.T, dz2)\n    dz1 = np.multiply(da1, np.int64(a1 &gt; 0))\n    dW1 = np.dot(dz1, X.T)\n    db1 = np.sum(dz1, axis=1, keepdims = True)\n    \n    gradients = {\"dz3\": dz3, \"dW3\": dW3, \"db3\": db3,\n                 \"da2\": da2, \"dz2\": dz2, \"dW2\": dW2, \"db2\": db2,\n                 \"da1\": da1, \"dz1\": dz1, \"dW1\": dW1, \"db1\": db1}\n    \n    return gradients\n\n\ndef predict(X, y, parameters):\n\n    \n    m = X.shape[1]\n    p = np.zeros((1,m))\n    \n    # Forward propagation\n    a3, caches = forward_propagation(X, parameters)\n    \n    # convert probas to 0/1 predictions\n    for i in range(0, a3.shape[1]):\n        if a3[0,i] &gt; 0.5:\n            p[0,i] = 1\n        else:\n            p[0,i] = 0\n\n    print(\"Accuracy: \"  + str(np.mean((p[0,:] == y[0,:]))))\n    \n    return p\n\n\ndef predict_dec(parameters, X):\n    \"\"\"\n    Used for plotting decision boundary.\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters \n    X -- input data of size (m, K)\n    \n    Returns\n    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n    \"\"\"\n    \n    # Predict using forward propagation and a classification threshold of 0.5\n    a3, cache = forward_propagation(X, parameters)\n    predictions = (a3 &gt; 0.5)\n    return predictions\n\ndef load_dataset():\n    np.random.seed(3)\n    train_X, train_Y = sklearn.datasets.make_moons(n_samples=300, noise=.2) #300 #0.2 \n    # Visualize the data\n    plt.scatter(train_X[:, 0], train_X[:, 1], c=train_Y, s=40, cmap=plt.cm.Spectral);\n    train_X = train_X.T\n    train_Y = train_Y.reshape((1, train_Y.shape[0]))\n    \n    return train_X, train_Y\n\n\ntrain_X, train_Y = load_dataset()\n\n\n\n\n\n\n\n\n\n\ndef update_parameters_with_gd(parameters, grads, learning_rate):\n   \n    L = len(parameters) // 2 # number of layers in the neural networks\n\n    # Update rule for each parameter\n    for l in range(L):\n        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * grads[\"db\" + str(l + 1)]\n        \n    return parameters\n\n\nimport math\ndef random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n\n    \n    np.random.seed(seed)            # To make your \"random\" minibatches the same as ours\n    m = X.shape[1]                  # number of training examples\n    mini_batches = []\n\n    permutation = list(np.random.permutation(m))\n    shuffled_X = X[:, permutation]\n    shuffled_Y = Y[:, permutation].reshape((1,m))\n\n    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n    for k in range(0, num_complete_minibatches):\n        mini_batch_X = shuffled_X[:,k * mini_batch_size:(k + 1) * mini_batch_size]\n        mini_batch_Y = shuffled_Y[:,k * mini_batch_size:(k + 1) * mini_batch_size]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    # Handling the end case (last mini-batch &lt; mini_batch_size)\n    if m % mini_batch_size != 0:\n        end = m - mini_batch_size * math.floor(m / mini_batch_size)\n        mini_batch_X = shuffled_X[:,num_complete_minibatches * mini_batch_size:]\n        mini_batch_Y = shuffled_Y[:,num_complete_minibatches * mini_batch_size:]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    return mini_batches\n\n\nmini_batches = random_mini_batches(train_X, train_Y, mini_batch_size = 128 )\nprint (\"shape of the X_train is \" , np.shape (train_X))\nprint(\"shape of the 1st mini_batch_X: \" + str(mini_batches[0][0].shape))\nprint(\"shape of the 2nd mini_batch_X: \" + str(mini_batches[1][0].shape))\nprint(\"shape of the 3rd mini_batch_X: \" + str(mini_batches[2][0].shape))\nprint(\"shape of the 1st mini_batch_Y: \" + str(mini_batches[0][1].shape))\nprint(\"shape of the 2nd mini_batch_Y: \" + str(mini_batches[1][1].shape)) \nprint(\"shape of the 3rd mini_batch_Y: \" + str(mini_batches[2][1].shape))\nprint(\"mini batch sanity check: \" + str(mini_batches[0][0][0][0:3]))\n\nshape of the X_train is  (2, 300)\nshape of the 1st mini_batch_X: (2, 128)\nshape of the 2nd mini_batch_X: (2, 128)\nshape of the 3rd mini_batch_X: (2, 44)\nshape of the 1st mini_batch_Y: (1, 128)\nshape of the 2nd mini_batch_Y: (1, 128)\nshape of the 3rd mini_batch_Y: (1, 44)\nmini batch sanity check: [-0.14656235  0.22452308  1.38239247]\n\n\n\nMomentum\n\n\ndef initialize_velocity(parameters):\n  \n    \n    L = len(parameters) // 2 # number of layers in the neural networks\n    v = {}\n    \n    # Initialize velocity\n    for l in range(L):\n        ### START CODE HERE ### (approx. 2 lines)\n        v[\"dW\" + str(l + 1)] = np.zeros_like(parameters[\"W\" + str(l+1)])\n        v[\"db\" + str(l + 1)] = np.zeros_like(parameters[\"b\" + str(l+1)])\n        ### END CODE HERE ###\n        \n    return v\n\n\n\ndef update_parameters_with_momentum(parameters, grads, v, beta, learning_rate) :\n\n    L = len(parameters) // 2 # number of layers in the neural networks\n    \n    # Momentum update for each parameter\n    for l in range(L):\n        \n        # compute velocities\n        v[\"dW\" + str(l + 1)] = beta * v[\"dW\" + str(l + 1)] + (1 - beta) * grads['dW' + str(l + 1)]\n        v[\"db\" + str(l + 1)] = beta * v[\"db\" + str(l + 1)] + (1 - beta) * grads['db' + str(l + 1)]\n        # update parameters\n        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * v[\"dW\" + str(l + 1)]\n        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * v[\"db\" + str(l + 1)]\n        \n    return parameters, v\n\n# ADAM Optimizer\n\ndef initialize_adam(parameters) :\n    \"\"\"\n    Initializes v and s as two python dictionaries with:\n                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" \n                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters.\n                    parameters[\"W\" + str(l)] = Wl\n                    parameters[\"b\" + str(l)] = bl\n    \n    Returns: \n    v -- python dictionary that will contain the exponentially weighted average of the gradient.\n                    v[\"dW\" + str(l)] = ...\n                    v[\"db\" + str(l)] = ...\n    s -- python dictionary that will contain the exponentially weighted average of the squared gradient.\n                    s[\"dW\" + str(l)] = ...\n                    s[\"db\" + str(l)] = ...\n\n    \"\"\"\n    \n    L = len(parameters) // 2 # number of layers in the neural networks\n    v = {}\n    s = {}\n    \n    # Initialize v, s. Input: \"parameters\". Outputs: \"v, s\".\n    for l in range(L):\n    ### START CODE HERE ### (approx. 4 lines)\n        v[\"dW\" + str(l + 1)] = np.zeros_like(parameters[\"W\" + str(l + 1)])\n        v[\"db\" + str(l + 1)] = np.zeros_like(parameters[\"b\" + str(l + 1)])\n\n        s[\"dW\" + str(l+1)] = np.zeros_like(parameters[\"W\" + str(l + 1)])\n        s[\"db\" + str(l+1)] = np.zeros_like(parameters[\"b\" + str(l + 1)])\n    ### END CODE HERE ###\n    \n    return v, s\n\n\ndef update_parameters_with_adam(parameters, grads, v, s, t, learning_rate=0.01,\n                                beta1=0.9, beta2=0.999, epsilon=1e-8):\n    \n    L = len(parameters) // 2                 # number of layers in the neural networks\n    v_corrected = {}                         # Initializing first moment estimate, python dictionary\n    s_corrected = {}                         # Initializing second moment estimate, python dictionary\n    \n    # Perform Adam update on all parameters\n    for l in range(L):\n        # Moving average of the gradients. Inputs: \"v, grads, beta1\". Output: \"v\".\n        v[\"dW\" + str(l + 1)] = beta1 * v[\"dW\" + str(l + 1)] + (1 - beta1) * grads['dW' + str(l + 1)]\n        v[\"db\" + str(l + 1)] = beta1 * v[\"db\" + str(l + 1)] + (1 - beta1) * grads['db' + str(l + 1)]\n\n        # Compute bias-corrected first moment estimate. Inputs: \"v, beta1, t\". Output: \"v_corrected\".\n        v_corrected[\"dW\" + str(l + 1)] = v[\"dW\" + str(l + 1)] / (1 - np.power(beta1, t))\n        v_corrected[\"db\" + str(l + 1)] = v[\"db\" + str(l + 1)] / (1 - np.power(beta1, t))\n\n        # Moving average of the squared gradients. Inputs: \"s, grads, beta2\". Output: \"s\".\n        s[\"dW\" + str(l + 1)] = beta2 * s[\"dW\" + str(l + 1)] + (1 - beta2) * np.power(grads['dW' + str(l + 1)], 2)\n        s[\"db\" + str(l + 1)] = beta2 * s[\"db\" + str(l + 1)] + (1 - beta2) * np.power(grads['db' + str(l + 1)], 2)\n\n        # Compute bias-corrected second raw moment estimate. Inputs: \"s, beta2, t\". Output: \"s_corrected\".\n        s_corrected[\"dW\" + str(l + 1)] = s[\"dW\" + str(l + 1)] / (1 - np.power(beta2, t))\n        s_corrected[\"db\" + str(l + 1)] = s[\"db\" + str(l + 1)] / (1 - np.power(beta2, t))\n\n        # Update parameters. Inputs: \"parameters, learning_rate, v_corrected, s_corrected, epsilon\". Output: \"parameters\".\n        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * v_corrected[\"dW\" + str(l + 1)] / np.sqrt(s[\"dW\" + str(l + 1)] + epsilon)\n        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * v_corrected[\"db\" + str(l + 1)] / np.sqrt(s[\"db\" + str(l + 1)] + epsilon)\n\n    return parameters, v, s\n\n\ndef model(X, Y, layers_dims, optimizer, learning_rate=0.0007, mini_batch_size=64, beta=0.9,\n          beta1=0.9, beta2=0.999, epsilon=1e-8, num_epochs=10000, print_cost=True):\n\n    L = len(layers_dims)             # number of layers in the neural networks\n    costs = []                       # to keep track of the cost\n    t = 0                            # initializing the counter required for Adam update\n    seed = 10                        # For grading purposes, so that your \"random\" minibatches are the same as ours\n    \n    # Initialize parameters\n    parameters = initialize_parameters(layers_dims)\n\n    # Initialize the optimizer\n    if optimizer == \"gd\":\n        pass # no initialization required for gradient descent\n    elif optimizer == \"momentum\":\n        v = initialize_velocity(parameters)\n    elif optimizer == \"adam\":\n        v, s = initialize_adam(parameters)\n    \n    # Optimization loop\n    for i in range(num_epochs):\n        \n        # Define the random minibatches. We increment the seed to reshuffle differently the dataset after each epoch\n        seed = seed + 1\n        minibatches = random_mini_batches(X, Y, mini_batch_size, seed)\n\n        for minibatch in minibatches:\n\n            # Select a minibatch\n            (minibatch_X, minibatch_Y) = minibatch\n\n            # Forward propagation\n            a3, caches = forward_propagation(minibatch_X, parameters)\n\n            # Compute cost\n            cost = compute_cost(a3, minibatch_Y)\n\n            # Backward propagation\n            grads = backward_propagation(minibatch_X, minibatch_Y, caches)\n\n            # Update parameters\n            if optimizer == \"gd\":\n                parameters = update_parameters_with_gd(parameters, grads, learning_rate)\n            elif optimizer == \"momentum\":\n                parameters, v = update_parameters_with_momentum(parameters, grads, v, beta, learning_rate)\n            elif optimizer == \"adam\":\n                t = t + 1 # Adam counter\n                parameters, v, s = update_parameters_with_adam(parameters, grads, v, s,\n                                                               t, learning_rate, beta1, beta2,  epsilon)\n        \n        # Print the cost every 1000 epoch\n        if print_cost and i % 1000 == 0:\n            print(\"Cost after epoch %i: %f\" % (i, cost))\n        if print_cost and i % 100 == 0:\n            costs.append(cost)\n                \n    # plot the cost\n    plt.plot(costs)\n    plt.ylabel('cost')\n    plt.xlabel('epochs (per 100)')\n    plt.title(\"Learning rate = \" + str(learning_rate))\n    plt.show()\n\n    return parameters\n\n\n# train 3-layer model\nlayers_dims = [train_X.shape[0], 5, 2, 1]\nparameters = model(train_X, train_Y, layers_dims, optimizer=\"gd\")\n\n# Predict\npredictions = predict(train_X, train_Y, parameters)\n\n\nCost after epoch 0: 0.776085\nCost after epoch 1000: 0.709262\nCost after epoch 2000: 0.663615\nCost after epoch 3000: 0.663728\nCost after epoch 4000: 0.597440\nCost after epoch 5000: 0.507172\nCost after epoch 6000: 0.487885\nCost after epoch 7000: 0.346078\nCost after epoch 8000: 0.372416\nCost after epoch 9000: 0.498869\n\n\n\n\n\n\n\n\n\nAccuracy: 0.85\n\n\n\n\n# train 3-layer model\nlayers_dims = [train_X.shape[0], 5, 2, 1]\nparameters = model(train_X, train_Y, layers_dims, beta=0.9, optimizer=\"momentum\")\n\n# Predict\npredictions = predict(train_X, train_Y, parameters)\n\n\nCost after epoch 0: 0.776325\nCost after epoch 1000: 0.709252\nCost after epoch 2000: 0.663705\nCost after epoch 3000: 0.663868\nCost after epoch 4000: 0.597699\nCost after epoch 5000: 0.507593\nCost after epoch 6000: 0.488211\nCost after epoch 7000: 0.346261\nCost after epoch 8000: 0.372549\nCost after epoch 9000: 0.498889\n\n\n\n\n\n\n\n\n\nAccuracy: 0.85\n\n\n\n# train 3-layer model\nlayers_dims = [train_X.shape[0], 5, 2, 1]\nparameters = model(train_X, train_Y, layers_dims, optimizer=\"adam\")\n\n# Predict\npredictions = predict(train_X, train_Y, parameters)\n\n\n\nCost after epoch 0: 0.706337\nCost after epoch 1000: 0.173407\nCost after epoch 2000: 0.191965\nCost after epoch 3000: 0.049790\nCost after epoch 4000: 0.147109\nCost after epoch 5000: 0.108286\nCost after epoch 6000: 0.122491\nCost after epoch 7000: 0.028581\nCost after epoch 8000: 0.131736\nCost after epoch 9000: 0.159093\n\n\n\n\n\n\n\n\n\nAccuracy: 0.9433333333333334"
  },
  {
    "objectID": "labs/neural_network_optimization.html#neural-network-with-optimization-methods-comparison",
    "href": "labs/neural_network_optimization.html#neural-network-with-optimization-methods-comparison",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "This notebook demonstrates the implementation of a three-layer neural network and compares the performance of different optimization methods: Gradient Descent (GD), Momentum, and Adam. The neural network is trained on a two-moon-shaped dataset using various optimization techniques to find the optimal parameters that minimize the cost function and improve the accuracy of the model.\n\n\nThe dataset used in this notebook is generated using the make_moons() function from the sklearn.datasets module. The dataset consists of two classes that form two moon shapes. It is a non-linearly separable dataset, making it a suitable scenario for testing different optimization methods for training a neural network.\n\n\n\nThe neural network uses two activation functions:\n\nReLU (Rectified Linear Unit): The ReLU activation function is used in the hidden layers of the neural network. It allows the model to handle non-linearities efficiently and avoids the vanishing gradient problem, which can occur with sigmoid activation.\nSigmoid: The sigmoid activation function is used in the output layer to compute the final probability of the binary classification task. It maps the input to the range (0, 1), representing the probability of the input belonging to class 1.\n\n\n\n\nThe neural network architecture is defined as a three-layer model with the following layer dimensions: [input size, 5, 2, 1]. The input size corresponds to the features of the dataset, and the output size is set to 1 for binary classification.\n\n\n\n\n\nGradient Descent is a first-order optimization algorithm that updates the model’s parameters in the opposite direction of the gradient of the cost function. The magnitude of the update is controlled by the learning rate. While GD is a simple and intuitive optimization method, it may suffer from slow convergence, especially for large datasets or in cases where the cost function has high curvatures.\n\n\n\nMomentum is an extension of GD that introduces a moving average of the gradients to accelerate convergence. It accumulates the past gradients’ information to continue moving in the same direction even when the gradients change direction frequently. This helps in faster convergence and reduces oscillations in the cost function. The momentum hyperparameter controls the influence of the past gradients.\n\n\n\nAdam is a popular optimization algorithm that combines the ideas of both Momentum and RMSprop. It uses moving averages of the past gradients and squared gradients to adapt the learning rate for each parameter. The algorithm automatically adjusts the learning rate based on the history of the gradients and their magnitudes. This adaptive learning rate makes Adam robust and efficient in practice, requiring minimal hyperparameter tuning.\n\n\n\n\nThe neural network is trained using each of the three optimization methods. For each optimization method, the model’s parameters are updated iteratively over a specified number of epochs using mini-batch gradient descent. At the end of each epoch, the cost function is computed and printed to monitor the training progress. The final trained model’s accuracy on the training data is also calculated and displayed.\n\n\n\nThe notebook provides an insight into the effectiveness of different optimization methods in training neural networks. By comparing the performance of GD, Momentum, and Adam, we can gain a better understanding of how these algorithms handle the optimization process and improve the neural network’s convergence and accuracy on non-linear datasets like the two-moon-shaped dataset used here.\nSure, let’s delve into the details and theory of each optimization method along with their advantages and disadvantages.\n\n\n\nTheory: Gradient Descent is a first-order optimization algorithm used to minimize the cost function of a neural network. It works by iteratively updating the model’s parameters in the opposite direction of the gradient of the cost function with respect to those parameters. The gradient points in the direction of steepest ascent, so taking the opposite direction allows the algorithm to move towards the minimum of the cost function.\nAlgorithm: 1. Initialize the model’s parameters randomly. 2. Compute the gradient of the cost function with respect to each parameter. 3. Update each parameter using the formula: parameter = parameter - learning_rate * gradient, where the learning_rate controls the size of the update step.\nAdvantages: - Simple and easy to implement. - Can be applied to large datasets since it processes one data point at a time. - Can handle non-convex cost functions.\nDisadvantages: - Convergence can be slow, especially for large datasets or complex cost functions. - Sensitive to the learning rate choice; a large learning rate may lead to overshooting the minimum, while a small learning rate may result in slow convergence.\n\n\n\nTheory: Momentum is an extension of GD that aims to accelerate the convergence of the optimization process. It introduces a moving average of the past gradients to continue moving in the same direction even when the gradients change direction frequently. This helps to overcome oscillations in the cost function and speeds up convergence.\nAlgorithm: 1. Initialize the model’s parameters and the velocity (initialized as zeros) for each parameter. 2. Compute the gradient of the cost function with respect to each parameter. 3. Update each parameter using the formula: velocity = beta * velocity + (1 - beta) * gradient    parameter = parameter - learning_rate * velocity where beta is the momentum hyperparameter.\nAdvantages: - Accelerates convergence, especially in areas with high curvature or noisy gradients. - Reduces oscillations and overshooting, leading to more stable updates.\nDisadvantages: - Momentum may accumulate too much velocity in flat regions, making it harder to escape local minima. - May overshoot and oscillate when the learning rate is too large.\n\n\n\nTheory: Adam is an adaptive learning rate optimization algorithm that combines the ideas of both Momentum and RMSprop. It uses moving averages of the past gradients and squared gradients to adapt the learning rate for each parameter. The algorithm automatically adjusts the learning rate based on the history of the gradients and their magnitudes.\nAlgorithm: 1. Initialize the model’s parameters and the first and second moment estimates (initialized as zeros) for each parameter. 2. Compute the gradient of the cost function with respect to each parameter. 3. Update each parameter using the formula: first_moment = beta1 * first_moment + (1 - beta1) * gradient    second_moment = beta2 * second_moment + (1 - beta2) * gradient^2    first_moment_corrected = first_moment / (1 - beta1^t)    second_moment_corrected = second_moment / (1 - beta2^t)    parameter = parameter - learning_rate * first_moment_corrected / sqrt(second_moment_corrected + epsilon) where beta1 and beta2 are the moment hyperparameters, and epsilon is a small constant to prevent division by zero.\nAdvantages: - Adaptive learning rate for each parameter, reducing the need for extensive learning rate tuning. - Efficient and robust in practice, suitable for a wide range of neural network architectures and cost functions. - Fast convergence and good generalization on various datasets.\nDisadvantages: - Adam may exhibit slow convergence on certain non-stationary objectives or saddle points. - Requires more memory to store the additional moving average parameters.\n\n\n\nEach optimization method has its strengths and weaknesses, and the choice of the optimization algorithm may depend on the specific problem, dataset, and neural network architecture. While Gradient Descent is a simple baseline algorithm, Momentum and Adam often offer faster convergence and more stable updates in practice. However, Adam may require more memory due to the additional moving average parameters.\nIt is recommended to experiment with different optimization methods and hyperparameter values to find the optimal combination that yields the best convergence and accuracy for a given neural network task. Additionally, other optimization techniques, such as Adagrad, RMSprop, and Nesterov Accelerated Gradient (NAG), are also widely used in practice and may be worth exploring for specific scenarios.\n\nimport numpy as np \nimport matplotlib.pyplot as plt \nimport h5py \nimport scipy.io \nimport sklearn \nimport sklearn.datasets\n\n\n#creat L-layer neural net with different optimizations \n\n\ndef sigmoid (x) : \n    s = 1/(1+np.exp(-x))\n    return s\n\n\ndef relu(x):\n    s = np.maximum(0,x)\n    return s\n\n\n\ndef load_params_and_grads(seed=1):\n    np.random.seed(seed)\n    W1 = np.random.randn(2,3)\n    b1 = np.random.randn(2,1)\n    W2 = np.random.randn(3,3)\n    b2 = np.random.randn(3,1)\n\n    dW1 = np.random.randn(2,3)\n    db1 = np.random.randn(2,1)\n    dW2 = np.random.randn(3,3)\n    db2 = np.random.randn(3,1)\n    \n    return W1, b1, W2, b2, dW1, db1, dW2, db2\n\n\ndef initialize_parameters(layer_dims) : \n    np.random.seed(3)\n    parameters = {}\n    L = len (layer_dims)\n\n    for i in range (1 , L) : \n        parameters['W' + str (i)] = np.random.rand ( layer_dims[i] ,layer_dims[i-1]) * np.sqrt (2/layer_dims[i-1])\n        parameters['b' + str(i)] = np.zeros(shape=(layer_dims[i] ,1 ))\n\n    return parameters \n\n\ndef compute_cost (a3 , y) : \n    m = max (np.shape(y))\n    cost = 1./m * np.sum(np.multiply (-np.log(a3),y) + np.multiply(-np.log(1 - a3), 1 - y))\n    return cost \n\n\ndef forward_propagation(X , parameters) : \n        \n    # retrieve parameters\n    W1 = parameters[\"W1\"]\n    b1 = parameters[\"b1\"]\n    W2 = parameters[\"W2\"]\n    b2 = parameters[\"b2\"]\n    W3 = parameters[\"W3\"]\n    b3 = parameters[\"b3\"]\n    \n    # LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID\n    z1 = np.dot(W1, X) + b1\n    a1 = relu(z1)\n    z2 = np.dot(W2, a1) + b2\n    a2 = relu(z2)\n    z3 = np.dot(W3, a2) + b3\n    a3 = sigmoid(z3)\n    \n    cache = (z1, a1, W1, b1, z2, a2, W2, b2, z3, a3, W3, b3)\n    \n    return a3, cache \n\n\ndef backward_propagation(X, Y, cache):\n    m = X.shape[1]\n    (z1, a1, W1, b1, z2, a2, W2, b2, z3, a3, W3, b3) = cache\n    \n    dz3 = 1./m * (a3 - Y)\n    dW3 = np.dot(dz3, a2.T)\n    db3 = np.sum(dz3, axis=1, keepdims = True)\n    \n    da2 = np.dot(W3.T, dz3)\n    dz2 = np.multiply(da2, np.int64(a2 &gt; 0))\n    dW2 = np.dot(dz2, a1.T)\n    db2 = np.sum(dz2, axis=1, keepdims = True)\n    \n    da1 = np.dot(W2.T, dz2)\n    dz1 = np.multiply(da1, np.int64(a1 &gt; 0))\n    dW1 = np.dot(dz1, X.T)\n    db1 = np.sum(dz1, axis=1, keepdims = True)\n    \n    gradients = {\"dz3\": dz3, \"dW3\": dW3, \"db3\": db3,\n                 \"da2\": da2, \"dz2\": dz2, \"dW2\": dW2, \"db2\": db2,\n                 \"da1\": da1, \"dz1\": dz1, \"dW1\": dW1, \"db1\": db1}\n    \n    return gradients\n\n\ndef predict(X, y, parameters):\n\n    \n    m = X.shape[1]\n    p = np.zeros((1,m))\n    \n    # Forward propagation\n    a3, caches = forward_propagation(X, parameters)\n    \n    # convert probas to 0/1 predictions\n    for i in range(0, a3.shape[1]):\n        if a3[0,i] &gt; 0.5:\n            p[0,i] = 1\n        else:\n            p[0,i] = 0\n\n    print(\"Accuracy: \"  + str(np.mean((p[0,:] == y[0,:]))))\n    \n    return p\n\n\ndef predict_dec(parameters, X):\n    \"\"\"\n    Used for plotting decision boundary.\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters \n    X -- input data of size (m, K)\n    \n    Returns\n    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n    \"\"\"\n    \n    # Predict using forward propagation and a classification threshold of 0.5\n    a3, cache = forward_propagation(X, parameters)\n    predictions = (a3 &gt; 0.5)\n    return predictions\n\ndef load_dataset():\n    np.random.seed(3)\n    train_X, train_Y = sklearn.datasets.make_moons(n_samples=300, noise=.2) #300 #0.2 \n    # Visualize the data\n    plt.scatter(train_X[:, 0], train_X[:, 1], c=train_Y, s=40, cmap=plt.cm.Spectral);\n    train_X = train_X.T\n    train_Y = train_Y.reshape((1, train_Y.shape[0]))\n    \n    return train_X, train_Y\n\n\ntrain_X, train_Y = load_dataset()\n\n\n\n\n\n\n\n\n\n\ndef update_parameters_with_gd(parameters, grads, learning_rate):\n   \n    L = len(parameters) // 2 # number of layers in the neural networks\n\n    # Update rule for each parameter\n    for l in range(L):\n        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * grads[\"db\" + str(l + 1)]\n        \n    return parameters\n\n\nimport math\ndef random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n\n    \n    np.random.seed(seed)            # To make your \"random\" minibatches the same as ours\n    m = X.shape[1]                  # number of training examples\n    mini_batches = []\n\n    permutation = list(np.random.permutation(m))\n    shuffled_X = X[:, permutation]\n    shuffled_Y = Y[:, permutation].reshape((1,m))\n\n    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n    for k in range(0, num_complete_minibatches):\n        mini_batch_X = shuffled_X[:,k * mini_batch_size:(k + 1) * mini_batch_size]\n        mini_batch_Y = shuffled_Y[:,k * mini_batch_size:(k + 1) * mini_batch_size]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    # Handling the end case (last mini-batch &lt; mini_batch_size)\n    if m % mini_batch_size != 0:\n        end = m - mini_batch_size * math.floor(m / mini_batch_size)\n        mini_batch_X = shuffled_X[:,num_complete_minibatches * mini_batch_size:]\n        mini_batch_Y = shuffled_Y[:,num_complete_minibatches * mini_batch_size:]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    return mini_batches\n\n\nmini_batches = random_mini_batches(train_X, train_Y, mini_batch_size = 128 )\nprint (\"shape of the X_train is \" , np.shape (train_X))\nprint(\"shape of the 1st mini_batch_X: \" + str(mini_batches[0][0].shape))\nprint(\"shape of the 2nd mini_batch_X: \" + str(mini_batches[1][0].shape))\nprint(\"shape of the 3rd mini_batch_X: \" + str(mini_batches[2][0].shape))\nprint(\"shape of the 1st mini_batch_Y: \" + str(mini_batches[0][1].shape))\nprint(\"shape of the 2nd mini_batch_Y: \" + str(mini_batches[1][1].shape)) \nprint(\"shape of the 3rd mini_batch_Y: \" + str(mini_batches[2][1].shape))\nprint(\"mini batch sanity check: \" + str(mini_batches[0][0][0][0:3]))\n\nshape of the X_train is  (2, 300)\nshape of the 1st mini_batch_X: (2, 128)\nshape of the 2nd mini_batch_X: (2, 128)\nshape of the 3rd mini_batch_X: (2, 44)\nshape of the 1st mini_batch_Y: (1, 128)\nshape of the 2nd mini_batch_Y: (1, 128)\nshape of the 3rd mini_batch_Y: (1, 44)\nmini batch sanity check: [-0.14656235  0.22452308  1.38239247]\n\n\n\nMomentum\n\n\ndef initialize_velocity(parameters):\n  \n    \n    L = len(parameters) // 2 # number of layers in the neural networks\n    v = {}\n    \n    # Initialize velocity\n    for l in range(L):\n        ### START CODE HERE ### (approx. 2 lines)\n        v[\"dW\" + str(l + 1)] = np.zeros_like(parameters[\"W\" + str(l+1)])\n        v[\"db\" + str(l + 1)] = np.zeros_like(parameters[\"b\" + str(l+1)])\n        ### END CODE HERE ###\n        \n    return v\n\n\n\ndef update_parameters_with_momentum(parameters, grads, v, beta, learning_rate) :\n\n    L = len(parameters) // 2 # number of layers in the neural networks\n    \n    # Momentum update for each parameter\n    for l in range(L):\n        \n        # compute velocities\n        v[\"dW\" + str(l + 1)] = beta * v[\"dW\" + str(l + 1)] + (1 - beta) * grads['dW' + str(l + 1)]\n        v[\"db\" + str(l + 1)] = beta * v[\"db\" + str(l + 1)] + (1 - beta) * grads['db' + str(l + 1)]\n        # update parameters\n        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * v[\"dW\" + str(l + 1)]\n        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * v[\"db\" + str(l + 1)]\n        \n    return parameters, v\n\n# ADAM Optimizer\n\ndef initialize_adam(parameters) :\n    \"\"\"\n    Initializes v and s as two python dictionaries with:\n                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" \n                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters.\n                    parameters[\"W\" + str(l)] = Wl\n                    parameters[\"b\" + str(l)] = bl\n    \n    Returns: \n    v -- python dictionary that will contain the exponentially weighted average of the gradient.\n                    v[\"dW\" + str(l)] = ...\n                    v[\"db\" + str(l)] = ...\n    s -- python dictionary that will contain the exponentially weighted average of the squared gradient.\n                    s[\"dW\" + str(l)] = ...\n                    s[\"db\" + str(l)] = ...\n\n    \"\"\"\n    \n    L = len(parameters) // 2 # number of layers in the neural networks\n    v = {}\n    s = {}\n    \n    # Initialize v, s. Input: \"parameters\". Outputs: \"v, s\".\n    for l in range(L):\n    ### START CODE HERE ### (approx. 4 lines)\n        v[\"dW\" + str(l + 1)] = np.zeros_like(parameters[\"W\" + str(l + 1)])\n        v[\"db\" + str(l + 1)] = np.zeros_like(parameters[\"b\" + str(l + 1)])\n\n        s[\"dW\" + str(l+1)] = np.zeros_like(parameters[\"W\" + str(l + 1)])\n        s[\"db\" + str(l+1)] = np.zeros_like(parameters[\"b\" + str(l + 1)])\n    ### END CODE HERE ###\n    \n    return v, s\n\n\ndef update_parameters_with_adam(parameters, grads, v, s, t, learning_rate=0.01,\n                                beta1=0.9, beta2=0.999, epsilon=1e-8):\n    \n    L = len(parameters) // 2                 # number of layers in the neural networks\n    v_corrected = {}                         # Initializing first moment estimate, python dictionary\n    s_corrected = {}                         # Initializing second moment estimate, python dictionary\n    \n    # Perform Adam update on all parameters\n    for l in range(L):\n        # Moving average of the gradients. Inputs: \"v, grads, beta1\". Output: \"v\".\n        v[\"dW\" + str(l + 1)] = beta1 * v[\"dW\" + str(l + 1)] + (1 - beta1) * grads['dW' + str(l + 1)]\n        v[\"db\" + str(l + 1)] = beta1 * v[\"db\" + str(l + 1)] + (1 - beta1) * grads['db' + str(l + 1)]\n\n        # Compute bias-corrected first moment estimate. Inputs: \"v, beta1, t\". Output: \"v_corrected\".\n        v_corrected[\"dW\" + str(l + 1)] = v[\"dW\" + str(l + 1)] / (1 - np.power(beta1, t))\n        v_corrected[\"db\" + str(l + 1)] = v[\"db\" + str(l + 1)] / (1 - np.power(beta1, t))\n\n        # Moving average of the squared gradients. Inputs: \"s, grads, beta2\". Output: \"s\".\n        s[\"dW\" + str(l + 1)] = beta2 * s[\"dW\" + str(l + 1)] + (1 - beta2) * np.power(grads['dW' + str(l + 1)], 2)\n        s[\"db\" + str(l + 1)] = beta2 * s[\"db\" + str(l + 1)] + (1 - beta2) * np.power(grads['db' + str(l + 1)], 2)\n\n        # Compute bias-corrected second raw moment estimate. Inputs: \"s, beta2, t\". Output: \"s_corrected\".\n        s_corrected[\"dW\" + str(l + 1)] = s[\"dW\" + str(l + 1)] / (1 - np.power(beta2, t))\n        s_corrected[\"db\" + str(l + 1)] = s[\"db\" + str(l + 1)] / (1 - np.power(beta2, t))\n\n        # Update parameters. Inputs: \"parameters, learning_rate, v_corrected, s_corrected, epsilon\". Output: \"parameters\".\n        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * v_corrected[\"dW\" + str(l + 1)] / np.sqrt(s[\"dW\" + str(l + 1)] + epsilon)\n        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * v_corrected[\"db\" + str(l + 1)] / np.sqrt(s[\"db\" + str(l + 1)] + epsilon)\n\n    return parameters, v, s\n\n\ndef model(X, Y, layers_dims, optimizer, learning_rate=0.0007, mini_batch_size=64, beta=0.9,\n          beta1=0.9, beta2=0.999, epsilon=1e-8, num_epochs=10000, print_cost=True):\n\n    L = len(layers_dims)             # number of layers in the neural networks\n    costs = []                       # to keep track of the cost\n    t = 0                            # initializing the counter required for Adam update\n    seed = 10                        # For grading purposes, so that your \"random\" minibatches are the same as ours\n    \n    # Initialize parameters\n    parameters = initialize_parameters(layers_dims)\n\n    # Initialize the optimizer\n    if optimizer == \"gd\":\n        pass # no initialization required for gradient descent\n    elif optimizer == \"momentum\":\n        v = initialize_velocity(parameters)\n    elif optimizer == \"adam\":\n        v, s = initialize_adam(parameters)\n    \n    # Optimization loop\n    for i in range(num_epochs):\n        \n        # Define the random minibatches. We increment the seed to reshuffle differently the dataset after each epoch\n        seed = seed + 1\n        minibatches = random_mini_batches(X, Y, mini_batch_size, seed)\n\n        for minibatch in minibatches:\n\n            # Select a minibatch\n            (minibatch_X, minibatch_Y) = minibatch\n\n            # Forward propagation\n            a3, caches = forward_propagation(minibatch_X, parameters)\n\n            # Compute cost\n            cost = compute_cost(a3, minibatch_Y)\n\n            # Backward propagation\n            grads = backward_propagation(minibatch_X, minibatch_Y, caches)\n\n            # Update parameters\n            if optimizer == \"gd\":\n                parameters = update_parameters_with_gd(parameters, grads, learning_rate)\n            elif optimizer == \"momentum\":\n                parameters, v = update_parameters_with_momentum(parameters, grads, v, beta, learning_rate)\n            elif optimizer == \"adam\":\n                t = t + 1 # Adam counter\n                parameters, v, s = update_parameters_with_adam(parameters, grads, v, s,\n                                                               t, learning_rate, beta1, beta2,  epsilon)\n        \n        # Print the cost every 1000 epoch\n        if print_cost and i % 1000 == 0:\n            print(\"Cost after epoch %i: %f\" % (i, cost))\n        if print_cost and i % 100 == 0:\n            costs.append(cost)\n                \n    # plot the cost\n    plt.plot(costs)\n    plt.ylabel('cost')\n    plt.xlabel('epochs (per 100)')\n    plt.title(\"Learning rate = \" + str(learning_rate))\n    plt.show()\n\n    return parameters\n\n\n# train 3-layer model\nlayers_dims = [train_X.shape[0], 5, 2, 1]\nparameters = model(train_X, train_Y, layers_dims, optimizer=\"gd\")\n\n# Predict\npredictions = predict(train_X, train_Y, parameters)\n\n\nCost after epoch 0: 0.776085\nCost after epoch 1000: 0.709262\nCost after epoch 2000: 0.663615\nCost after epoch 3000: 0.663728\nCost after epoch 4000: 0.597440\nCost after epoch 5000: 0.507172\nCost after epoch 6000: 0.487885\nCost after epoch 7000: 0.346078\nCost after epoch 8000: 0.372416\nCost after epoch 9000: 0.498869\n\n\n\n\n\n\n\n\n\nAccuracy: 0.85\n\n\n\n\n# train 3-layer model\nlayers_dims = [train_X.shape[0], 5, 2, 1]\nparameters = model(train_X, train_Y, layers_dims, beta=0.9, optimizer=\"momentum\")\n\n# Predict\npredictions = predict(train_X, train_Y, parameters)\n\n\nCost after epoch 0: 0.776325\nCost after epoch 1000: 0.709252\nCost after epoch 2000: 0.663705\nCost after epoch 3000: 0.663868\nCost after epoch 4000: 0.597699\nCost after epoch 5000: 0.507593\nCost after epoch 6000: 0.488211\nCost after epoch 7000: 0.346261\nCost after epoch 8000: 0.372549\nCost after epoch 9000: 0.498889\n\n\n\n\n\n\n\n\n\nAccuracy: 0.85\n\n\n\n# train 3-layer model\nlayers_dims = [train_X.shape[0], 5, 2, 1]\nparameters = model(train_X, train_Y, layers_dims, optimizer=\"adam\")\n\n# Predict\npredictions = predict(train_X, train_Y, parameters)\n\n\n\nCost after epoch 0: 0.706337\nCost after epoch 1000: 0.173407\nCost after epoch 2000: 0.191965\nCost after epoch 3000: 0.049790\nCost after epoch 4000: 0.147109\nCost after epoch 5000: 0.108286\nCost after epoch 6000: 0.122491\nCost after epoch 7000: 0.028581\nCost after epoch 8000: 0.131736\nCost after epoch 9000: 0.159093\n\n\n\n\n\n\n\n\n\nAccuracy: 0.9433333333333334"
  },
  {
    "objectID": "LICENSE.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "href": "LICENSE.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "title": "Attribution-ShareAlike 4.0 International",
    "section": "Creative Commons Attribution-ShareAlike 4.0 International Public License",
    "text": "Creative Commons Attribution-ShareAlike 4.0 International Public License\nBy exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-ShareAlike 4.0 International Public License (“Public License”). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.\n\nSection 1 – Definitions.\n\nAdapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.\nAdapter’s License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License.\nBY-SA Compatible License means a license listed at creativecommons.org/compatiblelicenses, approved by Creative Commons as essentially the equivalent of this Public License.\nCopyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.\nEffective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.\nExceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.\nLicense Elements means the license attributes listed in the name of a Creative Commons Public License. The License Elements of this Public License are Attribution and ShareAlike.\nLicensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License.\nLicensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.\nLicensor means the individual(s) or entity(ies) granting rights under this Public License.\nShare means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.\nSui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.\nYou means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.\n\n\n\nSection 2 – Scope.\n\nLicense grant.\n\nSubject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:\nA. reproduce and Share the Licensed Material, in whole or in part; and\nB. produce, reproduce, and Share Adapted Material.\nExceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.\nTerm. The term of this Public License is specified in Section 6(a).\nMedia and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)(4) never produces Adapted Material.\nDownstream recipients.\nA. Offer from the Licensor – Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.\nB. Additional offer from the Licensor – Adapted Material. Every recipient of Adapted Material from You automatically receives an offer from the Licensor to exercise the Licensed Rights in the Adapted Material under the conditions of the Adapter’s License You apply.\nC. No downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.\nNo endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).\n\nOther rights.\n\nMoral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.\nPatent and trademark rights are not licensed under this Public License.\nTo the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties.\n\n\n\n\nSection 3 – License Conditions.\nYour exercise of the Licensed Rights is expressly made subject to the following conditions.\n\nAttribution.\n\nIf You Share the Licensed Material (including in modified form), You must:\nA. retain the following if it is supplied by the Licensor with the Licensed Material:\n\nidentification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);\na copyright notice;\na notice that refers to this Public License;\na notice that refers to the disclaimer of warranties;\na URI or hyperlink to the Licensed Material to the extent reasonably practicable;\n\nB. indicate if You modified the Licensed Material and retain an indication of any previous modifications; and\nC. indicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.\nYou may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.\nIf requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.\n\nShareAlike.\n\nIn addition to the conditions in Section 3(a), if You Share Adapted Material You produce, the following conditions also apply.\n\nThe Adapter’s License You apply must be a Creative Commons license with the same License Elements, this version or later, or a BY-SA Compatible License.\nYou must include the text of, or the URI or hyperlink to, the Adapter’s License You apply. You may satisfy this condition in any reasonable manner based on the medium, means, and context in which You Share Adapted Material.\nYou may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, Adapted Material that restrict exercise of the rights granted under the Adapter’s License You apply.\n\n\n\nSection 4 – Sui Generis Database Rights.\nWhere the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:\n\nfor the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database;\nif You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material, including for purposes of Section 3(b); and\nYou must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.\n\nFor the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.\n\n\nSection 5 – Disclaimer of Warranties and Limitation of Liability.\n\nUnless otherwise separately undertaken by the Licensor, to the extent possible, the Licensor offers the Licensed Material as-is and as-available, and makes no representations or warranties of any kind concerning the Licensed Material, whether express, implied, statutory, or other. This includes, without limitation, warranties of title, merchantability, fitness for a particular purpose, non-infringement, absence of latent or other defects, accuracy, or the presence or absence of errors, whether or not known or discoverable. Where disclaimers of warranties are not allowed in full or in part, this disclaimer may not apply to You.\nTo the extent possible, in no event will the Licensor be liable to You on any legal theory (including, without limitation, negligence) or otherwise for any direct, special, indirect, incidental, consequential, punitive, exemplary, or other losses, costs, expenses, or damages arising out of this Public License or use of the Licensed Material, even if the Licensor has been advised of the possibility of such losses, costs, expenses, or damages. Where a limitation of liability is not allowed in full or in part, this limitation may not apply to You.\nThe disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.\n\n\n\nSection 6 – Term and Termination.\n\nThis Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.\nWhere Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:\n\nautomatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or\nupon express reinstatement by the Licensor.\n\nFor the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.\nFor the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.\nSections 1, 5, 6, 7, and 8 survive termination of this Public License.\n\n\n\nSection 7 – Other Terms and Conditions.\n\nThe Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.\nAny arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.\n\n\n\nSection 8 – Interpretation.\n\nFor the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.\nTo the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.\nNo term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.\nNothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.\n\n\nCreative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the “Licensor.” The text of the Creative Commons public licenses is dedicated to the public domain under the CC0 Public Domain Dedication. Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies, Creative Commons does not authorize the use of the trademark “Creative Commons” or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.\nCreative Commons may be contacted at creativecommons.org."
  },
  {
    "objectID": "weeks/week-6.html",
    "href": "weeks/week-6.html",
    "title": "Week 6",
    "section": "",
    "text": "Slides & Recordings\nEnsembling code skeleton\nThe definition of differential privacy\nIn-class Exercise & Solutions\nIn-person lab session: optimization and differential privacy lab05\nLecture 6 pre-recorded videos and quizzes\nMath assignment 1 due"
  },
  {
    "objectID": "weeks/week-5.html",
    "href": "weeks/week-5.html",
    "title": "Week 5",
    "section": "",
    "text": "Week 5 Overview\nSlides & Recordings\nIn-class Exercise & Solutions\nTaylor Series https://www.youtube.com/watch?v=3d6DsjIBzJ4\nTutorial: how to implement SGD with momentum neural network optimization\nLecture 5 pre-recorded videos and quizzes"
  },
  {
    "objectID": "weeks/week-1.html",
    "href": "weeks/week-1.html",
    "title": "Week 1",
    "section": "",
    "text": "Read the syllabus\nWeek 1 Overview\nSoftware Installations\nLecture 1 & Recordings\nNo tutorials/labs this week but please complete the Pre-Requisite Math Problems\nSuggested Review :\n\nLinear Algebra review \nProbability review"
  },
  {
    "objectID": "weeks/week-2.html",
    "href": "weeks/week-2.html",
    "title": "Week 2",
    "section": "",
    "text": "Week 2 Overview\nSlides & Recordings\nProf. Roger Grosse’s notes on backdrop\nNo exercises this week\nIn-person lab session: PyTorch basics with linear models lab01\nLec2 pre-recorded videos with quizzes"
  },
  {
    "objectID": "weeks/week-11.html",
    "href": "weeks/week-11.html",
    "title": "Week 11",
    "section": "",
    "text": "Slides & Recordings\nAutoencoder Notebook\nIn-class Exercise & Solutions\nIn-person midterm #2 which covers weeks 6-9. Held during the tutorial / lab sessions\nLecture 10 pre-recorded videos and quizzes\nFinal Project - Written feedback on project proposals sent by TAs and instructors"
  },
  {
    "objectID": "weeks/week-12.html",
    "href": "weeks/week-12.html",
    "title": "Week 12",
    "section": "",
    "text": "Slides & Recordings\nIn-class Exercise & Solutions\nIn-person lab session: RNN text classification lab10"
  },
  {
    "objectID": "weeks/week-8.html",
    "href": "weeks/week-8.html",
    "title": "Week 8",
    "section": "",
    "text": "Week 8 Overview\nSlides\nMissing recording due to technical issue\nIn-class Exercise & Solutions\nIn-person lab session: transfer learning and double descent lab07\nLecture 8 pre-recorded videos and quizzes"
  },
  {
    "objectID": "hw/hw-5.html",
    "href": "hw/hw-5.html",
    "title": "HW 5 - Statistics Experience",
    "section": "",
    "text": "The world of statistics and data science is vast and continually growing! The goal of the statistics experience assignments is to help you engage with the statistics and data science communities outside of the classroom.\nYou may submit the statistics experience assignment anytime between now and the deadline.\nEach experience has two parts:\n1️⃣ Have a statistics experience\n2️⃣ Make a slide summarizing on your experience\nYou must complete both parts to receive credit."
  },
  {
    "objectID": "hw/hw-5.html#part-1-experience-statistics-outside-of-the-classroom",
    "href": "hw/hw-5.html#part-1-experience-statistics-outside-of-the-classroom",
    "title": "HW 5 - Statistics Experience",
    "section": "Part 1: Experience statistics outside of the classroom",
    "text": "Part 1: Experience statistics outside of the classroom\nComplete an activity in one of the categories below. Under each category are suggested activities. You do not have to do one these suggested activities. You are welcome to find other activities as long as they are related to statistics/data science and they fit in one of the six categories. If there is an activity you’d like to do but you’re not sure if it qualifies for the statistics experience, just ask!\n\nCategory 1: Attend a talk or conference\nAttend an talk, panel, or conference related to statistics or data science. If you are attending a single talk or panel, it must be at least 30 minutes to count towards the statistics experience. The event can be in-person or online.\n\n\nCategory 2: Talk with a statistician/ data scientist\nTalk with someone who uses statistics in their daily work. This could include a professor, professional in industry, graduate student, etc.\n\n\nCategory 3: Listen to a podcast / watch video\nListen to a podcast or watch a video about statistics and data science. The podcast or video must be at least 30 minutes to count towards the statistics experience. A few suggestions are below:\n\nStats + Stories Podcast\nCausal Inference Podcast\nFiveThirtyEight Model Talk\nrstudio::global 2021 talks\nrstudio::conf 2020 talks\n\nThis list is not exhaustive. You may listen to other podcasts or watch other statistics/data science videos not included on this list. Ask your professor if you are unsure whether a particular podcast or video will count towards the statistics experience.\n\n\nCategory 4: Participate in a data science competition or challenge\nParticipate in a statistics or data science competition. You can participate individually or with a team. One option is DataFest, which will take place over the April 1-3, 2022 weekend. More information to follow here.\n\n\nCategory 5: Read a book on statistics/data science\nThere are a lot of books about statistics, data science, and related topics. A few suggestions are below. If you decide to read a book that isn’t on this list, ask your professor to make sure it counts toward the experience. Many of these books are available through Duke library.\n\nWeapons of Math Destruction by Cathy O’Neil\nHow Charts Lie: Getting Smarter about Visual Information by Alberto Cairo\nThe Theory that Would Not Die by Sharon Bertsch McGrayne\nThe Art of Statistics: How to learn from data by David Spiegelhalter\nThe Signal and the Noise: Why so many predictions fail - but some don’t by Nate Silver\nHow Charts Lie by Alberto Cairo\nList of books about data science ethics\n\n\n\nCategory 6: TidyTuesday\nYou may also participate in a TidyTuesday challenge. New data sets are announced on Monday afternoons.You can find more information about TidyTuesday and see the data in the TidyTuesday GitHub repo.\nA few guidelines:\n✅ Create a GitHub repo for your TidyTuesday submission. Your repo should include\n\nThe R Markdown file with all the code needed to reproduce your visualization.\nA README that includes an image of your final visualization and a short summary (~ 1 paragraph) about your visualization.\n\n✅ The visualization should include features or customization that are beyond what we’ve done in class .\n✅ Include the link to your GitHub repo in the slide summarizing your experience.\n\n\nCategory 7: Coding out loud\nWatch an episode of Coding out loud (either live or pre-recorded) and work through the project.\nA few guidelines:\n✅ Create a GitHub repo for your Coding out loud submission. Your repo should include\n\nThe Quarto file with all the code needed to reproduce your visualization.\nA README that includes an image of your final visualization and a short summary (~ 1 paragraph) about your visualization.\n\n✅ The final product (visualuzation, table, etc.) should include features or customization that are beyond what was achieved in the Coding out loud episode.\n✅ Include the link to your GitHub repo in the slide summarizing your experience."
  },
  {
    "objectID": "hw/hw-5.html#part-2-summarize-your-experience",
    "href": "hw/hw-5.html#part-2-summarize-your-experience",
    "title": "HW 5 - Statistics Experience",
    "section": "Part 2: Summarize your experience",
    "text": "Part 2: Summarize your experience\nMake one slide summarizing your experience. Submit the slide as a PDF on Gradescope.\nInclude the following on your slide:\n\nName and brief description of the event/podcast/competition/etc.\nSomething you found new, interesting, or unexpected\nHow the event/podcast/competition/etc. connects to something we’ve done in class.\nCitation or link to web page for event/competition/etc.\n\nClick here to see a template to help you get started on your slide. Your slide does not have to follow this exact format; it just needs to include the information mentioned above and be easily readable (i.e. use a reasonable font size!). Creativity is encouraged!"
  },
  {
    "objectID": "hw/hw-5.html#submission",
    "href": "hw/hw-5.html#submission",
    "title": "HW 5 - Statistics Experience",
    "section": "Submission",
    "text": "Submission\nSubmit the reflection as a PDF under the HW 5 - Statistics Experience assignment on Gradescope by Fri, Apr 15 at 5 pm ET. It must be submitted by the deadline on Gradescope to be considered for grading."
  },
  {
    "objectID": "hw/hw-3.html",
    "href": "hw/hw-3.html",
    "title": "HW 3 - Logistic regression and log transformation",
    "section": "",
    "text": "In this assignment, you’ll get to put into practice the logistic regression skills you’ve developed.\n\n\nIn this assignment, you will…\n\nFit and interpret logistic regression models.\nFit and interpret multiple linear regression models with log transformed outcomes.\nReason around log transformations of various types.\nContinue developing a workflow for reproducible data analysis.\n\n\n\n\nYour repo for this assignment is at github.com/sta210-s22 and starts with the prefix hw-3. For more detailed instructions on getting started, see HW 1.\n\n\n\nThe following packages will be used in this assignment. You can add other packages as needed.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(palmerpenguins)"
  },
  {
    "objectID": "hw/hw-3.html#introduction",
    "href": "hw/hw-3.html#introduction",
    "title": "HW 3 - Logistic regression and log transformation",
    "section": "",
    "text": "In this assignment, you’ll get to put into practice the logistic regression skills you’ve developed.\n\n\nIn this assignment, you will…\n\nFit and interpret logistic regression models.\nFit and interpret multiple linear regression models with log transformed outcomes.\nReason around log transformations of various types.\nContinue developing a workflow for reproducible data analysis.\n\n\n\n\nYour repo for this assignment is at github.com/sta210-s22 and starts with the prefix hw-3. For more detailed instructions on getting started, see HW 1.\n\n\n\nThe following packages will be used in this assignment. You can add other packages as needed.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(palmerpenguins)"
  },
  {
    "objectID": "hw/hw-3.html#part-1---palmer-penguins",
    "href": "hw/hw-3.html#part-1---palmer-penguins",
    "title": "HW 3 - Logistic regression and log transformation",
    "section": "Part 1 - Palmer penguins",
    "text": "Part 1 - Palmer penguins\nIn this part we’ll go back to the Palmer penguins dataset from HW 2.\nWe will use the following variables:\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nspecies\ninteger\nPenguin species (Adelie, Gentoo, Chinstrap)\n\n\nisland\ninteger\nIsland where recorded (Biscoe, Dream, Torgersen)\n\n\nflipper_length_mm\ninteger\nFlipper length in mm\n\n\n\nThe goal of this analysis is to use logistic regression to understand the relationship between flipper length, island, and whether a penguin is from the Adelie species. First, we need to create a new response variable to identify whether a penguin is from the Adelie species.\n\npenguins &lt;- penguins %&gt;%\n  mutate(adelie = factor(if_else(species == \"Adelie\", 1, 0)))\n\nAnd let’s check to make sure the new variable looks right before we continue with the analysis.\n\npenguins %&gt;%\n  count(adelie, species)\n\n# A tibble: 3 × 3\n  adelie species       n\n  &lt;fct&gt;  &lt;fct&gt;     &lt;int&gt;\n1 0      Chinstrap    68\n2 0      Gentoo      124\n3 1      Adelie      152\n\n\nLet’s start by looking at the relationship between island and whether a penguin is from the Adelie species.\n\nWhat does the values_fill argument do in the following chunk? The documentation for the function will be helpful in answering this question.\n\npenguins %&gt;%\n  count(island, adelie) %&gt;%\n  pivot_wider(names_from = adelie, values_from = n, values_fill = 0)\n\n# A tibble: 3 × 3\n  island      `0`   `1`\n  &lt;fct&gt;     &lt;int&gt; &lt;int&gt;\n1 Biscoe      124    44\n2 Dream        68    56\n3 Torgersen     0    52\n\n\nCalculate the odds ratio of a penguin being from the Adelie species for those recorded on Dream compared to those recorded on Biscoe.\nYou want to fit a model using island to predict the odds of being from the Adelie species. Let \\(\\pi\\) be the probability a penguin is from the Adelie species. The model has the following form. What do you expect the value of \\(\\hat{\\beta}_1\\), the estimated coefficient for Dream, to be? Explain your reasoning.\n\n\\[\n\\log\\Big(\\frac{\\pi}{1-\\pi}\\Big) = \\beta_0 + \\beta_1 ~ Dream + \\beta_2 ~ Torgersen\n\\]\n\nFit a model predicting adelie from island and display the model output. For the following exercise, use this model.\nBased on this model, what are the odds of a penguin being from the Adelie species if it was recorded on Biscoe island? on Dream island?\nNext, add flipper length to the model so that there are two predictors. Display the model output. For the following exercises, use this model.\nWrite the regression equation for the model.\nInterpret the coefficient of flipper_length_mm in terms of the log-odds of being from the Adelie species.\nInterpret the coefficient of flipper_length_mm in terms of the odds of being from the Adelie species.\nInterpret the coefficient of Dream in terms of the odds of being from the Adelie species.\nHow do you expect the log-odds of being from the Adelie species to change when going from a penguin with flipper length 185 mm to a penguin with flipper length 200 mm? Assume both penguins were recorded on the Dream island.\nHow do you expect the odds of being from the Adelie species to change when going from a penguin with flipper length 185 mm to a penguin with flipper length 200 mm? Assume both penguins were recorded on the Dream island."
  },
  {
    "objectID": "hw/hw-3.html#part-2---gdp-and-urban-population",
    "href": "hw/hw-3.html#part-2---gdp-and-urban-population",
    "title": "HW 3 - Logistic regression and log transformation",
    "section": "Part 2 - GDP and Urban population",
    "text": "Part 2 - GDP and Urban population\nData on countries’ Gross Domestic Product (GDP) and percentage of urban population was collected and made available by The World Bank in 2020. A description of the variables as defined by The World Bank are provided below.\n\nGDP: “GDP per capita is gross domestic product divided by midyear population. GDP is the sum of gross value added by all resident producers in the economy plus any product taxes and minus any subsidies not included in the value of the products. It is calculated without making deductions for depreciation of fabricated assets or for depletion and degradation of natural resources. Data are in current U.S. dollars.”\nUrban Population (% of total): “Urban population refers to people living in urban areas as defined by national statistical offices. It is calculated using World Bank population estimates and urban ratios from the United Nations World Urbanization Prospects.”\n\nThe data can be found in the data folder of your repository. Read the data and name it gdp_2020.\n\nFit a model predicting GDP from urban population. Then make a plot of residuals vs. fitted for this model. Does the linear model seem appropriate for modeling this relationship? Explain your reasoning.\nAdd a new column to the gdp_2020 dataset called gdp_log which is the (natural) log of gdp.\nFit a new model, predicting the log of GDP from urban population. Then make a plot of residuals vs. fitted for this model. Does the model predicting logged GDP or original GDP appear to be a better fit? Explain your reasoning.\n\nThe model output for predicting logged GDP.\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n6.107\n0.202\n30.291\n0\n\n\nurban\n0.042\n0.003\n13.769\n0\n\n\n\n\n\nThe linear model for predicting log of GDP can be expressed as follows:\n\\[\n\\widehat{\\log(GDP)} = 6.11 + 0.042 \\times urban\n\\]\nTherefore, the coefficient of urban (0.042) can be interpreted as the change in logged GDP associated with 1 percentage point increase in urban population. The problem is, logged GDP is not a very informative value to talk about. So we need to undo the transformation we’ve done.\nTo do so, let’s do a quick review of some properties of logs.\n\nSubtraction and logs: \\(log(a) − log(b) = log(\\frac{a}{b})\\)\nNatural logarithm: \\(e^{log(x)} = x\\)\n\nBased on the interpretation of the slope above, the difference between the predicted values of logged GDP for a given value of urban and a value that is 1 percentage point higher is 0.0425. Let’s write this out mathematically, and then use the properties we’ve listed above to work through the equation.\n\\[\n\\begin{aligned}\nlog(\\text{GDP for urban } x + 1) - log(\\text{GDP for urban } x) &= 0.042 \\\\\nlog\\Big( \\frac{\\text{GDP for urban } x + 1}{\\text{GDP for urban } x} \\Big) &= 0.042 \\\\\ne^{log\\Big( \\frac{\\text{GDP for urban } x + 1}{\\text{GDP for urban } x} \\Big)} &= e^{0.042}\\\\\n\\frac{\\text{GDP for urban } x + 1}{\\text{GDP for urban } x} &= e^{0.042}\n\\end{aligned}\n\\]\n\nBased on the derivation above, fill in the blanks in the following sentence for an alternative (and more useful interpretation) of the slope of urban.\n\nFor each additional percentage point the urban population is higher, the GDP of a country is expected to be ___, on average, by a factor of ___."
  },
  {
    "objectID": "hw/hw-3.html#submission",
    "href": "hw/hw-3.html#submission",
    "title": "HW 3 - Logistic regression and log transformation",
    "section": "Submission",
    "text": "Submission\n\n\n\n\n\n\nWarning\n\n\n\nBefore you wrap up the assignment, make sure all documents are updated on your GitHub repo. We will be checking these to make sure you have been practicing how to commit and push changes.\nRemember – you must turn in a PDF file to the Gradescope page before the submission deadline for full credit.\n\n\nTo submit your assignment:\n\nGo to http://www.gradescope.com and click Log in in the top right corner.\nClick School Credentials ➡️ Duke NetID and log in using your NetID credentials.\nClick on your STA 210 course.\nClick on the assignment, and you’ll be prompted to submit it.\nMark the pages associated with each exercise. All of the pages of your lab should be associated with at least one question (i.e., should be “checked”).\nSelect the first page of your PDF submission to be associated with the “Workflow & formatting” section."
  },
  {
    "objectID": "hw/hw-3.html#grading",
    "href": "hw/hw-3.html#grading",
    "title": "HW 3 - Logistic regression and log transformation",
    "section": "Grading",
    "text": "Grading\nTotal points available: 50 points.\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1 - 9\n45\n\n\nWorkflow & formatting\n51"
  },
  {
    "objectID": "hw/hw-3.html#footnotes",
    "href": "hw/hw-3.html#footnotes",
    "title": "HW 3 - Logistic regression and log transformation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe “Workflow & formatting” grade is to assess the reproducible workflow. This includes having at least 3 informative commit messages and updating the name and date in the YAML.↩︎"
  },
  {
    "objectID": "hw/hw-1.html",
    "href": "hw/hw-1.html",
    "title": "HW 1 - In-person voting trends",
    "section": "",
    "text": "In this assignment, you’ll use simple linear regression to explore the percent of votes cast in-person in the 2020 U.S. election based on the county’s political leanings.\n\n\nIn this assignment, you will…\n\nFit and interpret simple linear regression models\nAssess the conditions for simple linear regression.\nCreate and interpret spatial data visualizations using R.\nContinue developing a workflow for reproducible data analysis."
  },
  {
    "objectID": "hw/hw-1.html#introduction",
    "href": "hw/hw-1.html#introduction",
    "title": "HW 1 - In-person voting trends",
    "section": "",
    "text": "In this assignment, you’ll use simple linear regression to explore the percent of votes cast in-person in the 2020 U.S. election based on the county’s political leanings.\n\n\nIn this assignment, you will…\n\nFit and interpret simple linear regression models\nAssess the conditions for simple linear regression.\nCreate and interpret spatial data visualizations using R.\nContinue developing a workflow for reproducible data analysis."
  },
  {
    "objectID": "hw/hw-1.html#getting-started",
    "href": "hw/hw-1.html#getting-started",
    "title": "HW 1 - In-person voting trends",
    "section": "Getting started",
    "text": "Getting started\n\nLog in to RStudio\n\nGo to https://vm-manage.oit.duke.edu/containers and login with your Duke NetID and Password.\nClick STA210 to log into the Docker container. You should now see the RStudio environment.\n\n\n\nClone the repo & start new RStudio project\n\nGo to the course organization at github.com/sta210-s22 organization on GitHub. Click on the repo with the prefix hw-1. It contains the starter documents you need to complete the lab.\nClick on the green CODE button, select Use SSH (this might already be selected by default, and if it is, you’ll see the text Clone with SSH). Click on the clipboard icon to copy the repo URL.\nIn RStudio, go to File ➛ New Project ➛Version Control ➛ Git.\nCopy and paste the URL of your assignment repo into the dialog box Repository URL. Again, please make sure to have SSH highlighted under Clone when you copy the address.\nClick Create Project, and the files from your GitHub repo will be displayed in the Files pane in RStudio.\nClick hw-1-voting.qmd to open the template R Markdown file. This is where you will write up your code and narrative for the lab."
  },
  {
    "objectID": "hw/hw-1.html#packages",
    "href": "hw/hw-1.html#packages",
    "title": "HW 1 - In-person voting trends",
    "section": "Packages",
    "text": "Packages\nThe following packages will be used in this assignment:\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(scales)"
  },
  {
    "objectID": "hw/hw-1.html#data-2020-election",
    "href": "hw/hw-1.html#data-2020-election",
    "title": "HW 1 - In-person voting trends",
    "section": "Data: 2020 Election",
    "text": "Data: 2020 Election\nThere are multiple data sets for this assignment. Use the code below to load the data.\n\nelection_nc &lt;- read_csv(\"data/nc-election-2020.csv\") %&gt;%\n  mutate(fips = as.integer(FIPS))\ncounty_map_data &lt;-  read_csv(\"data/nc-county-map-data.csv\")\nelection_sample &lt;- read_csv(\"data/us-election-2020-sample.csv\")\n\nThe county-level election data in election_nc and election_sample are from The Economist GitHub repo. The data were originally analyzed in the July 2021 article In-person voting really did accelerate covid-19’s spread in America. For this analysis, we will focus on the following variables:\n\ninperson_pct: The proportion of a county’s votes cast in-person in the 2020 election\npctTrump_2016: The proportion of a county’s votes cast for Donald Trump in the 2016 election\n\nThe data in county_map_data were obtained from the maps package in R. We will not analyze any of the variables in this data set but will use it to help create maps in the assignment. Click here to see the documentation for the maps package. Click here for code examples."
  },
  {
    "objectID": "hw/hw-1.html#exercises",
    "href": "hw/hw-1.html#exercises",
    "title": "HW 1 - In-person voting trends",
    "section": "Exercises",
    "text": "Exercises\nDue to COVID-19 pandemic, many states made alternatives in-person voting, such as voting by mail, more widely available for the 2020 U.S. election. The general consensus was that voters who were more Democratic leaning would be more likely to vote by mail, while more Republican leaning voters would largely vote in-person. This was supported by multiple surveys, including this survey conducted by Pew Research.\nThe goal of this analysis is to use regression analysis to explore the relationship between a county’s political leanings and the proportion of votes cast in-person in 2020. The ultimate question we want to answer is “Did counties with more Republican leanings have a larger proportion of votes cast in-person in the 2020 election?”\nWe will use the proportion of votes cast for Donald Trump in 2016 (pctTrump_2016) as a measure of a county’s political leaning. Counties with a higher proportion of votes for Trump in 2016 are considered to have more Republican leanings.\n\n\n\n\n\n\nNote\n\n\n\nAll narrative should be written in complete sentences, and all visualizations should have informative titles and axis labels.\n\n\n\nPart 1: Counties in North Carolina\nFor this part of the analysis, we will focus on counties in North Carolina. We will use the data sets election_nc and county_map_data.\n\nVisualize the distribution of the response variable inperson_pct and calculate appropriate summary statistics. Use the visualization and summary statistics to describe the distribution. Include an informative title and axis labels on the plot.\nLet’s view the data in another way. Use the code below to make a map of North Carolina with the color of each county filled in based on the percentage of votes cast in-person in the 2020 election. Fill in title and axis labels.\nThen use the plot answer the following:\n\nWhat are 2 - 3 observations you have from the plot?\nWhat is a feature that is apparent in the map that wasn’t apparent from the histogram in the previous exercise? What is a feature that is apparent in the histogram that is not apparent in the map?\n\n\n\nelection_map_data &lt;- left_join(election_nc, county_map_data)\n\nggplot() +\n  geom_polygon(data = county_map_data,\n    mapping = aes(x = long, y = lat, group = group),\n    fill = \"lightgray\", color = \"white\"\n    ) +\n  geom_polygon(data = election_map_data, \n    mapping = aes(x = long, y = lat, group = group,\n    fill = inperson_pct)\n    ) +\n  labs(\n    x = \"___\",\n    y = \"___\",\n    fill = \"___\",\n    title = \"___\"\n  ) +\n  scale_fill_viridis_c(labels = label_percent(scale = 1)) +\n  coord_quickmap()\n\n\nCreate a visualization of the relationship between inperson_pct and pctTrump_2016. Use the visualization to describe the relationship between the two variables.\n\n\n\n\n\n\n\nWarning\n\n\n\nIf you haven’t yet done so, now is a good time to render your document and commit (with a meaningful commit message) and push all updates.\n\n\n\nWe can use a linear regression model to better quantify the relationship between the variables.\n\nFit the linear model to understand variability in the percent of in-person votes based on the percent of votes for Trump in the 2016 election. Neatly display the model output with 3 digits.\nWrite the regression equation using mathematical notation.\n\nNow let’s use the model coefficients to describe the relationship.\n\nInterpret the slope. The interpretation should be written in a way that is meaningful in the context of the data.\nDoes it make sense to interpret the intercept? If so, write the interpretation in the context of the data. Otherwise, briefly explain why not.\n\nIf the linear model is a good fit to these data, there should be no structure left in the residuals and the residuals should have constant variance. Augment the data with the model to obtain the residuals and predicted values for each observation, and call the augmented data frame nc_election_aug (You will use this name in Exercise 8). Then, make a plot of the residuals vs. the fitted values, and based on this plot, and provide a brief explanation for whether these two conditions are met. Hint: Zoom out on the plot by extending the limits of the y-axis.\n\n\n\n\n\n\n\nWarning\n\n\n\nNow is a good time to render your document again if you haven’t done so recently and commit (with a meaningful commit message) and push all updates.\n\n\n\nWe might also be interested in our observations being independent, particularly if we are to use these data for inference. To evaluate whether the independence condition is met, we will examine a map of the counties in North Carolina with the color filled based on the value of the residuals.\n\nBriefly explain why we may want to view the residuals on a map to assess independence.\nBriefly explain what pattern (if any) we would expect to observe on the map if the independence condition is satisfied.\n\nFill in the name of your model in the code below to calculate the residuals and add them to election_map_data. Then, a map with the color of each county filled in based on the value of the residual. Hint: Start with the code from Exercise 2.\nIs the independence condition satisfied? Briefly explain based on what you observe from the plot.\n\nnc_election_aug &lt;- nc_election_aug %&gt;% \n  bind_cols(fips = election_nc$fips)\n\nelection_map_data &lt;- left_join(election_map_data, nc_election_aug)\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBefore moving on to the next part, make sure you render your document and commit (with a meaningful commit message) and push all updates.\n\n\n\n\nPart 2: Inference for the U.S.\nTo get a better understanding of the trend across the entire United States, we analyze data from a random sample of 200 counties. This data is in the election_sample data frame. Because these counties were randomly selected out of the 3,006 counties in the United States, we can reasonably treat the counties as independent observations.\n\nFit the linear model to these sample data to understand variability in the percent of in-person votes based on the percent of votes for Trump in the 2016 election. Neatly display the model output with 3 digits.\nConduct a hypothesis test for the slope using a permutation test. In your response, state the null and alternative hypotheses in words, and state the conclusion in the context of the data.\nNext, construct a 95% confidence interval for the slope using bootstrapping. Interpret the confidence interval in the context of the data.\nComment on whether the hypothesis test and confidence interval support the general consensus that Republican voters were more likely to vote in-person in the 2020 election? A brief explanation is sufficient but it should be based on your conclusions from Exercises 10 and 11.\n\n\n\n\n\n\n\nWarning\n\n\n\nBefore submitting, make sure you render your document and commit (with a meaningful commit message) and push all updates."
  },
  {
    "objectID": "hw/hw-1.html#submission",
    "href": "hw/hw-1.html#submission",
    "title": "HW 1 - In-person voting trends",
    "section": "Submission",
    "text": "Submission\n\n\n\n\n\n\nWarning\n\n\n\nBefore you wrap up the assignment, make sure all documents are updated on your GitHub repo. We will be checking these to make sure you have been practicing how to commit and push changes.\nRemember – you must turn in a PDF file to the Gradescope page before the submission deadline for full credit.\n\n\nTo submit your assignment:\n\nGo to http://www.gradescope.com and click Log in in the top right corner.\nClick School Credentials ➡️ Duke NetID and log in using your NetID credentials.\nClick on your STA 210 course.\nClick on the assignment, and you’ll be prompted to submit it.\nMark the pages associated with each exercise. All of the pages of your lab should be associated with at least one question (i.e., should be “checked”).\nSelect the first page of your PDF submission to be associated with the “Workflow & formatting” section."
  },
  {
    "objectID": "hw/hw-1.html#grading",
    "href": "hw/hw-1.html#grading",
    "title": "HW 1 - In-person voting trends",
    "section": "Grading",
    "text": "Grading\nTotal points available: 50 points.\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1 - 10\n45\n\n\nWorkflow & formatting\n51"
  },
  {
    "objectID": "hw/hw-1.html#footnotes",
    "href": "hw/hw-1.html#footnotes",
    "title": "HW 1 - In-person voting trends",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe “Workflow & formatting” grade is to assess the reproducible workflow. This includes having at least 3 informative commit messages and updating the name and date in the YAML.↩︎"
  },
  {
    "objectID": "ae/ae-11-volcanoes.html",
    "href": "ae/ae-11-volcanoes.html",
    "title": "AE 11: Multinomial classification",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate the repo titled ae-11-volcanoes-YOUR_GITHUB_USERNAME to get started."
  },
  {
    "objectID": "ae/ae-11-volcanoes.html#packages",
    "href": "ae/ae-11-volcanoes.html#packages",
    "title": "AE 11: Multinomial classification",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(colorblindr)"
  },
  {
    "objectID": "ae/ae-11-volcanoes.html#data",
    "href": "ae/ae-11-volcanoes.html#data",
    "title": "AE 11: Multinomial classification",
    "section": "Data",
    "text": "Data\nFor this application exercise we will work with a dataset of on volcanoes. The data come from The Smithsonian Institution via TidyTuesday.\n\nvolcano &lt;- read_csv(here::here(\"ae\", \"data/volcano.csv\"))\n\nRows: 958 Columns: 26\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (18): volcano_name, primary_volcano_type, last_eruption_year, country, r...\ndbl  (8): volcano_number, latitude, longitude, elevation, population_within_...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nFirst, a bit of data prep:\n\nvolcano &lt;- volcano %&gt;%\n  mutate(\n    volcano_type = case_when(\n      str_detect(primary_volcano_type, \"Stratovolcano\") ~ \"Stratovolcano\",\n      str_detect(primary_volcano_type, \"Shield\") ~ \"Shield\",\n      TRUE ~ \"Other\"\n    ),\n    volcano_type = fct_relevel(volcano_type, \"Stratovolcano\", \"Shield\", \"Other\")\n  ) %&gt;%\n  select(\n    volcano_type, latitude, longitude, \n    elevation, tectonic_settings, major_rock_1\n    ) %&gt;%\n  mutate(across(where(is.character), as_factor))"
  },
  {
    "objectID": "ae/ae-11-volcanoes.html#exploratory-data-analysis",
    "href": "ae/ae-11-volcanoes.html#exploratory-data-analysis",
    "title": "AE 11: Multinomial classification",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\n\nCreate a map of volcanoes that is faceted by volcano_type.\n\n\nworld &lt;- map_data(\"world\")\n\nworld_map &lt;- ggplot() +\n  geom_polygon(\n    data = world, \n    aes(\n      x = long, y = lat, group = group),\n      color = \"white\", fill = \"gray50\", \n      size = 0.05, alpha = 0.2\n    ) +\n  theme_minimal() +\n  coord_quickmap() +\n  labs(x = NULL, y = NULL)\n\nworld_map +\n  geom_point(\n    data = volcano,\n    aes(x = longitude, y = latitude,\n        color = volcano_type, \n        shape = volcano_type),\n    alpha = 0.5\n  ) +\n  facet_wrap(~volcano_type) +\n  scale_color_OkabeIto()"
  },
  {
    "objectID": "ae/ae-11-volcanoes.html#build-a-new-model",
    "href": "ae/ae-11-volcanoes.html#build-a-new-model",
    "title": "AE 11: Multinomial classification",
    "section": "Build a new model",
    "text": "Build a new model\n\nBuild a new model that uses a recipe that includes geographic information (latitude and longitude). How does this model compare to the original? Note:\nUse the same test/train split as well as same cross validation folds. Code for these is provided below.\n\n\n# test/train split\nset.seed(1234)\n\nvolcano_split &lt;- initial_split(volcano)\nvolcano_train &lt;- training(volcano_split)\nvolcano_test  &lt;- testing(volcano_split)\n\n# cv folds\nset.seed(9876)\n\nvolcano_folds &lt;- vfold_cv(volcano_train, v = 5)\nvolcano_folds\n\n#  5-fold cross-validation \n# A tibble: 5 × 2\n  splits            id   \n  &lt;list&gt;            &lt;chr&gt;\n1 &lt;split [574/144]&gt; Fold1\n2 &lt;split [574/144]&gt; Fold2\n3 &lt;split [574/144]&gt; Fold3\n4 &lt;split [575/143]&gt; Fold4\n5 &lt;split [575/143]&gt; Fold5\n\n\nNew recipe, including geographic information:\n\nvolcano_rec2 &lt;- recipe(volcano_type ~ ., data = volcano_train) %&gt;%\n  step_other(tectonic_settings) %&gt;%\n  step_other(major_rock_1) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_zv(all_predictors()) %&gt;%\n  step_center(all_predictors())\n\nOriginal model specification and new workflow:\n\nvolcano_spec &lt;- multinom_reg() %&gt;%\n  set_engine(\"nnet\")\n\nvolcano_wflow2 &lt;- workflow() %&gt;%\n  add_recipe(volcano_rec2) %&gt;%\n  add_model(volcano_spec)\n\nvolcano_wflow2\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: multinom_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_other()\n• step_other()\n• step_dummy()\n• step_zv()\n• step_center()\n\n── Model ───────────────────────────────────────────────────────────────────────\nMultinomial Regression Model Specification (classification)\n\nComputational engine: nnet \n\n\nFit resamples:\n\nvolcano_fit_rs2 &lt;- volcano_wflow2 %&gt;%\n  fit_resamples(\n    volcano_folds, \n    control = control_resamples(save_pred = TRUE)\n    )\n\nvolcano_fit_rs2\n\n# Resampling results\n# 5-fold cross-validation \n# A tibble: 5 × 5\n  splits            id    .metrics         .notes           .predictions      \n  &lt;list&gt;            &lt;chr&gt; &lt;list&gt;           &lt;list&gt;           &lt;list&gt;            \n1 &lt;split [574/144]&gt; Fold1 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 1]&gt; &lt;tibble [144 × 7]&gt;\n2 &lt;split [574/144]&gt; Fold2 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 1]&gt; &lt;tibble [144 × 7]&gt;\n3 &lt;split [574/144]&gt; Fold3 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 1]&gt; &lt;tibble [144 × 7]&gt;\n4 &lt;split [575/143]&gt; Fold4 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 1]&gt; &lt;tibble [143 × 7]&gt;\n5 &lt;split [575/143]&gt; Fold5 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 1]&gt; &lt;tibble [143 × 7]&gt;\n\n\nCollect metrics:\n\ncollect_metrics(volcano_fit_rs2)\n\n# A tibble: 2 × 6\n  .metric  .estimator  mean     n std_err .config             \n  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy multiclass 0.606     5  0.0138 Preprocessor1_Model1\n2 roc_auc  hand_till  0.695     5  0.0245 Preprocessor1_Model1\n\n\nROC curves:\n\nvolcano_fit_rs2 %&gt;%\n  collect_predictions() %&gt;%\n  group_by(id) %&gt;%\n  roc_curve(\n    truth = volcano_type,\n    .pred_Stratovolcano:.pred_Other\n  ) %&gt;%\n  autoplot()"
  },
  {
    "objectID": "ae/ae-11-volcanoes.html#roc-curves",
    "href": "ae/ae-11-volcanoes.html#roc-curves",
    "title": "AE 11: Multinomial classification",
    "section": "ROC curves",
    "text": "ROC curves\n\nRecreate the ROC curve from the slides.\n\n\nfinal_fit &lt;- last_fit(\n  volcano_wflow2, \n  split = volcano_split\n  )\n\ncollect_predictions(final_fit) %&gt;%\n  roc_curve(truth = volcano_type, .pred_Stratovolcano:.pred_Other) %&gt;%\n  ggplot(aes(x = 1 - specificity, y = sensitivity, color = .level)) +\n  geom_path(size = 1) +\n  scale_color_OkabeIto() +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"gray\") +\n  theme_minimal() +\n  labs(color = NULL)"
  },
  {
    "objectID": "ae/ae-11-volcanoes.html#acknowledgement",
    "href": "ae/ae-11-volcanoes.html#acknowledgement",
    "title": "AE 11: Multinomial classification",
    "section": "Acknowledgement",
    "text": "Acknowledgement\nThis exercise was inspired by https://juliasilge.com/blog/multinomial-volcano-eruptions."
  },
  {
    "objectID": "ae/ae-6-the-office-cv.html",
    "href": "ae/ae-6-the-office-cv.html",
    "title": "AE 6: The Office",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate the repo titled ae-6-the-office-cv-YOUR_GITHUB_USERNAME to get started."
  },
  {
    "objectID": "ae/ae-6-the-office-cv.html#packages",
    "href": "ae/ae-6-the-office-cv.html#packages",
    "title": "AE 6: The Office",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)"
  },
  {
    "objectID": "ae/ae-6-the-office-cv.html#load-data",
    "href": "ae/ae-6-the-office-cv.html#load-data",
    "title": "AE 6: The Office",
    "section": "Load data",
    "text": "Load data\n\noffice_episodes &lt;- read_csv(\"data/office_episodes.csv\")"
  },
  {
    "objectID": "ae/ae-6-the-office-cv.html#split-data-into-training-and-testing",
    "href": "ae/ae-6-the-office-cv.html#split-data-into-training-and-testing",
    "title": "AE 6: The Office",
    "section": "Split data into training and testing",
    "text": "Split data into training and testing\nSplit your data into testing and training sets.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-6-the-office-cv.html#specify-model",
    "href": "ae/ae-6-the-office-cv.html#specify-model",
    "title": "AE 6: The Office",
    "section": "Specify model",
    "text": "Specify model\nSpecify a linear regression model. Call it office_spec.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-6-the-office-cv.html#create-recipe",
    "href": "ae/ae-6-the-office-cv.html#create-recipe",
    "title": "AE 6: The Office",
    "section": "Create recipe",
    "text": "Create recipe\nCreate the recipe from class. Call it office_rec1.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-6-the-office-cv.html#create-workflow",
    "href": "ae/ae-6-the-office-cv.html#create-workflow",
    "title": "AE 6: The Office",
    "section": "Create workflow",
    "text": "Create workflow\nCreate the workflow that brings together the model specification and recipe. Call it office_wflow1.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-6-the-office-cv.html#cross-validation",
    "href": "ae/ae-6-the-office-cv.html#cross-validation",
    "title": "AE 6: The Office",
    "section": "Cross validation",
    "text": "Cross validation\nConduct 10-fold cross validation.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-6-the-office-cv.html#summarize-cv-metrics",
    "href": "ae/ae-6-the-office-cv.html#summarize-cv-metrics",
    "title": "AE 6: The Office",
    "section": "Summarize CV metrics",
    "text": "Summarize CV metrics\nSummarize metrics from your CV resamples.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-6-the-office-cv.html#another-model---model-2",
    "href": "ae/ae-6-the-office-cv.html#another-model---model-2",
    "title": "AE 6: The Office",
    "section": "Another model - Model 2",
    "text": "Another model - Model 2\nCreate a different (simpler, involving fewer variables) recipe and call it office_rec2. Conduct 10-fold cross validation and summarize metrics. Describe how the two models compare to each other based on cross validation metrics."
  },
  {
    "objectID": "ae/ae-1-dcbikeshare.html",
    "href": "ae/ae-1-dcbikeshare.html",
    "title": "AE 02: Bike rentals in DC",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate the repo titled ae-1-dcbikeshare-YOUR_GITHUB_USERNAME to get started."
  },
  {
    "objectID": "ae/ae-1-dcbikeshare.html#bike-rentals-in-dc",
    "href": "ae/ae-1-dcbikeshare.html#bike-rentals-in-dc",
    "title": "AE 02: Bike rentals in DC",
    "section": "Bike rentals in DC",
    "text": "Bike rentals in DC\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(patchwork)"
  },
  {
    "objectID": "ae/ae-1-dcbikeshare.html#data",
    "href": "ae/ae-1-dcbikeshare.html#data",
    "title": "AE 02: Bike rentals in DC",
    "section": "Data",
    "text": "Data\nOur dataset contains daily rentals from the Capital Bikeshare in Washington, DC in 2011 and 2012. It was obtained from the dcbikeshare data set in the dsbox R package.\nWe will focus on the following variables in the analysis:\n\ncount: total bike rentals\ntemp_orig: Temperature in degrees Celsius\nseason: 1 - winter, 2 - spring, 3 - summer, 4 - fall\n\nClick here for the full list of variables and definitions.\n\nbikeshare &lt;- read_csv(\"data/dcbikeshare.csv\")"
  },
  {
    "objectID": "ae/ae-1-dcbikeshare.html#daily-counts-and-temperature",
    "href": "ae/ae-1-dcbikeshare.html#daily-counts-and-temperature",
    "title": "AE 02: Bike rentals in DC",
    "section": "Daily counts and temperature",
    "text": "Daily counts and temperature\n\nExercise 1\nVisualize the distribution of daily bike rentals and temperature as well as the relationship between these two variables.\n\nggplot(bikeshare, aes(x = count)) +\n  geom_histogram(binwidth = 250)\n\n\n\n\n\n\n\nggplot(bikeshare, aes(y = count, x = temp_orig)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\nExercise 2\nDescribe the distribution of daily bike rentals and the distribution of temperature based on the visualizations created in Exercise 1. Include the shape, center, spread, and presence of any potential outliers.\n[Add your answer here]\n\n\nExercise 3\nThere appears to be one day with a very small number of bike rentals. What was the day? Why were the number of bike rentals so low on that day? Hint: You can Google the date to figure out what was going on that day.\n[Add your answer here]\n\n\nExercise 4\nDescribe the relationship between daily bike rentals and temperature based on the visualization created in Exercise 1. Comment on how we expect the number of bike rentals to change as the temperature increases.\n[Add your answer here]\n\n\nExercise 5\nSuppose you want to fit a model so you can use the temperature to predict the number of bike rentals. Would a model of the form\n\\[\\text{count} = \\beta_0 + \\beta_1 ~ \\text{temp\\_orig} + \\epsilon\\]\nbe the best fit for the data? Why or why not?\nNo."
  },
  {
    "objectID": "ae/ae-1-dcbikeshare.html#daily-counts-temperature-and-season",
    "href": "ae/ae-1-dcbikeshare.html#daily-counts-temperature-and-season",
    "title": "AE 02: Bike rentals in DC",
    "section": "Daily counts, temperature, and season",
    "text": "Daily counts, temperature, and season\n\nExercise 6\nIn the raw data, seasons are coded as 1, 2, 3, 4 as numerical values, corresponding to winter, spring, summer, and fall respectively. Recode the season variable to make it a categorical variable (a factor) with levels corresponding to season names, making sure that the levels appear in a reasonable order in the variable (i.e., not alphabetical).\n\n# add code developed during livecoding here\n\n\n\nExercise 7\nNext, let’s look at how the daily bike rentals differ by season. Let’s visualize the distribution of bike rentals by season using density plots. You can think of a density plot as a “smoothed out histogram”. Compare and contrast the distributions. Is this what you expected? Why or why not?\n\n# add code developed during livecoding here\n\n[Add your answer here]\n\n\nExercise 8\nWe want to evaluate whether the relationship between temperature and daily bike rentals is the same for each season. To answer this question, first create a scatter plot of daily bike rentals vs. temperature faceted by season.\n\n# add code developed during livecoding here\n\n\n\nExercise 9\n\nWhich season appears to have the strongest relationship between temperature and daily bike rentals? Why do you think the relationship is strongest in this season?\nWhich season appears to have the weakest relationship between temperature and daily bike rentals? Why do you think the relationship is weakest in this season?\n\n[Add your answer here]"
  },
  {
    "objectID": "ae/ae-1-dcbikeshare.html#modeling",
    "href": "ae/ae-1-dcbikeshare.html#modeling",
    "title": "AE 02: Bike rentals in DC",
    "section": "Modeling",
    "text": "Modeling\n\nExercise 10\nFilter your data for the season with the strongest apparent relationship between temperature and daily bike rentals.\n\n# add code developed during livecoding here\n\n\n\nExercise 11\nUsing the data you filtered in Exercise 10, fit a linear model for predicting daily bike rentals from temperature for this season.\n\n# add code developed during livecoding here\n\n\n\nExercise 12\nUse the output to write out the estimated regression equation.\n[Add your answer here]\n\n\nExercise 13\nInterpret the slope in the context of the data.\n[Add your answer here]\n\n\nExercise 14\nInterpret the intercept in the context of the data.\n[Add your answer here]"
  },
  {
    "objectID": "ae/ae-1-dcbikeshare.html#synthesis",
    "href": "ae/ae-1-dcbikeshare.html#synthesis",
    "title": "AE 02: Bike rentals in DC",
    "section": "Synthesis",
    "text": "Synthesis\n\nExercise 15\nSuppose you work for a bike share company in Durham, NC, and they want to predict daily bike rentals in 2022. What is one reason you might recommend they use your analysis for this task? What is one reason you would recommend they not use your analysis for this task?\n[Add your answer here]\n\nThe following exercises will be completed only if time permits.\n\n\nExercise 16\nPick another season. Based on the visualization in Exercise 8, would you expect the slope of the relationship between temperature and daily bike rentals to be smaller or larger than the slope of the model you’ve been working with so far? Explain your reasoning.\n[Add your answer here]\n\n\nExercise 17\nFor this season you picked in Exercise 16, fit a linear model for predicting daily bike rentals from temperature. Note, you will need to filter your data for this season first. Use the output to write out the estimated regression equation and interpret the slope and the intercept of this model.\n\n# add your code here\n\n[Add your answer here]"
  },
  {
    "objectID": "ae/ae-9-odds.html",
    "href": "ae/ae-9-odds.html",
    "title": "AE 9: Odds",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate the repo titled ae-9-odds-YOUR_GITHUB_USERNAME to get started."
  },
  {
    "objectID": "ae/ae-9-odds.html#packages",
    "href": "ae/ae-9-odds.html#packages",
    "title": "AE 9: Odds",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\n\nheart_disease &lt;- read_csv(here::here(\"ae\", \"data/framingham.csv\")) %&gt;%\n  select(totChol, TenYearCHD) %&gt;%\n  drop_na() %&gt;%\n  mutate(high_risk = as.factor(TenYearCHD)) %&gt;%\n  select(totChol, high_risk)"
  },
  {
    "objectID": "ae/ae-9-odds.html#linear-regression-vs.-logistic-regression",
    "href": "ae/ae-9-odds.html#linear-regression-vs.-logistic-regression",
    "title": "AE 9: Odds",
    "section": "Linear regression vs. logistic regression",
    "text": "Linear regression vs. logistic regression\nState whether a linear regression model or logistic regression model is more appropriate for each scenario:\n\nUse age and education to predict if a randomly selected person will vote in the next election.\nUse budget and run time (in minutes) to predict a movie’s total revenue.\nUse age and sex to calculate the probability a randomly selected adult will visit Duke Health in the next year."
  },
  {
    "objectID": "ae/ae-9-odds.html#heart-disease",
    "href": "ae/ae-9-odds.html#heart-disease",
    "title": "AE 9: Odds",
    "section": "Heart disease",
    "text": "Heart disease\n\nData: Framingham study\nThis data set is from an ongoing cardiovascular study on residents of the town of Framingham, Massachusetts. We want to use the total cholesterol to predict if a randomly selected adult is high risk for heart disease in the next 10 years.\n\nhigh_risk:\n\n1: High risk of having heart disease in next 10 years\n0: Not high risk of having heart disease in next 10 years\n\ntotChol: total cholesterol (mg/dL)\n\n\n\nOutcome: high_risk\n\nggplot(data = heart_disease, aes(x = high_risk)) + \n  geom_bar() + \n  scale_x_discrete(labels = c(\"1\" = \"High risk\", \"0\" = \"Low risk\")) +\n  labs(\n    title = \"Distribution of 10-year risk of heart disease\", \n    x = NULL)\n\n\n\n\n\n\n\n\n\nheart_disease %&gt;%\n  count(high_risk)\n\n# A tibble: 2 × 2\n  high_risk     n\n  &lt;fct&gt;     &lt;int&gt;\n1 0          3555\n2 1           635\n\n\n\n\nCalculating probability and odds\n\nWhat is the probability a randomly selected person in the study is not high risk for heart disease?\nWhat are the odds a randomly selected person in the study is not high risk for heart disease?\n\n\n\nLogistic regression model\nFit a logistic regression model to understand the relationship between total cholesterol and risk for heart disease.\nLet \\(pi\\) be the probability an adult is high risk. The statistical model is\n\\[\\log\\Big(\\frac{\\pi_i}{1-\\pi_i}\\Big) = \\beta_0 + \\beta_1 TotChol_i\\]\n\nheart_disease_fit &lt;- logistic_reg() %&gt;%\n  set_engine(\"glm\") %&gt;%\n  fit(high_risk ~ totChol, data = heart_disease, family = \"binomial\")\n\ntidy(heart_disease_fit) %&gt;% kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-2.894\n0.230\n-12.607\n0\n\n\ntotChol\n0.005\n0.001\n5.268\n0\n\n\n\n\n\n\nWrite the regression equation. Round to 3 digits.\n\n\n\nCalculating log-odds, odds and probabilities\nBased on the model, if a randomly selected person has a total cholesterol of 250 mg/dL,\n\nWhat are the log-odds they are high risk for heart disease?\nWhat are the odds they are high risk for heart disease?\nWhat is the probability they are high risk for heart disease? Use the odds to calculate your answer.\n\n\n\nComparing observations\nSuppose a person’s cholesterol changes from 250 mg/dL to 200 mg/dL.\n\nHow do you expect the log-odds that this person is high risk for heart disease to change?\nHow do you expect the odds that this person is high risk for heart disease to change?"
  },
  {
    "objectID": "ae/ae-0-movies.html",
    "href": "ae/ae-0-movies.html",
    "title": "Movie budgets and revenues",
    "section": "",
    "text": "Important\n\n\n\nThis application exercise is a demo only. You do not have a corresponding repository for it and you’re not expected to turn in anything for it.\nWe will look at the relationship between budget and revenue for movies made in the United States in 1986 to 2020. The dataset is created based on data from the Internet Movie Database (IMDB).\nlibrary(tidyverse) # for data analysis and visualisation\nlibrary(scales)    # for pretty axis labels\nlibrary(DT)        # for interactive table"
  },
  {
    "objectID": "ae/ae-0-movies.html#data",
    "href": "ae/ae-0-movies.html#data",
    "title": "Movie budgets and revenues",
    "section": "Data",
    "text": "Data\nThe movies data set includes basic information about each movie including budget, genre, movie studio, director, etc. A full list of the variables may be found here.\n\nmovies &lt;- read_csv(\"https://raw.githubusercontent.com/danielgrijalva/movie-stats/master/movies.csv\")\n\nView the first 10 rows of data.\n\nmovies\n\n# A tibble: 7,668 × 15\n   name   rating genre  year released score  votes director writer star  country\n   &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;  \n 1 The S… R      Drama  1980 June 13…   8.4 9.27e5 Stanley… Steph… Jack… United…\n 2 The B… R      Adve…  1980 July 2,…   5.8 6.5 e4 Randal … Henry… Broo… United…\n 3 Star … PG     Acti…  1980 June 20…   8.7 1.2 e6 Irvin K… Leigh… Mark… United…\n 4 Airpl… PG     Come…  1980 July 2,…   7.7 2.21e5 Jim Abr… Jim A… Robe… United…\n 5 Caddy… R      Come…  1980 July 25…   7.3 1.08e5 Harold … Brian… Chev… United…\n 6 Frida… R      Horr…  1980 May 9, …   6.4 1.23e5 Sean S.… Victo… Bets… United…\n 7 The B… R      Acti…  1980 June 20…   7.9 1.88e5 John La… Dan A… John… United…\n 8 Ragin… R      Biog…  1980 Decembe…   8.2 3.3 e5 Martin … Jake … Robe… United…\n 9 Super… PG     Acti…  1980 June 19…   6.8 1.01e5 Richard… Jerry… Gene… United…\n10 The L… R      Biog…  1980 May 16,…   7   1   e4 Walter … Bill … Davi… United…\n# … with 7,658 more rows, and 4 more variables: budget &lt;dbl&gt;, gross &lt;dbl&gt;,\n#   company &lt;chr&gt;, runtime &lt;dbl&gt;\n\n\nThe ___ dataset has ___ observations and ___ variables."
  },
  {
    "objectID": "ae/ae-0-movies.html#analysis",
    "href": "ae/ae-0-movies.html#analysis",
    "title": "Movie budgets and revenues",
    "section": "Analysis",
    "text": "Analysis\n\nGross over time\nWe begin by looking at how the average gross revenue (gross) has changed over time. Since we want to visualize the results, we will choose a few genres of interest for the analysis.\n\ngenre_list &lt;- c(\"Comedy\", \"Action\", \"Animation\", \"Horror\")\n\nThen, we will filter for these genres and visualize the average gross revenue over time.\n\nmovies %&gt;%\n  filter(genre %in% genre_list) %&gt;% \n  group_by(genre,year) %&gt;%\n  summarise(avg_gross = mean(gross)) %&gt;%\n  ggplot(mapping = aes(x = year, y = avg_gross, color= genre)) +\n    geom_point() + \n    geom_line() +\n    scale_color_viridis_d() +\n    scale_y_continuous(labels = label_dollar()) +\n    labs(\n      x = \"Year\",\n      y = \"Average Gross Revenue (US Dollars)\",\n      color = \"Genre\",\n      title = \"Gross Revenue Over Time\"\n    )\n\n`summarise()` has grouped output by 'genre'. You can override using the\n`.groups` argument.\n\n\nWarning: Removed 47 rows containing missing values (geom_point).\n\n\nWarning: Removed 23 row(s) containing missing values (geom_path).\n\n\n\n\n\n\n\n\n\nThe plot suggests …\n\n\nBudget and gross\nNext, let’s see the relationship between a movie’s budget and its gross revenue.\n\nmovies %&gt;%\n  filter(genre %in% genre_list, budget &gt; 0) %&gt;% \n  ggplot(mapping = aes(x=log(budget), y = log(gross), color=genre)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  facet_wrap(~ genre) + \n  scale_color_viridis_d() +\n  labs(\n    x = \"Log-transformed Budget\",\n    y = \"Log-transformed Gross Revenue\"\n  )\n\n`geom_smooth()` using formula 'y ~ x'\n\n\nWarning: Removed 35 rows containing non-finite values (stat_smooth).\n\n\nWarning: Removed 35 rows containing missing values (geom_point)."
  },
  {
    "objectID": "ae/ae-0-movies.html#exercises",
    "href": "ae/ae-0-movies.html#exercises",
    "title": "Movie budgets and revenues",
    "section": "Exercises",
    "text": "Exercises\n\nSuppose we fit a regression model for each genre that uses budget to predict gross revenue. What are the signs of the correlation between budget and gross and the slope in each regression equation?\nSuppose we fit the regression model from the previous question. Which genre would you expect to have the smallest residuals, on average (residual = observed revenue - predicted revenue)?\nIn the remaining time, discuss the following: Notice in the graph above that budget and gross are log-transformed. Why are the log-transformed values of the variables displayed rather than the original values (in U.S. dollars)?"
  },
  {
    "objectID": "ae/ae-0-movies.html#appendix",
    "href": "ae/ae-0-movies.html#appendix",
    "title": "Movie budgets and revenues",
    "section": "Appendix",
    "text": "Appendix\nBelow is a list of genres in the data set:\n\nmovies %&gt;% \n  distinct(genre) %&gt;%\n  arrange(genre) %&gt;% \n  datatable()"
  },
  {
    "objectID": "ae/ae-2-dcbikeshare.html",
    "href": "ae/ae-2-dcbikeshare.html",
    "title": "AE 2: Bike rentals in DC (continued)",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate the repo titled ae-2-dcbikeshare-YOUR_GITHUB_USERNAME to get started."
  },
  {
    "objectID": "ae/ae-2-dcbikeshare.html#bike-rentals-in-dc",
    "href": "ae/ae-2-dcbikeshare.html#bike-rentals-in-dc",
    "title": "AE 2: Bike rentals in DC (continued)",
    "section": "Bike rentals in DC",
    "text": "Bike rentals in DC\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(patchwork)"
  },
  {
    "objectID": "ae/ae-2-dcbikeshare.html#data",
    "href": "ae/ae-2-dcbikeshare.html#data",
    "title": "AE 2: Bike rentals in DC (continued)",
    "section": "Data",
    "text": "Data\nOur dataset contains daily rentals from the Capital Bikeshare in Washington, DC in 2011 and 2012. It was obtained from the dcbikeshare data set in the dsbox R package.\nWe will focus on the following variables in the analysis:\n\ncount: total bike rentals\ntemp_orig: Temperature in degrees Celsius\nseason: 1 - winter, 2 - spring, 3 - summer, 4 - fall\n\nClick here for the full list of variables and definitions.\n\nbikeshare &lt;- read_csv(\"data/dcbikeshare.csv\")\n\nSee AE 1 for the first part of this analysis."
  },
  {
    "objectID": "ae/ae-2-dcbikeshare.html#daily-counts-temperature-and-season",
    "href": "ae/ae-2-dcbikeshare.html#daily-counts-temperature-and-season",
    "title": "AE 2: Bike rentals in DC (continued)",
    "section": "Daily counts, temperature, and season",
    "text": "Daily counts, temperature, and season\n\nExercise 1\nIn the raw data, seasons are coded as 1, 2, 3, 4 as numerical values, corresponding to winter, spring, summer, and fall respectively. Recode the season variable to make it a categorical variable (a factor) with levels corresponding to season names, making sure that the levels appear in a reasonable order in the variable (i.e., not alphabetical).\n\n# add code developed during livecoding here\n\n\n\nExercise 2\nNext, let’s look at how the daily bike rentals differ by season. Let’s visualize the distribution of bike rentals by season using density plots. You can think of a density plot as a “smoothed out histogram”. Compare and contrast the distributions. Is this what you expected? Why or why not?\n\n# add code developed during livecoding here\n\n[Add your answer here]\n\n\nExercise 3\nWe want to evaluate whether the relationship between temperature and daily bike rentals is the same for each season. To answer this question, first create a scatter plot of daily bike rentals vs. temperature faceted by season.\n\n# add code developed during livecoding here\n\n\n\nExercise 4\n\nWhich season appears to have the strongest relationship between temperature and daily bike rentals? Why do you think the relationship is strongest in this season?\nWhich season appears to have the weakest relationship between temperature and daily bike rentals? Why do you think the relationship is weakest in this season?\n\n[Add your answer here]"
  },
  {
    "objectID": "ae/ae-2-dcbikeshare.html#modeling",
    "href": "ae/ae-2-dcbikeshare.html#modeling",
    "title": "AE 2: Bike rentals in DC (continued)",
    "section": "Modeling",
    "text": "Modeling\n\nExercise 5\nFilter your data for the season with the strongest apparent relationship between temperature and daily bike rentals.\n\n# add code developed during livecoding here\n\n\n\nExercise 6\nUsing the data you filtered in Exercise 5, fit a linear model for predicting daily bike rentals from temperature for this season.\n\n# add code developed during livecoding here"
  },
  {
    "objectID": "ae/ae-2-dcbikeshare.html#synthesis",
    "href": "ae/ae-2-dcbikeshare.html#synthesis",
    "title": "AE 2: Bike rentals in DC (continued)",
    "section": "Synthesis",
    "text": "Synthesis\n\nExercise 7\nSuppose you work for a bike share company in Durham, NC, and they want to predict daily bike rentals in 2022. What is one reason you might recommend they use your analysis for this task? What is one reason you would recommend they not use your analysis for this task?\n[Add your answer here]"
  },
  {
    "objectID": "NOTE.html",
    "href": "NOTE.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "Index page - following quercus link give error and say it is no longer supported - piazza link leads to just defauly piazza page\nSchedule - for tutorials, noticed it was in slide format, so ask prof wether he would want it to also be converted to qmd slides"
  },
  {
    "objectID": "NOTE.html#publishing-pages",
    "href": "NOTE.html#publishing-pages",
    "title": "CSC477 - Fall 2024",
    "section": "Publishing pages",
    "text": "Publishing pages\n\nsince gh-page branch is not set up need to follow below instructions\n\ngit checkout –orphan gh-pages git reset –hard # make sure all changes are committed before running this! -&gt; git reset means resetting the current branch to the most recent commit git commit –allow-empty -m “Initialising gh-pages branch” git push origin gh-pages"
  },
  {
    "objectID": "ex/w03/exercises03_solution.html",
    "href": "ex/w03/exercises03_solution.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "Assume you are given datapoints \\((\\fx_i)_{i=1}^N\\) with \\(\\fx_i\\in\\R^n\\) coming from a Gaussian distribution. Derive the maximum likelihood estimator of its mean.\n\n\nFirst, let’s quickly remember that the maximum likelihood estimator (MLE) of a probability distribution from dataapoints \\(\\fx_1, \\ldots, \\fx_N\\) is given by \\[\n\\hte_{\\mathrm{MLE}} = \\argmax_{\\te \\in \\Te} \\prod_{i=1}^N f(\\fx_i | \\te),\n\\] where \\(f\\) is the probability density function of the considered probability distribution family, \\(\\te\\) are the parameters of the distribution, and \\(\\Te\\) is the parameter space (a set containing all possible parameters). The product on the right hand side is also known as the likelihood function. In practice, we usually work with the log-likelihood function instead. Because the logarithm is monotonously increasing, the resulting estimators are the same, i.e. \\[\n\\hte_{\\mathrm{MLE}}\n= \\argmax_{\\te \\in \\Te} \\prod_{i=1}^N f(\\fx_i | \\te),\n= \\argmax_{\\te \\in \\Te} \\sum_{i=1}^N \\log \\li( f(\\fx_i | \\te)\\ri).\n\\]\nSecond, let’s remember that the probability density function of a multivariate Gaussian distribution is given by \\[\nf(\\fx_i | \\mu, \\Si)\n= \\frac{1}{(2 \\pi)^{n/2} |\\Si|^{1/2}}\n   \\exp \\li(\n     - \\frac{1}{2}\n     (\\fx_i - \\mu)^\\top\n     \\Si^{-1}\n     (\\fx_i - \\mu)\n   \\ri)\n\\] with parameters \\(\\te=(\\mu, \\Si)\\), where \\(\\mu\\in\\R^n\\) is the mean vector and \\(\\Si\\in\\R^{n\\times n}\\) is the covariance matrix. Moreover, the notation \\(|\\Si|\\) denotes the determinant of \\(\\Si\\).\nOur goal is to maximize the log-likelihood function which in this case is \\[\\begin{aligned}\nl(\\mu, \\Si| \\fx_1, \\ldots, \\fx_N)\n  & := \\sum_{i=1}^N \\log f(\\fx_i | \\mu, \\Si) \\\\\n  & = \\sum_{i=1}^N \\li(\n    - \\frac{n}{2} \\log (2 \\pi)\n    - \\frac{1}{2} \\log |\\Si|  \n    - \\frac{1}{2}  (\\fx_i - \\mu)^\\top \\Si^{-1} (\\fx_i - \\mu)\n  \\ri).\n\\end{aligned}\\] For obtaining the MLE of \\(\\mu\\), we can simply take the gradient of \\(l\\) with respect to \\(\\mu\\) and set it to zero resulting in: \\[\n\\nabla_\\mu l(\\mu, \\Si| \\fx_1, \\ldots, \\fx_N)\n= \\sum_{i=1}^N  \\Si^{-1} ( \\fx_i - \\mu )\n= 0\n\\] Since \\(\\Si\\) is a covariance matrix, it is positive definite. Thus, we can multiply both sides of the equation by \\(\\Si\\) and obtain \\[\n\\begin{aligned}\n0 & = N \\mu - \\sum_{i=1}^N  \\fx_i,\n\\\\\n\\Rightarrow \\hmu_{\\mathrm{MLE}} &=  \\frac{1}{N} \\sum_{i=1}^N \\fx_i .\n\\end{aligned}\n\\] Conveniently, \\(\\Si\\) disappears and thus we do not have to worry about it."
  },
  {
    "objectID": "ex/w03/exercises03_solution.html#exercise-1---maximum-likelihood-estimation-refresher",
    "href": "ex/w03/exercises03_solution.html#exercise-1---maximum-likelihood-estimation-refresher",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "Assume you are given datapoints \\((\\fx_i)_{i=1}^N\\) with \\(\\fx_i\\in\\R^n\\) coming from a Gaussian distribution. Derive the maximum likelihood estimator of its mean.\n\n\nFirst, let’s quickly remember that the maximum likelihood estimator (MLE) of a probability distribution from dataapoints \\(\\fx_1, \\ldots, \\fx_N\\) is given by \\[\n\\hte_{\\mathrm{MLE}} = \\argmax_{\\te \\in \\Te} \\prod_{i=1}^N f(\\fx_i | \\te),\n\\] where \\(f\\) is the probability density function of the considered probability distribution family, \\(\\te\\) are the parameters of the distribution, and \\(\\Te\\) is the parameter space (a set containing all possible parameters). The product on the right hand side is also known as the likelihood function. In practice, we usually work with the log-likelihood function instead. Because the logarithm is monotonously increasing, the resulting estimators are the same, i.e. \\[\n\\hte_{\\mathrm{MLE}}\n= \\argmax_{\\te \\in \\Te} \\prod_{i=1}^N f(\\fx_i | \\te),\n= \\argmax_{\\te \\in \\Te} \\sum_{i=1}^N \\log \\li( f(\\fx_i | \\te)\\ri).\n\\]\nSecond, let’s remember that the probability density function of a multivariate Gaussian distribution is given by \\[\nf(\\fx_i | \\mu, \\Si)\n= \\frac{1}{(2 \\pi)^{n/2} |\\Si|^{1/2}}\n   \\exp \\li(\n     - \\frac{1}{2}\n     (\\fx_i - \\mu)^\\top\n     \\Si^{-1}\n     (\\fx_i - \\mu)\n   \\ri)\n\\] with parameters \\(\\te=(\\mu, \\Si)\\), where \\(\\mu\\in\\R^n\\) is the mean vector and \\(\\Si\\in\\R^{n\\times n}\\) is the covariance matrix. Moreover, the notation \\(|\\Si|\\) denotes the determinant of \\(\\Si\\).\nOur goal is to maximize the log-likelihood function which in this case is \\[\\begin{aligned}\nl(\\mu, \\Si| \\fx_1, \\ldots, \\fx_N)\n  & := \\sum_{i=1}^N \\log f(\\fx_i | \\mu, \\Si) \\\\\n  & = \\sum_{i=1}^N \\li(\n    - \\frac{n}{2} \\log (2 \\pi)\n    - \\frac{1}{2} \\log |\\Si|  \n    - \\frac{1}{2}  (\\fx_i - \\mu)^\\top \\Si^{-1} (\\fx_i - \\mu)\n  \\ri).\n\\end{aligned}\\] For obtaining the MLE of \\(\\mu\\), we can simply take the gradient of \\(l\\) with respect to \\(\\mu\\) and set it to zero resulting in: \\[\n\\nabla_\\mu l(\\mu, \\Si| \\fx_1, \\ldots, \\fx_N)\n= \\sum_{i=1}^N  \\Si^{-1} ( \\fx_i - \\mu )\n= 0\n\\] Since \\(\\Si\\) is a covariance matrix, it is positive definite. Thus, we can multiply both sides of the equation by \\(\\Si\\) and obtain \\[\n\\begin{aligned}\n0 & = N \\mu - \\sum_{i=1}^N  \\fx_i,\n\\\\\n\\Rightarrow \\hmu_{\\mathrm{MLE}} &=  \\frac{1}{N} \\sum_{i=1}^N \\fx_i .\n\\end{aligned}\n\\] Conveniently, \\(\\Si\\) disappears and thus we do not have to worry about it."
  },
  {
    "objectID": "ex/w03/exercises03_solution.html#exercise-2---more-gradients",
    "href": "ex/w03/exercises03_solution.html#exercise-2---more-gradients",
    "title": "CSC477 - Fall 2024",
    "section": "Exercise 2 - More Gradients",
    "text": "Exercise 2 - More Gradients\nYou are an ML Engineer at Googlezon where you are working on an internal ML framework called TorchsorFlow. You are tasked with implementing a new layer known as BatchNormalization. The idea of this layer is as follows:\nDuring training, consider the outputs of the previous layer \\(\\fa_i=(a_i^{(1)}, \\ldots, a_i^{(N)})\\) for each element \\(i\\in \\{1, \\ldots, M\\}\\) of the input batch. Compute the mean \\(\\mu_j\\) and variance \\(\\si_j^2\\) over each input dimension \\(j\\). Use the resulting statistics to normalize the output of the previous layer. Finally, rescale the resulting vector with a learned constant \\(\\gm\\) and shift it by another learned constant \\(\\be\\).\n\nWrite down the mathematical expression for the BatchNormalization layer. What are its learnable parameters?\nCompute the gradient of the loss \\(\\mcL\\) with respect to the input of the BatchNormalization \\(\\fa_i\\) layer.\nAt test time, the batch size is usually 1. So, it is not meaningful (or even possible) to compute mean / variance. How would you implement a layer like this?\n\n\nSolution\nFor the purpose of batch normalization, we can consider each output neuron individually. Thus, we will simplify our notation and write \\(a_i\\), \\(y_i\\), … instead of \\(a_i^{(j)}\\), \\(y_i^{(j)}\\), … respectively.\n\nThe forward pass is given by the following equations \\[\n\\begin{aligned}\n\\mu_B\n  &:= \\frac{1}{M}\\sum_{i=1}^m a_i,\n  &\\text{(mini-batch mean)}  \\\\\n\\sigma_B^2\n  &:=\\frac{1}{M}\\sum_{i=1}^M (a_i-\\mu_B)^2,\n  &\\text{(mini-batch variance)}\\\\\n\\ha_i\n  &:= \\frac{a_i-\\mu_B}{\\sqrt{\\si_B^2+\\epsilon}},   \n&\\text{(normalize)}\\\\\n  y_i\n  &:= BN_{\\gamma, \\beta}((a_i)_{i=1}^M)\n  := \\gamma\\ha_i + \\beta.\n  &\\text{(scale and shift)}\n\\end{aligned}\n\\] The entire layer is defined as \\[\nBN(\\fa_1, \\ldots \\fa_M)=\\big(\n  BN_{\\gm^{(1)}, \\be^{(1)}}\\big((a_i^{(1)})_{i=1}^M\\big),\n  \\ldots,\n  BN_{\\gm^{(N)}, \\be^{(N)}}\\big((a_i^{(N)})_{i=1}^M\\big)\n  \\big)\n\\] where \\(\\gm^{(1)}, \\ldots, \\gm^{(N)}\\) and \\(\\be^{(1)}, \\ldots, \\be^{(N)}\\) are learnable parameters.\nThe derivatives can be expressed using the chain rule where we obtain \\(\\mu_B\\), \\(\\si_B\\), \\(\\ha_i\\), and \\(y_i\\) during the forward pass while \\(\\partial \\mcL/\\partial y_i\\) is obtained from earlier steps of the backward pass. The remaining derivatives are: \\[\n\\begin{aligned}\n\\frac{\\partial\\mcL}{\\partial \\ha_i} &= \\fr{\\partial \\mcL}{y_i}\\cdot \\gm, \\\\\n\\frac{\\partial\\mcL}{\\partial \\si_B^2}\n  &=\n  \\sum_{i=1}^M \\fr{\\partial\\mcL}{\\partial\\ha_i}\\cdot(a_i-\\mu_B)\\cdot\n  \\frac{-1}{2}(\\si_B^2+\\epsilon)^{-3/2}, \\\\\n\\fr{\\partial\\mcL}{\\partial\\mu_B}\n  &=\n  \\bigg(\\sum_{i=1}^M \\fr{\\partial\\mcL}{\\partial\\ha_i}\n  \\cdot\\frac{-1}{\\sqrt{\\sigma_B^2+\\epsilon}}\\bigg)\n  +\\fr{\\partial\\mcL}{ \\partial \\si_B^2}\n  \\cdot\\frac{\\sum_{i=1}^M  -2(a_i-\\mu_B)}{M},\\\\\n\\fr{\\partial\\mcL}{\\partial a_i}\n  &=\n  \\fr{\\partial\\mcL}{\\partial\\ha_i}\n  \\cdot \\frac{1}{\\sqrt{\\sigma_B^2+\\epsilon}}  + \\fr{\\partial \\mcL}{\\partial \\sigma_B^2} \\cdot \\frac{2(a_i-\\mu_B)}{M} + \\fr{\\partial \\mcL}{\\partial \\mu_B}\\cdot \\frac{1}{M},\\\\\n\\fr{\\partial\\mcL}{\\partial\\gamma}&= \\sum_{i=1}^M \\fr{\\partial\\mcL}{\\partial y_i} \\cdot \\ha_i, \\\\\n\\fr{\\partial\\mcL}{\\partial\\beta} &= \\sum_{i=1}^M \\fr{\\partial\\mcL}{\\partial y_i}.\n\\end{aligned}\n\\] Here \\(\\epsilon\\) is a small constant which is added in practice to the variance to avoid division by zero. It is actually not part of the derivative."
  },
  {
    "objectID": "ex/w03/exercises03_solution.html#exercise-3---autodiff-modes",
    "href": "ex/w03/exercises03_solution.html#exercise-3---autodiff-modes",
    "title": "CSC477 - Fall 2024",
    "section": "Exercise 3 - Autodiff Modes",
    "text": "Exercise 3 - Autodiff Modes\nConsider the function \\(F(x) = f_3(f_2(f_1(x))\\) and assume you also know the derivatives \\(f_i'\\) for all \\(f_i\\).\n\nApply the chain rule to express \\(F'(x)\\) in terms of \\(f_i'\\)s and \\(f_i\\).\nWrite down the pseudocode for computing \\(F'(x)\\) using the forward mode and the reverse mode respectively. Assume all functions to be scalar functions of a scalar variable, i.e. \\(f_i: \\R \\rightarrow \\R\\).\nIf you simply ask your interpreter / compiler to evaluate the expression in (a), will the computation be in forward mode, reverse mode, or neither of the modes? Why? You can assume that your interpreter / compiler does not do any caching or optimization and simply evaluates the expression from left to right. Does anything change if you assume that your interpreter caches results that have been computed before?\n\n\nSolution\n\nFor better readability we will write \\((g\\circ h)(x)\\) for \\(g(h(x))\\). By applying the chain rule, we obtain \\[\n\\begin{aligned}\nF'(x)\n&= (f_3 \\circ f_2 \\circ f_1)' (x) \\\\\n&= (f_2 \\circ f_1)'(x)\n\\cdot (f_3' \\circ f_2 \\circ f_1)(x)\\\\\n&= f_1'(x) \\cdot (f_2' \\circ f_1)(x)\n\\cdot (f_3' \\circ f_2 \\circ f_1)(x)\n\\end{aligned}\n\\]\nFirst, let’s start with the forward mode\nd = f1'(x)\nv = f1(x)\nd = f2'(x) * d\nv = f2(v)\nd = f3'(v) * d\nNow, for the reverse mode, we first do a “forward pass” before computing gradients:\nv1 = f1(x)\nv2 = f2(v1)\nd = f3'(v2)\nd = d*f2'(v1)\nd = d*f1'(x)\nSimply evaluating the expression in (a) is not in line with any of the modes. It also involves repeated computations because \\(f_1(x)\\) will be computed twice. Now, if we allow for caching of ingtermediate results, this doubling of compugtaiton disappears. The order written above will then be in line with forward move automatic differentiation. However, this is specific to our example and in general not true."
  },
  {
    "objectID": "ex/w03/exercises03_solution.html#exercise-4---glove-embeddings",
    "href": "ex/w03/exercises03_solution.html#exercise-4---glove-embeddings",
    "title": "CSC477 - Fall 2024",
    "section": "Exercise 4 - GloVe Embeddings",
    "text": "Exercise 4 - GloVe Embeddings\nOpen the notebook presented in class and work through it by trying some of the ideas presented therein for different word combinations."
  },
  {
    "objectID": "ex/w03/questions/autodiff-modes-sol.html",
    "href": "ex/w03/questions/autodiff-modes-sol.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "For better readability we will write \\((g\\circ h)(x)\\) for \\(g(h(x))\\). By applying the chain rule, we obtain \\[\n\\begin{aligned}\nF'(x)\n&= (f_3 \\circ f_2 \\circ f_1)' (x) \\\\\n&= (f_2 \\circ f_1)'(x)\n\\cdot (f_3' \\circ f_2 \\circ f_1)(x)\\\\\n&= f_1'(x) \\cdot (f_2' \\circ f_1)(x)\n\\cdot (f_3' \\circ f_2 \\circ f_1)(x)\n\\end{aligned}\n\\]\nFirst, let’s start with the forward mode\nd = f1'(x)\nv = f1(x)\nd = f2'(x) * d\nv = f2(v)\nd = f3'(v) * d\nNow, for the reverse mode, we first do a “forward pass” before computing gradients:\nv1 = f1(x)\nv2 = f2(v1)\nd = f3'(v2)\nd = d*f2'(v1)\nd = d*f1'(x)\nSimply evaluating the expression in (a) is not in line with any of the modes. It also involves repeated computations because \\(f_1(x)\\) will be computed twice. Now, if we allow for caching of ingtermediate results, this doubling of compugtaiton disappears. The order written above will then be in line with forward move automatic differentiation. However, this is specific to our example and in general not true."
  },
  {
    "objectID": "ex/w03/questions/backprop-batchnorm.html",
    "href": "ex/w03/questions/backprop-batchnorm.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "You are an ML Engineer at Googlezon where you are working on an internal ML framework called TorchsorFlow. You are tasked with implementing a new layer known as BatchNormalization. The idea of this layer is as follows:\nDuring training, consider the outputs of the previous layer \\(\\fa_i=(a_i^{(1)}, \\ldots, a_i^{(N)})\\) for each element \\(i\\in \\{1, \\ldots, M\\}\\) of the input batch. Compute the mean \\(\\mu_j\\) and variance \\(\\si_j^2\\) over each input dimension \\(j\\). Use the resulting statistics to normalize the output of the previous layer. Finally, rescale the resulting vector with a learned constant \\(\\gm\\) and shift it by another learned constant \\(\\be\\).\n\nWrite down the mathematical expression for the BatchNormalization layer. What are its learnable parameters?\nCompute the gradient of the loss \\(\\mcL\\) with respect to the input of the BatchNormalization \\(\\fa_i\\) layer.\nAt test time, the batch size is usually 1. So, it is not meaningful (or even possible) to compute mean / variance. How would you implement a layer like this?"
  },
  {
    "objectID": "ex/w03/questions/backprop-batchnorm-sol.html",
    "href": "ex/w03/questions/backprop-batchnorm-sol.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "For the purpose of batch normalization, we can consider each output neuron individually. Thus, we will simplify our notation and write \\(a_i\\), \\(y_i\\), … instead of \\(a_i^{(j)}\\), \\(y_i^{(j)}\\), … respectively.\n\nThe forward pass is given by the following equations \\[\n\\begin{aligned}\n\\mu_B\n  &:= \\frac{1}{M}\\sum_{i=1}^m a_i,\n  &\\text{(mini-batch mean)}  \\\\\n\\sigma_B^2\n  &:=\\frac{1}{M}\\sum_{i=1}^M (a_i-\\mu_B)^2,\n  &\\text{(mini-batch variance)}\\\\\n\\ha_i\n  &:= \\frac{a_i-\\mu_B}{\\sqrt{\\si_B^2+\\epsilon}},   \n&\\text{(normalize)}\\\\\n  y_i\n  &:= BN_{\\gamma, \\beta}((a_i)_{i=1}^M)\n  := \\gamma\\ha_i + \\beta.\n  &\\text{(scale and shift)}\n\\end{aligned}\n\\] The entire layer is defined as \\[\nBN(\\fa_1, \\ldots \\fa_M)=\\big(\n  BN_{\\gm^{(1)}, \\be^{(1)}}\\big((a_i^{(1)})_{i=1}^M\\big),\n  \\ldots,\n  BN_{\\gm^{(N)}, \\be^{(N)}}\\big((a_i^{(N)})_{i=1}^M\\big)\n  \\big)\n\\] where \\(\\gm^{(1)}, \\ldots, \\gm^{(N)}\\) and \\(\\be^{(1)}, \\ldots, \\be^{(N)}\\) are learnable parameters.\nThe derivatives can be expressed using the chain rule where we obtain \\(\\mu_B\\), \\(\\si_B\\), \\(\\ha_i\\), and \\(y_i\\) during the forward pass while \\(\\partial \\mcL/\\partial y_i\\) is obtained from earlier steps of the backward pass. The remaining derivatives are: \\[\n\\begin{aligned}\n\\frac{\\partial\\mcL}{\\partial \\ha_i} &= \\fr{\\partial \\mcL}{y_i}\\cdot \\gm, \\\\\n\\frac{\\partial\\mcL}{\\partial \\si_B^2}\n  &=\n  \\sum_{i=1}^M \\fr{\\partial\\mcL}{\\partial\\ha_i}\\cdot(a_i-\\mu_B)\\cdot\n  \\frac{-1}{2}(\\si_B^2+\\epsilon)^{-3/2}, \\\\\n\\fr{\\partial\\mcL}{\\partial\\mu_B}\n  &=\n  \\bigg(\\sum_{i=1}^M \\fr{\\partial\\mcL}{\\partial\\ha_i}\n  \\cdot\\frac{-1}{\\sqrt{\\sigma_B^2+\\epsilon}}\\bigg)\n  +\\fr{\\partial\\mcL}{ \\partial \\si_B^2}\n  \\cdot\\frac{\\sum_{i=1}^M  -2(a_i-\\mu_B)}{M},\\\\\n\\fr{\\partial\\mcL}{\\partial a_i}\n  &=\n  \\fr{\\partial\\mcL}{\\partial\\ha_i}\n  \\cdot \\frac{1}{\\sqrt{\\sigma_B^2+\\epsilon}}  + \\fr{\\partial \\mcL}{\\partial \\sigma_B^2} \\cdot \\frac{2(a_i-\\mu_B)}{M} + \\fr{\\partial \\mcL}{\\partial \\mu_B}\\cdot \\frac{1}{M},\\\\\n\\fr{\\partial\\mcL}{\\partial\\gamma}&= \\sum_{i=1}^M \\fr{\\partial\\mcL}{\\partial y_i} \\cdot \\ha_i, \\\\\n\\fr{\\partial\\mcL}{\\partial\\beta} &= \\sum_{i=1}^M \\fr{\\partial\\mcL}{\\partial y_i}.\n\\end{aligned}\n\\] Here \\(\\epsilon\\) is a small constant which is added in practice to the variance to avoid division by zero. It is actually not part of the derivative."
  },
  {
    "objectID": "ex/w04/exercises04-notes.html",
    "href": "ex/w04/exercises04-notes.html",
    "title": "Solution Computations",
    "section": "",
    "text": "This file is for internal use where we compute the solutions to the exercises. I also compute the solutions for seemingly easy exercises here to use the code later for automation of the process."
  },
  {
    "objectID": "ex/w04/exercises04-notes.html#cnn-size-exercise-solutions",
    "href": "ex/w04/exercises04-notes.html#cnn-size-exercise-solutions",
    "title": "Solution Computations",
    "section": "CNN Size exercise solutions",
    "text": "CNN Size exercise solutions\n\nimport torch\nimport torch.nn as nn\n\nconv = nn.Conv2d(1, 1, kernel_size=2, stride=1, padding=0, bias=False)\n\nx = torch.tensor([\n    [ 1, 0, -2, 1],\n    [ 0, 1, 1, 0],\n    [ 0, 1, 0, 1],\n    [ -3, 4, 0, 0]\n])\n\nk = torch.tensor([\n    [ 2, 1],\n    [ 0, 1]\n])\n\nconv.weight.data = k.view(1, 1, 2, 2).float()\n\nprint(conv(x.view(1, 1, 4, 4).float()))\n\n\n\ntensor([[[[ 3., -1., -3.],\n          [ 2.,  3.,  3.],\n          [ 5.,  2.,  1.]]]], grad_fn=&lt;ConvolutionBackward0&gt;)\n\n\n\nconv = nn.Conv2d(1, 1, kernel_size=2, stride=2, padding=0, bias=False)\nconv.weight.data = k.view(1, 1, 2, 2).float()\nprint(conv(x.view(1, 1, 4, 4).float()))\n\ntensor([[[[ 3., -3.],\n          [ 5.,  1.]]]], grad_fn=&lt;ConvolutionBackward0&gt;)\n\n\n\n# PyTorch uses zero-padding by default\nconv = nn.Conv2d(1, 1, kernel_size=2, stride=2, padding=1, bias=False)\nconv.weight.data = k.view(1, 1, 2, 2).float()\nprint(conv(x.view(1, 1, 4, 4).float()))\n\ntensor([[[[ 1., -2.,  0.],\n          [ 0.,  3.,  0.],\n          [-3.,  8.,  0.]]]], grad_fn=&lt;ConvolutionBackward0&gt;)"
  },
  {
    "objectID": "ex/w04/exercises04-notes.html#exercise-3-mlp-sizes",
    "href": "ex/w04/exercises04-notes.html#exercise-3-mlp-sizes",
    "title": "Solution Computations",
    "section": "Exercise 3 MLP Sizes",
    "text": "Exercise 3 MLP Sizes\n\na = nn.Sequential(\n    nn.Linear(10,5),\n    nn.Linear(5,10),\n    nn.Linear(10,5)\n)\n\nfor i in a:\n    print(sum(p.numel() for p in i.parameters()))\n\n55\n60\n55"
  },
  {
    "objectID": "ex/w04/exercises04-notes.html#exercise-4-cnn-sizes",
    "href": "ex/w04/exercises04-notes.html#exercise-4-cnn-sizes",
    "title": "Solution Computations",
    "section": "Exercise 4 CNN Sizes",
    "text": "Exercise 4 CNN Sizes\n\ninp = torch.randn(1, 3, 100, 100)\narch = nn.Sequential(\n    nn.Conv2d(3, 5, kernel_size=3),\n    nn.MaxPool2d(2, stride=2),\n    nn.Conv2d(5, 10, kernel_size=3, stride=1),\n    nn.MaxPool2d(2, stride=2),\n    nn.Conv2d(10, 5, kernel_size=3, stride=1),\n    nn.Flatten(),\n    nn.Linear(2205, 20),\n    nn.Linear(20,10)\n)\n\ncur = inp\nfor num, layer in enumerate(arch):\n    cur = layer(cur)\n    print(f\"{num+1}. Activation: {cur.shape}, Params: {sum(p.numel() for p in layer.parameters())}\") \n\n1. Activation: torch.Size([1, 5, 98, 98]), Params: 140\n2. Activation: torch.Size([1, 5, 49, 49]), Params: 0\n3. Activation: torch.Size([1, 10, 47, 47]), Params: 460\n4. Activation: torch.Size([1, 10, 23, 23]), Params: 0\n5. Activation: torch.Size([1, 5, 21, 21]), Params: 455\n6. Activation: torch.Size([1, 2205]), Params: 0\n7. Activation: torch.Size([1, 20]), Params: 44120\n8. Activation: torch.Size([1, 10]), Params: 210\n\n\n\n\n\n0 5\n1 4\n2 2"
  },
  {
    "objectID": "ex/w04/questions/mlp-sizes-sol.html",
    "href": "ex/w04/questions/mlp-sizes-sol.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "The number of parameters for each neuron is the number of weights plus one for the biaas term. The number of weights corresponds to the number of inputs / activations from the previous layer. So for the first layer, we have 10 inputs and thus 11 parameters per neuron resulting in 55 parameters total per layer.\nA similar computation gives 60 and 55 as the number of parameters for the next two layers. Thus, the network has a total of 170 parameters."
  },
  {
    "objectID": "ex/w04/questions/cnn-sizes.html",
    "href": "ex/w04/questions/cnn-sizes.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "You are givne a neural network with the following architecture:\nInput: 100 x 100 x 3 Image\n\nLayers:\n1. Conv(in_channels=3, out_channels=5, kernel_size=3, stride=1, padding=0)\n2. MaxPool2d(kernel_size=2, stride=2, padding=0)\n3. Conv(in_channels=5, out_channels=10, kernel_size=3, stride=1, padding=0)\n4. MaxPool2d(kernel_size=2, stride=2, padding=0)\n5. Conv(in_channels=10, out_channels=5, kernel_size=3, stride=1, padding=0)\n6. Flatten()\n7. MLP(neurons=20)\n8. MLP(neurons=10)\n\nWhat is the dimensionality of the activations after each layer.\nHow many parameters does this network have?"
  },
  {
    "objectID": "ex/w04/questions/mlp-sizes.html",
    "href": "ex/w04/questions/mlp-sizes.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "You are given an MLP with ReLU activations. It has 3 layers consisting of 5, 10, and 5 neurons respectively. The input is a vector of size 10. How many parameters does this network have?"
  },
  {
    "objectID": "ex/w04/questions/cnn-by_hand.html",
    "href": "ex/w04/questions/cnn-by_hand.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "Consider the following \\(4\\times 4 \\times 1\\) input X and a \\(2\\times 2 \\times 1\\) convolutional kernel K with no bias term\n\\[\nX =\n\\bpmat\n1 & 0 & -2 & 1 \\\\\n0 & 1 & 1 & 0 \\\\\n0 & 1 & 0 & 1 \\\\\n-3 & 4 & 0 & 0\n\\epmat, \\qquad\n%\nK = \\bpmat\n2, & 1 \\\\\n0, & 1 \\\\\n\\epmat\n\\]\n\nWhat is the output of the convolutional layer for the case of stride 1 and no padding?\nWhat if we have stride 2 and no padding?\nWhat if we have stride 2 and zero-padding of size 1?"
  },
  {
    "objectID": "ex/w05/exercises05-notes.html",
    "href": "ex/w05/exercises05-notes.html",
    "title": "Ex 2: Eigenvalues and Eigenvectors to matrices",
    "section": "",
    "text": "Part (a) is straightforward. One needs to simply concatenate the given eigenvalues in a matrix.\n\nimport numpy as np\n\nla1 = 2\nla2 = 3\nev1 = np.array([1, 0])\nev2 = np.array([0, -1])\n\nLa = np.diag([la1, la2])\nO = np.array([ev1, ev2])\n\nnp.matmul(np.matmul(O, La), O.T)\n\narray([[2, 0],\n       [0, 3]])\n\n\nPart (b) has a catch. The eigenvectors are not normalized. So, we need to normalize them first before concatenating them in a matrix.\n\nla1 = 2\nla2 = 3\nev1 = np.array([1, 1])\nev2 = np.array([1, -1])\n\nLa = np.diag([la1, la2])\nO = np.array([ev1, ev2])\n\nprint(np.matmul(np.matmul(O, La), np.linalg.inv(O)))\nprint(np.matmul(np.matmul(O, La), (O).T)/2) # Sanity check\n\n[[ 2.5 -0.5]\n [-0.5  2.5]]\n[[ 2.5 -0.5]\n [-0.5  2.5]]\n1.4142135623730951\n\n\n\nnp.linalg.norm(ev2)\n\n2.8284271247461903\n\n\n\nEx 3: SGD w. Momentum Implementation\nFirst, let’s define an objective function, its gradient, and a starting point for the optimizer. In our case, this is simply \\(f(x) = x^2\\) and \\(x_{init}\\)=1.\n\ndef objective(x):\n    return x**2\n\ndef obj_grad(x):\n    return 2*x\n\nNow, we implement the actual optimizer.\n\ndef sgd_with_momentum(obj, grad, x_init, learning_rate, momentum, max_iter):\n  x = x_init\n  update = 0\n  for i in range(max_iter):\n    update = momentum * update - learning_rate * grad(x)\n    x = x + update\n\n    print('&gt;%d f(%s) = %.5f' % (i, x, obj(x)))\n  return x\n\n\nsgd_with_momentum(\n    objective, obj_grad, x_init=3.0, learning_rate=0.1, momentum=0.5, \n    max_iter=20\n    )\n\n&gt;0 f(2.4) = 5.76000\n&gt;1 f(1.6199999999999999) = 2.62440\n&gt;2 f(0.9059999999999999) = 0.82084\n&gt;3 f(0.3677999999999999) = 0.13528\n&gt;4 f(0.02513999999999994) = 0.00063\n&gt;5 f(-0.15121800000000002) = 0.02287\n&gt;6 f(-0.2091534) = 0.04375\n&gt;7 f(-0.19629041999999997) = 0.03853\n&gt;8 f(-0.15060084599999995) = 0.02268\n&gt;9 f(-0.09763588979999996) = 0.00953\n&gt;10 f(-0.05162623373999997) = 0.00267\n&gt;11 f(-0.018296158961999986) = 0.00033\n&gt;12 f(0.0020281102194000047) = 0.00000\n&gt;13 f(0.011784622766219999) = 0.00014\n&gt;14 f(0.014305954486385997) = 0.00020\n&gt;15 f(0.012705429449191796) = 0.00016\n&gt;16 f(0.009364081040756336) = 0.00009\n&gt;17 f(0.005820590628387338) = 0.00003\n&gt;18 f(0.002884727296525372) = 0.00001\n&gt;19 f(0.0008398501712893144) = 0.00000\n\n\n0.0008398501712893144"
  },
  {
    "objectID": "ex/w05/exercises05.html",
    "href": "ex/w05/exercises05.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "Compute the Taylor series expansion up to the second order term for the following multivariate functions around a given point:\n\n\\(f(x) = 5x^3\\) around \\(x_0=1\\).\n\\(f(x,y) = x^2 \\cdot y^3 + x^2\\) around \\(x_0=3\\), \\(y_0=2\\).\n\\(f(\\fx) = x_1^3 \\cdot x_2 \\cdot \\log(x_2)\\) around \\(\\fx_0=(2,1)^\\top\\).\n\\(f(\\fx) = \\sin(x_1) + \\cos(x_2)\\) around \\(\\fx_0=(-\\pi,\\pi)^\\top\\)."
  },
  {
    "objectID": "ex/w05/exercises05.html#exercise-1---taylor-series",
    "href": "ex/w05/exercises05.html#exercise-1---taylor-series",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "Compute the Taylor series expansion up to the second order term for the following multivariate functions around a given point:\n\n\\(f(x) = 5x^3\\) around \\(x_0=1\\).\n\\(f(x,y) = x^2 \\cdot y^3 + x^2\\) around \\(x_0=3\\), \\(y_0=2\\).\n\\(f(\\fx) = x_1^3 \\cdot x_2 \\cdot \\log(x_2)\\) around \\(\\fx_0=(2,1)^\\top\\).\n\\(f(\\fx) = \\sin(x_1) + \\cos(x_2)\\) around \\(\\fx_0=(-\\pi,\\pi)^\\top\\)."
  },
  {
    "objectID": "ex/w05/exercises05.html#exercise-2---eigenvalues-eigenvectors",
    "href": "ex/w05/exercises05.html#exercise-2---eigenvalues-eigenvectors",
    "title": "CSC477 - Fall 2024",
    "section": "Exercise 2 - Eigenvalues, Eigenvectors",
    "text": "Exercise 2 - Eigenvalues, Eigenvectors\nYou are given the sets of eigevalues and eigenvectors. Compute the corresponding matrix.\n\n\\(\\la_1 = 2\\), \\(\\la_2 = 3\\), \\(\\fv_1 = (1,0)^\\top\\), \\(\\fv_2 = (0,1)^\\top\\).\n\\(\\la_1 = 2\\), \\(\\la_2 = 3\\), \\(\\fv_1 = (1,1)^\\top\\), \\(\\fv_2 = (1,-1)^\\top\\)."
  },
  {
    "objectID": "ex/w05/exercises05.html#exercise-3---sgd-with-momentum",
    "href": "ex/w05/exercises05.html#exercise-3---sgd-with-momentum",
    "title": "CSC477 - Fall 2024",
    "section": "Exercise 3 - SGD with Momentum",
    "text": "Exercise 3 - SGD with Momentum\nImplement stochastic gradient descent with momentum and apply it to optimize some elementary functions in 1d and 2d."
  },
  {
    "objectID": "ex/w05/questions/calc-taylor.html",
    "href": "ex/w05/questions/calc-taylor.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "Compute the Taylor series expansion up to the second order term for the following multivariate functions around a given point:\n\n\\(f(x) = 5x^3\\) around \\(x_0=1\\).\n\\(f(x,y) = x^2 \\cdot y^3 + x^2\\) around \\(x_0=3\\), \\(y_0=2\\).\n\\(f(\\fx) = x_1^3 \\cdot x_2 \\cdot \\log(x_2)\\) around \\(\\fx_0=(2,1)^\\top\\).\n\\(f(\\fx) = \\sin(x_1) + \\cos(x_2)\\) around \\(\\fx_0=(-\\pi,\\pi)^\\top\\)."
  },
  {
    "objectID": "ex/w05/questions/linalg-evs_to_mat-sol.html",
    "href": "ex/w05/questions/linalg-evs_to_mat-sol.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "First, remember that the normalized eigenvectors of a symmetric matrix are orthogonal. Thus, we have \\[\n\\fe_i^\\top \\fe_j = \\begin{cases} 1 & i=j \\\\ 0 & i\\neq j \\end{cases}.\n\\]\nSecond, for symmetric \\(\\fA\\), its spectral decomposition is given by \\(\\fA = \\fQ \\fLa \\fQ^\\top\\), where \\(\\fQ\\) is a matrix where each column is an (orthogonal) eigenvector of unit length.\n\nHere, the eigenvectors are already normalized and orthogonal, so we can simply write \\(\\fQ = (\\fe_1, \\fe_2)\\) and \\(\\fLa = \\diag(\\lambda_1, \\lambda_2)\\). Then, we have \\[\n  \\fA = \\bpmat 2 & 0 \\\\\n0 & 3\n\\epmat\n  \\]\nHere, we have to normalize the eigenvectors first. Each has length \\(\\sqrt{2}\\), so we have to divide each of them by \\(\\sqrt{2}\\), i.e. we set \\(\\fte_i:=\\fe_i/\\sqrt{2}\\). With this, we can construct an orthogonal matrix of eigenvalues as \\(\\fQ = (\\fte_1, \\fte_2)\\). The resulting matrix \\(\\fA\\) is \\[\n  \\fA = \\bpmat\n2.5 & -0.5 \\\\\n-0.5 & 2.5\n\\epmat\n  \\]"
  },
  {
    "objectID": "ex/w05/questions/opt-momentum.html",
    "href": "ex/w05/questions/opt-momentum.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "Implement stochastic gradient descent with momentum and apply it to optimize some elementary functions in 1d and 2d."
  },
  {
    "objectID": "ex/w02/exercises02_solution.html",
    "href": "ex/w02/exercises02_solution.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "For a square matrix \\(\\fA\\) of size \\(n \\times n\\), a vector \\(\\fu_i \\neq 0\\) which satisﬁes \\[\\begin{equation}\n\\fA\\fu_i = \\la_i \\fu_i\n\\label{eq:eigen}\n\\end{equation}\\] is called a eigenvector of \\(\\fA\\), and \\(\\la_i\\) is the corresponding eigenvalue. For a matrix of size \\(n \\times n\\), there are \\(n\\) eigenvalues \\(\\la_i\\) (which are not necessarily distinct).\nShow that if \\(\\fu_1\\) and \\(\\fu_2\\) are eigenvectors with equal corresponding eigenvalues \\(\\la_1 = \\la_2\\), then \\(\\fu = \\al \\fu_1 + \\be \\fu_2\\) is also an eigenvector with the same eigenvalue.\n\n\n\nBecause \\(\\la_1=\\la_2\\), we will write \\(\\la\\) for simplicity. The result is obtained by applying the definition of eigenvalues and distributivity via \\[\\begin{equation*}\n\\fA\\fu\n  = \\fA (\\al \\fu_1 + \\be \\fu_2)\n  =   \\al \\fA \\fu_1 + \\be \\fA \\fu_2\n  =   \\al \\la \\fu_1 + \\be \\la \\fu_2\n  =   \\la (\\al\\fu_1 + \\be \\fu_2)\n  =   \\la \\fu .\n\\end{equation*}\\]"
  },
  {
    "objectID": "ex/w02/exercises02_solution.html#exercise-1---eigenvectors-and-eigenvalues",
    "href": "ex/w02/exercises02_solution.html#exercise-1---eigenvectors-and-eigenvalues",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "For a square matrix \\(\\fA\\) of size \\(n \\times n\\), a vector \\(\\fu_i \\neq 0\\) which satisﬁes \\[\\begin{equation}\n\\fA\\fu_i = \\la_i \\fu_i\n\\label{eq:eigen}\n\\end{equation}\\] is called a eigenvector of \\(\\fA\\), and \\(\\la_i\\) is the corresponding eigenvalue. For a matrix of size \\(n \\times n\\), there are \\(n\\) eigenvalues \\(\\la_i\\) (which are not necessarily distinct).\nShow that if \\(\\fu_1\\) and \\(\\fu_2\\) are eigenvectors with equal corresponding eigenvalues \\(\\la_1 = \\la_2\\), then \\(\\fu = \\al \\fu_1 + \\be \\fu_2\\) is also an eigenvector with the same eigenvalue.\n\n\n\nBecause \\(\\la_1=\\la_2\\), we will write \\(\\la\\) for simplicity. The result is obtained by applying the definition of eigenvalues and distributivity via \\[\\begin{equation*}\n\\fA\\fu\n  = \\fA (\\al \\fu_1 + \\be \\fu_2)\n  =   \\al \\fA \\fu_1 + \\be \\fA \\fu_2\n  =   \\al \\la \\fu_1 + \\be \\la \\fu_2\n  =   \\la (\\al\\fu_1 + \\be \\fu_2)\n  =   \\la \\fu .\n\\end{equation*}\\]"
  },
  {
    "objectID": "ex/w02/exercises02_solution.html#exercise-2---variance-and-expectation",
    "href": "ex/w02/exercises02_solution.html#exercise-2---variance-and-expectation",
    "title": "CSC477 - Fall 2024",
    "section": "Exercise 2 - Variance and Expectation",
    "text": "Exercise 2 - Variance and Expectation\n\nGiven a set of vectors \\(\\{\\fx_i\\}_{i=1}^N\\). Show that their empirical mean is equivalent to \\[\\hmu=\\argmin_\\fmu \\sum_i \\|\\fx_i - \\fmu\\|^2.\\]\nThere are two equivalent definitons of variance of a random variable. The first one is \\(\\Var(X) := \\E[(X - \\E[X])^2]\\) and the second is \\(\\Var(X) = \\E[X^2] - \\E[X]^2\\). Show that these two definitions actually are equivalent.\n\n\nSolution\n\nThe key idea here is to compute the gradient of the objective function and solve for \\(\\fmu\\). The gradient is obtained by applying the chain rule resulting in \\[0=\\nabla_\\fmu \\sum_i \\|\\fx_i - \\fmu\\|^2 = -2 \\sum_i (\\fx_i - \\fmu) .\\] Now, we solve this for \\(\\fmu\\) to obtain \\[\\fmu = \\frac{1}{N} \\sum_i \\fx_i .\\]\nHere, we simply need to apply some algebraic manipulations to show that the two definitions are equivalent. We start with the first definition and expand the square: \\[\\begin{align*}\n\\E[(X - \\E[X])^2]\n  &= \\E\\li[X^2 - 2X\\E[X]+\\E[X]^2\\ri] \\\\\n  &= \\E\\li[X^2\\ri] - \\E[2X\\E[X]]+\\E\\li[\\E[X]^2\\ri]\\\\\n  &= \\E\\li[X^2\\ri] - 2\\E[X]\\E[X]+\\E[X]^2\\\\\n  &= \\E\\li[X^2\\ri] - \\E[X]^2\n\\end{align*}\\]"
  },
  {
    "objectID": "ex/w02/exercises02_solution.html#exercise-3---linear-regression",
    "href": "ex/w02/exercises02_solution.html#exercise-3---linear-regression",
    "title": "CSC477 - Fall 2024",
    "section": "Exercise 3 - Linear Regression",
    "text": "Exercise 3 - Linear Regression\n\n\nIn the linear regression model with one feature, we have the following model/hypothesis: \\[y = f(x) = w x + b\\] ​with parameters, \\(w\\) and \\(b\\), which we wish to find by minimizing the cost: \\[\\mathcal{E}(w, b) = \\frac{1}{2N}\\sum_i ((w x^{(i)} + b) - t^{(i)})^2\\] What are the derivatives \\(\\frac{\\partial \\mathcal{E}}{\\partial w}\\) and \\(\\frac{\\partial \\mathcal{E}}{\\partial b}\\)?\nIn the linear regression model with many features, we have the following model/hypothesis: \\[y = f(x) = {\\bf w}^\\top {\\bf x}+ b\\] with parameters, \\({\\bf w} = [w_1, w_2, \\dots w_d]^T\\) and \\(b\\), which we wish to find by minimizing the cost: \\[\\mathcal{E}({\\bf w}, b) = \\frac{1}{2N}\\sum_i ((\\fw^\\top \\fx^{(i)}+b) - t^{(i)})^2\\] What is the derivative \\(\\frac{\\partial \\mathcal{E}}{\\partial w_j}\\) for a weight \\(w_j\\)?\n\n\nSolution\n\nWe obtain the derivative with respect to \\(w\\) directly using the chain rule resulting in \\[\n\\frac{\\partial \\mathcal{E}}{\\partial w}\n  = \\frac{1}{N}\\sum_i x^{(i)}((w x^{(i)} + b) - t^{(i)})\n\\] Similarly, the derivative with respect to \\(b\\) is \\[\n\\frac{\\partial \\mathcal{E}}{\\partial b}\n  = \\frac{1}{N}\\sum_i ((w x^{(i)} + b) - t^{(i)})\n\\]\nThe derivative with respect to \\(w_j\\) is \\[\n\\frac{\\partial \\mathcal{E}}{\\partial w_j}\n  = \\frac{1}{N}\\sum_i x_j^{(i)}((\\fw^\\top \\fx^{(i)}+b) - t^{(i)})\n\\]"
  },
  {
    "objectID": "ex/w02/exercises02_solution.html#exercise-4---gradients-and-computation-graphs",
    "href": "ex/w02/exercises02_solution.html#exercise-4---gradients-and-computation-graphs",
    "title": "CSC477 - Fall 2024",
    "section": "Exercise 4 - Gradients and Computation Graphs",
    "text": "Exercise 4 - Gradients and Computation Graphs\n\n\nCompute the \\(\\frac{\\partial \\mathcal{L}}{\\partial w_j}\\) gradient of \\(\\mathcal{L}\\) with respect to a \\(w_j\\) in the following computation: \\[\\begin{align*}\n  \\mathcal{L}(y, t) &= - t \\log(y) - (1-t) \\log(1-y) ,\n& y &= \\sigma(z) ,\n&  z &= {\\bf w}^\\top {\\bf x} .\n  \\end{align*}\\]\nDraw the computation graph for the following neural network, showing the relevant scalar quantities. Assume that \\(\\fy, \\fh, \\fx \\in \\mathbb{R}^2\\) \\[\\begin{align*}\n  \\mathcal{L} &= \\frac{1}{2}\\sum_k (y_k - t_k)^2 ,\n& y_k &= \\sum_i w_{ki}^{(2)} h_i + b_k^{(2)} ,\n& h_i &= \\sigma(z_i) ,\n& z_i &= \\sum_j w_{ij}^{(1)} x_j + b_i^{(1)} .\n  \\end{align*}\\]\n\n\nSolution\n\nApplying the chain rule, we have\n\\[ \\frac{\\partial \\mathcal{L}}{\\partial w_j} = \\frac{\\partial \\mathcal{L}}{\\partial y} \\frac{\\partial y}{\\partial z} \\frac{\\partial z}{\\partial w_j} \\]\nLooking at each term individually yields \\[\n\\begin{aligned}\n\\frac{\\partial \\mathcal{L}}{\\partial y}\n  &= \\frac{\\partial}{\\partial y} [-t \\log(y) - (1 - t) \\log(1 - y)]\n  = - \\frac{t}{y} + \\frac{1 - t}{1 - y}\\\\\n\\frac{\\partial y}{\\partial z}\n  &= \\frac{\\partial \\sigma(z)}{\\partial z}\n  = \\sigma(z) (1 - \\sigma(z))\n  = y (1 - y)\\\\\n\\frac{\\partial z}{\\partial w_j}\n  &= \\frac{\\partial}{\\partial w_j} (w^\\top x) = x_j\n\\end{aligned}\n\\]\nBringing it all together yields: \\[\n\\begin{aligned}\n\\frac{\\partial \\mathcal{L}}{\\partial w_j}\n  &= \\left( - \\frac{t}{y} + \\frac{1 - t}{1 - y} \\right) \\cdot y (1 - y) \\cdot x_j \\\\\n  &= (-t + ty + 1 - t - y + ty) x_j \\\\\n  &= (y - t) x_j\n\\end{aligned}\n\\]\nThe computation graph is given in the figure below.\n\n\n\n\nComputation graph for exercise 4 (b)"
  },
  {
    "objectID": "ex/w02/questions/nn-compgraph.html",
    "href": "ex/w02/questions/nn-compgraph.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "Compute the \\(\\frac{\\partial \\mathcal{L}}{\\partial w_j}\\) gradient of \\(\\mathcal{L}\\) with respect to a \\(w_j\\) in the following computation: \\[\\begin{align*}\n  \\mathcal{L}(y, t) &= - t \\log(y) - (1-t) \\log(1-y) ,\n& y &= \\sigma(z) ,\n&  z &= {\\bf w}^\\top {\\bf x} .\n  \\end{align*}\\]\nDraw the computation graph for the following neural network, showing the relevant scalar quantities. Assume that \\(\\fy, \\fh, \\fx \\in \\mathbb{R}^2\\) \\[\\begin{align*}\n  \\mathcal{L} &= \\frac{1}{2}\\sum_k (y_k - t_k)^2 ,\n& y_k &= \\sum_i w_{ki}^{(2)} h_i + b_k^{(2)} ,\n& h_i &= \\sigma(z_i) ,\n& z_i &= \\sum_j w_{ij}^{(1)} x_j + b_i^{(1)} .\n  \\end{align*}\\]"
  },
  {
    "objectID": "ex/w02/questions/linreg-sol.html",
    "href": "ex/w02/questions/linreg-sol.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "We obtain the derivative with respect to \\(w\\) directly using the chain rule resulting in \\[\n\\frac{\\partial \\mathcal{E}}{\\partial w}\n  = \\frac{1}{N}\\sum_i x^{(i)}((w x^{(i)} + b) - t^{(i)})\n\\] Similarly, the derivative with respect to \\(b\\) is \\[\n\\frac{\\partial \\mathcal{E}}{\\partial b}\n  = \\frac{1}{N}\\sum_i ((w x^{(i)} + b) - t^{(i)})\n\\]\nThe derivative with respect to \\(w_j\\) is \\[\n\\frac{\\partial \\mathcal{E}}{\\partial w_j}\n  = \\frac{1}{N}\\sum_i x_j^{(i)}((\\fw^\\top \\fx^{(i)}+b) - t^{(i)})\n\\]"
  },
  {
    "objectID": "ex/w02/questions/linreg.html",
    "href": "ex/w02/questions/linreg.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "In the linear regression model with one feature, we have the following model/hypothesis: \\[y = f(x) = w x + b\\] ​with parameters, \\(w\\) and \\(b\\), which we wish to find by minimizing the cost: \\[\\mathcal{E}(w, b) = \\frac{1}{2N}\\sum_i ((w x^{(i)} + b) - t^{(i)})^2\\] What are the derivatives \\(\\frac{\\partial \\mathcal{E}}{\\partial w}\\) and \\(\\frac{\\partial \\mathcal{E}}{\\partial b}\\)?\nIn the linear regression model with many features, we have the following model/hypothesis: \\[y = f(x) = {\\bf w}^\\top {\\bf x}+ b\\] with parameters, \\({\\bf w} = [w_1, w_2, \\dots w_d]^T\\) and \\(b\\), which we wish to find by minimizing the cost: \\[\\mathcal{E}({\\bf w}, b) = \\frac{1}{2N}\\sum_i ((\\fw^\\top \\fx^{(i)}+b) - t^{(i)})^2\\] What is the derivative \\(\\frac{\\partial \\mathcal{E}}{\\partial w_j}\\) for a weight \\(w_j\\)?"
  },
  {
    "objectID": "ex/w02/questions/prob-evvar.html",
    "href": "ex/w02/questions/prob-evvar.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "Given a set of vectors \\(\\{\\fx_i\\}_{i=1}^N\\). Show that their empirical mean is equivalent to \\[\\hmu=\\argmin_\\fmu \\sum_i \\|\\fx_i - \\fmu\\|^2.\\]\nThere are two equivalent definitons of variance of a random variable. The first one is \\(\\Var(X) := \\E[(X - \\E[X])^2]\\) and the second is \\(\\Var(X) = \\E[X^2] - \\E[X]^2\\). Show that these two definitions actually are equivalent."
  },
  {
    "objectID": "ex/w11/exercises11.html",
    "href": "ex/w11/exercises11.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "You are given the following set of eigevalues and eigenvectors. Compute the corresponding matrix.\n\\(\\la_1 = 1\\), \\(\\la_2 = 2\\), \\(\\fv_1 = (\\sqrt{0.5}, \\sqrt{0.5})^\\top\\), \\(\\fv_2 = (\\sqrt{0.5},-\\sqrt{0.5})^\\top\\)."
  },
  {
    "objectID": "ex/w11/exercises11.html#exercise-1---eigenvalues-and-eigenvectors",
    "href": "ex/w11/exercises11.html#exercise-1---eigenvalues-and-eigenvectors",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "You are given the following set of eigevalues and eigenvectors. Compute the corresponding matrix.\n\\(\\la_1 = 1\\), \\(\\la_2 = 2\\), \\(\\fv_1 = (\\sqrt{0.5}, \\sqrt{0.5})^\\top\\), \\(\\fv_2 = (\\sqrt{0.5},-\\sqrt{0.5})^\\top\\)."
  },
  {
    "objectID": "ex/w11/exercises11.html#exercise-2---parameter-counting",
    "href": "ex/w11/exercises11.html#exercise-2---parameter-counting",
    "title": "CSC477 - Fall 2024",
    "section": "Exercise 2 - Parameter Counting",
    "text": "Exercise 2 - Parameter Counting\nUse PyTorch to load the alexnet model and automatically compute its number of parameters. Output the number of parameters for each layer and the total number of parameters in the model."
  },
  {
    "objectID": "ex/w11/exercises11.html#exercise-3---convolutional-layers",
    "href": "ex/w11/exercises11.html#exercise-3---convolutional-layers",
    "title": "CSC477 - Fall 2024",
    "section": "Exercise 3 - Convolutional Layers",
    "text": "Exercise 3 - Convolutional Layers\nConsider the following \\(4\\times 4 \\times 1\\) input X and a \\(2\\times 2 \\times 1\\) convolutional kernel K with no bias term\n\\[\nX =\n\\bpmat\n1 & 0 & 1 & -1 \\\\\n1 & 0 & 1 & 0 \\\\\n0 & 3 & 0 & 1 \\\\\n1 & -1 & 0 & 1\n\\epmat, \\qquad\n%\nK = \\bpmat\n1, & 2 \\\\\n0, & 1 \\\\\n\\epmat\n\\]\n\nWhat is the output of the convolutional layer for the case of stride 1 and no padding?\nWhat if we have stride 2 and no padding?\nWhat if we have stride 2 and zero-padding of size 1?"
  },
  {
    "objectID": "ex/w11/exercises11.html#exercise-4---scaled-dot-product-attention",
    "href": "ex/w11/exercises11.html#exercise-4---scaled-dot-product-attention",
    "title": "CSC477 - Fall 2024",
    "section": "Exercise 4 - Scaled Dot-Product Attention",
    "text": "Exercise 4 - Scaled Dot-Product Attention\nConsider the matrices \\(Q\\), \\(K\\), \\(V\\) given by \\[\nQ = \\bpmat\n1 & 3\\\\\n0 & 1\n\\epmat,\\quad\nK = \\bpmat\n1 & 1\\\\\n1 & 2\\\\\n0 & 1\n\\epmat,\\quad\nV=\\bpmat\n1 & 0 & -2\\\\\n2 & 1 & 2 \\\\\n0 & 3 & -1\n\\epmat.\n\\] Compute the context matrix \\(C\\) using the scaled dot product attention."
  },
  {
    "objectID": "ex/w11/questions/pytorch-parameter_count.html",
    "href": "ex/w11/questions/pytorch-parameter_count.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "Use PyTorch to load the alexnet model and automatically compute its number of parameters. Output the number of parameters for each layer and the total number of parameters in the model."
  },
  {
    "objectID": "ex/w11/questions/attn-transformers_by_hand-notes.html",
    "href": "ex/w11/questions/attn-transformers_by_hand-notes.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "import torch\nimport torch.nn.functional as F\n\n\nQ = torch.tensor([\n    [1, 3], \n    [0, 1]]).float()\n\nK = torch.tensor([\n    [1, 1], \n    [1, 2],\n    [0, 1]]).float()\n\nV = torch.tensor([\n    [1, 0, -2],\n    [2, 1, 2], \n    [0, 3, -1]]).float()\n\n\nd_k = torch.tensor(K.shape[1])\nM = torch.matmul(Q, K.transpose(0, 1)) / torch.sqrt(d_k)\nS = torch.exp(M) / torch.sum(torch.exp(M), dim=1).view(-1,1)\ntorch.matmul(S, V)\n\ntensor([[1.7981, 0.9986, 1.4429],\n        [1.2552, 1.2483, 0.2622]])\n\n\nLazy version\n\nF.scaled_dot_product_attention(Q, K, V)\n\ntensor([[1.7981, 0.9986, 1.4429],\n        [1.2552, 1.2483, 0.2622]])"
  },
  {
    "objectID": "ex/w11/questions/linalg-evs_to_mat.html",
    "href": "ex/w11/questions/linalg-evs_to_mat.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "You are given the following set of eigevalues and eigenvectors. Compute the corresponding matrix.\n\\(\\la_1 = 1\\), \\(\\la_2 = 2\\), \\(\\fv_1 = (\\sqrt{0.5}, \\sqrt{0.5})^\\top\\), \\(\\fv_2 = (\\sqrt{0.5},-\\sqrt{0.5})^\\top\\)."
  },
  {
    "objectID": "ex/w11/questions/pytorch-parameter_count-notes.html",
    "href": "ex/w11/questions/pytorch-parameter_count-notes.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "import torchvision\nalexnet = torchvision.models.alexnet()\n\nprint(\n    f\"Total number of parameters: {sum(p.numel() for p in alexnet.parameters())}\")\n\nprint(\"\\n\\nParameter Overview in the backbone:\")\nfor layer in alexnet.features:\n    print(f\"{layer}: {sum(p.numel() for p in layer.parameters())}\")\n\nprint(\"\\n\\nParameter Ovewview in the head:\")\nfor layer in alexnet.classifier:\n    print(layer, sum(p.numel() for p in layer.parameters()))\n\nTotal number of parameters: 61100840\n\n\nParameter Overview in the backbone:\nConv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2)): 23296\nReLU(inplace=True): 0\nMaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False): 0\nConv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)): 307392\nReLU(inplace=True): 0\nMaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False): 0\nConv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)): 663936\nReLU(inplace=True): 0\nConv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)): 884992\nReLU(inplace=True): 0\nConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)): 590080\nReLU(inplace=True): 0\nMaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False): 0\n\n\nParameter Ovewview in the head:\nDropout(p=0.5, inplace=False) 0\nLinear(in_features=9216, out_features=4096, bias=True) 37752832\nReLU(inplace=True) 0\nDropout(p=0.5, inplace=False) 0\nLinear(in_features=4096, out_features=4096, bias=True) 16781312\nReLU(inplace=True) 0\nLinear(in_features=4096, out_features=1000, bias=True) 4097000"
  },
  {
    "objectID": "ex/w11/questions/linalg-evs_to_mat-notes.html",
    "href": "ex/w11/questions/linalg-evs_to_mat-notes.html",
    "title": "Eigenvalues and Eigenvectors to matrices",
    "section": "",
    "text": "Part (a) is straightforward. One needs to simply concatenate the given eigenvalues in a matrix.\n\nimport numpy as np\n\nla1 = 1\nla2 = 2\nev1 = np.array([np.sqrt(.5), np.sqrt(.5)])\nev2 = np.array([np.sqrt(.5), -np.sqrt(.5)])\n\nLa = np.diag([la1, la2])\nO = np.array([ev1, ev2])\n\n# Check orthogonality\nprint(np.dot(ev1, ev2))\n\nnp.matmul(np.matmul(O, La), O.T)\n\n4.266421588589642e-17\n\n\narray([[ 1.5, -0.5],\n       [-0.5,  1.5]])\n\n\nPart (b) has a catch. The eigenvectors are not normalized. So, we need to normalize them first before concatenating them in a matrix.\n\nla1 = 2\nla2 = 3\nla3 = 4\nev1 = np.array([np.sqrt(1/3), np.sqrt(1/3), np.sqrt(1/3)])\nev2 = np.array([np.sqrt(.5), -np.sqrt(.5), 0])\nev3 = np.array([np.sqrt(1/6), np.sqrt(1/6), -np.sqrt(2/3)])\n\nLa = np.diag([la1, la2, la3])\nO = np.array([ev1, ev2, ev3])\n\nprint(np.matmul(O, O.T))\nprint(np.matmul(np.matmul(O, La), np.linalg.inv(O)))\n\n[[ 1.00000000e+00 -3.39032612e-18  2.15314570e-17]\n [-3.39032612e-18  1.00000000e+00  1.84419141e-17]\n [ 2.15314570e-17  1.84419141e-17  1.00000000e+00]]\n[[ 3.         -0.40824829 -0.70710678]\n [-0.40824829  2.5        -0.28867513]\n [-0.70710678 -0.28867513  3.5       ]]\n\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n/Users/igor/Cloud/GIT/csc413/20239/ex/w11/questions/linalg-evs_to_mat-notes.ipynb Cell 5 line 1\n      &lt;a href='vscode-notebook-cell:/Users/igor/Cloud/GIT/csc413/20239/ex/w11/questions/linalg-evs_to_mat-notes.ipynb#W4sZmlsZQ%3D%3D?line=8'&gt;9&lt;/a&gt; O = np.array([ev1, ev2, ev3])\n     &lt;a href='vscode-notebook-cell:/Users/igor/Cloud/GIT/csc413/20239/ex/w11/questions/linalg-evs_to_mat-notes.ipynb#W4sZmlsZQ%3D%3D?line=10'&gt;11&lt;/a&gt; print(np.matmul(O, O.T))\n---&gt; &lt;a href='vscode-notebook-cell:/Users/igor/Cloud/GIT/csc413/20239/ex/w11/questions/linalg-evs_to_mat-notes.ipynb#W4sZmlsZQ%3D%3D?line=11'&gt;12&lt;/a&gt; print(np.matmul(np.matmul(O, La), np.linalg.inv(O)))*np.sqrt(.5)\n\nTypeError: unsupported operand type(s) for *: 'NoneType' and 'float'"
  },
  {
    "objectID": "ex/w11/questions/cnn-by_hand-notes.html",
    "href": "ex/w11/questions/cnn-by_hand-notes.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\n\nconv = nn.Conv2d(1, 1, kernel_size=2, stride=1, padding=0, bias=False)\n\nx = torch.tensor([\n    [ 1, 0, 1, -1],\n    [ 1, 0, 1, 0],\n    [ 0, 3, 0, 1],\n    [ 1, -1, 0, 1]\n])\n\nk = torch.tensor([\n    [ 1, 2],\n    [ 0, 1]\n])\n\nconv.weight.data = k.view(1, 1, 2, 2).float()\n\nprint(conv(x.view(1, 1, 4, 4).float()))\n\ntensor([[[[ 1.,  3., -1.],\n          [ 4.,  2.,  2.],\n          [ 5.,  3.,  3.]]]], grad_fn=&lt;ConvolutionBackward0&gt;)\n\n\n\nconv = nn.Conv2d(1, 1, kernel_size=2, stride=2, padding=0, bias=False)\nconv.weight.data = k.view(1, 1, 2, 2).float()\nprint(conv(x.view(1, 1, 4, 4).float()))\n\ntensor([[[[ 1., -1.],\n          [ 5.,  3.]]]], grad_fn=&lt;ConvolutionBackward0&gt;)\n\n\n\n# PyTorch uses zero-padding by default\nconv = nn.Conv2d(1, 1, kernel_size=2, stride=2, padding=1, bias=False)\nconv.weight.data = k.view(1, 1, 2, 2).float()\nprint(conv(x.view(1, 1, 4, 4).float()))\n\ntensor([[[[ 1.,  1.,  0.],\n          [ 2.,  2.,  0.],\n          [ 2., -1.,  1.]]]], grad_fn=&lt;ConvolutionBackward0&gt;)"
  },
  {
    "objectID": "ex/w10/exercises10.html",
    "href": "ex/w10/exercises10.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "What is the size of the output for a input tensor and a transposed convolutional layer if the parameters are given as follows (assume the number of channels is given in the first dimension).\n\nInput tensor size: \\(3\\times 2\\times 2\\)\nTransposed convolution: \\(3\\times 3\\) kernel, stride 1, output channels: 2\nInput tensor size: \\(3\\times 5\\times 5\\)\nTransposed convolutional: \\(2\\times 2\\) kernel, stride 2, output channels: 4\nInput tensor size: \\(c_{in}\\times h\\times w\\)\nTransposed convolutional: \\(3\\times 3\\) kernel, stride 2, output channels: 2\nInput tensor size: \\(c_{in}\\times h\\times w\\)\nTransposed convolutional: \\(h_k\\times w_k\\) kernel, stride \\(s\\), output channels: \\(c_{out}\\)"
  },
  {
    "objectID": "ex/w10/exercises10.html#exercise-1---transposed-convolution-output-sizes",
    "href": "ex/w10/exercises10.html#exercise-1---transposed-convolution-output-sizes",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "What is the size of the output for a input tensor and a transposed convolutional layer if the parameters are given as follows (assume the number of channels is given in the first dimension).\n\nInput tensor size: \\(3\\times 2\\times 2\\)\nTransposed convolution: \\(3\\times 3\\) kernel, stride 1, output channels: 2\nInput tensor size: \\(3\\times 5\\times 5\\)\nTransposed convolutional: \\(2\\times 2\\) kernel, stride 2, output channels: 4\nInput tensor size: \\(c_{in}\\times h\\times w\\)\nTransposed convolutional: \\(3\\times 3\\) kernel, stride 2, output channels: 2\nInput tensor size: \\(c_{in}\\times h\\times w\\)\nTransposed convolutional: \\(h_k\\times w_k\\) kernel, stride \\(s\\), output channels: \\(c_{out}\\)"
  },
  {
    "objectID": "ex/w10/exercises10.html#exercise-2---transposed-convolution-parameter-sizes",
    "href": "ex/w10/exercises10.html#exercise-2---transposed-convolution-parameter-sizes",
    "title": "CSC477 - Fall 2024",
    "section": "Exercise 2 - Transposed Convolution Parameter Sizes",
    "text": "Exercise 2 - Transposed Convolution Parameter Sizes\nWhat is the number of learnable parameters for each of the following transposed convolution layers defined in PyTorch. Try to calculate those by hand first and use pytorch later to verify your results.\n\nnn.ConvTranspose2d(in_channels=3, out_channels=2, kernel_size=3, stride=1)\nnn.ConvTranspose2d(in_channels=3, out_channels=10, kernel_size=3, stride=2)\nnn.ConvTranspose2d(in_channels=3, out_channels=2, kernel_size=4, stride=5)\nnn.ConvTranspose2d(in_channels=3, out_channels=4, kernel_size=3, stride=23)"
  },
  {
    "objectID": "ex/w10/exercises10.html#exercise-3---transposed-convolution-by-hand",
    "href": "ex/w10/exercises10.html#exercise-3---transposed-convolution-by-hand",
    "title": "CSC477 - Fall 2024",
    "section": "Exercise 3 - Transposed Convolution by Hand",
    "text": "Exercise 3 - Transposed Convolution by Hand\nYou are given an input matrix \\(X\\) (consisting of a single channel) and a kernel \\(K\\) as follows: \\[\nX = \\bpmat\n1 & 0 & 2 \\\\\n2 & 3 & 0 \\\\\n-1 & 0 & 3\n\\epmat, \\quad\nK = \\bpmat\n1 & 0 \\\\\n1 & 2\n\\epmat\n\\]\n\nCompute the transposed convolution by hand assuming stride 2.\nCompute the transposed convolution by hand assuming stride 1.\nUse PyTorch to verify your answers.\nImplement the transposed convolution in PyTorch without using its own implementaiton. You can assume no bias term a square kernel and no separate batch dimension. I.e. your task is to implement the following function\n\ndef conv_transpose2d(inp, weight, stride=1):\n    # inp - input of shape (C_in, H, W)\n    # weight - kernel of shape (C_in, C_out, K, K)\n    # stride - stride of the transposed convolution\n    # RETURNS\n    # output - output of shape (C_out, H_out, W_out)\n    #\n    # YOUR CODE HERE\n    return output"
  },
  {
    "objectID": "ex/w10/questions/tconv-by_hand.html",
    "href": "ex/w10/questions/tconv-by_hand.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "You are given an input matrix \\(X\\) (consisting of a single channel) and a kernel \\(K\\) as follows: \\[\nX = \\bpmat\n1 & 0 & 2 \\\\\n2 & 3 & 0 \\\\\n-1 & 0 & 3\n\\epmat, \\quad\nK = \\bpmat\n1 & 0 \\\\\n1 & 2\n\\epmat\n\\]\n\nCompute the transposed convolution by hand assuming stride 2.\nCompute the transposed convolution by hand assuming stride 1.\nUse PyTorch to verify your answers.\nImplement the transposed convolution in PyTorch without using its own implementaiton. You can assume no bias term a square kernel and no separate batch dimension. I.e. your task is to implement the following function\n\ndef conv_transpose2d(inp, weight, stride=1):\n    # inp - input of shape (C_in, H, W)\n    # weight - kernel of shape (C_in, C_out, K, K)\n    # stride - stride of the transposed convolution\n    # RETURNS\n    # output - output of shape (C_out, H_out, W_out)\n    #\n    # YOUR CODE HERE\n    return output"
  },
  {
    "objectID": "ex/w10/questions/tconv-params-notes.html",
    "href": "ex/w10/questions/tconv-params-notes.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\n\n\ntconv = nn.ConvTranspose2d(in_channels=3, out_channels=2, kernel_size=3)\nprint([p.numel() for p in tconv.parameters()])\n\ntconv = nn.ConvTranspose2d(\n    in_channels=3, out_channels=10, kernel_size=3)\nprint([p.numel() for p in tconv.parameters()])\n\ntconv = nn.ConvTranspose2d(\n    in_channels=3, out_channels=2, kernel_size=4)\nprint([p.numel() for p in tconv.parameters()])\n\ntconv = nn.ConvTranspose2d(\n    in_channels=3, out_channels=4, kernel_size=3)\nprint([p.numel() for p in tconv.parameters()])\n\n[54, 2]\n[270, 10]\n[96, 2]\n[108, 4]"
  },
  {
    "objectID": "ex/w10/questions/tconv-sizes-notes.html",
    "href": "ex/w10/questions/tconv-sizes-notes.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\n\nCompute the output sizes for the exercises.\n\nx = torch.randn(1, 3, 2, 2)\ntransp_conv = nn.ConvTranspose2d(\n    in_channels=3, out_channels=2, kernel_size=3, stride=1)\ntransp_conv(x).shape\n\ntorch.Size([1, 2, 4, 4])\n\n\n\nx = torch.randn(1, 3, 5, 5)\ntransp_conv = nn.ConvTranspose2d(\n    in_channels=3, out_channels=4, kernel_size=2, stride=2)\ntransp_conv(x).shape\n\ntorch.Size([1, 4, 10, 10])"
  },
  {
    "objectID": "ex/w10/questions/tconv-sizes.html",
    "href": "ex/w10/questions/tconv-sizes.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "What is the size of the output for a input tensor and a transposed convolutional layer if the parameters are given as follows (assume the number of channels is given in the first dimension).\n\nInput tensor size: \\(3\\times 2\\times 2\\)\nTransposed convolution: \\(3\\times 3\\) kernel, stride 1, output channels: 2\nInput tensor size: \\(3\\times 5\\times 5\\)\nTransposed convolutional: \\(2\\times 2\\) kernel, stride 2, output channels: 4\nInput tensor size: \\(c_{in}\\times h\\times w\\)\nTransposed convolutional: \\(3\\times 3\\) kernel, stride 2, output channels: 2\nInput tensor size: \\(c_{in}\\times h\\times w\\)\nTransposed convolutional: \\(h_k\\times w_k\\) kernel, stride \\(s\\), output channels: \\(c_{out}\\)"
  },
  {
    "objectID": "ex/w07/exercises07.html",
    "href": "ex/w07/exercises07.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "The exercises this week involve some old material so you can check your learning and understanding."
  },
  {
    "objectID": "ex/w07/exercises07.html#exercise-1---maximum-likelihood-estimator",
    "href": "ex/w07/exercises07.html#exercise-1---maximum-likelihood-estimator",
    "title": "CSC477 - Fall 2024",
    "section": "Exercise 1 - Maximum Likelihood Estimator",
    "text": "Exercise 1 - Maximum Likelihood Estimator\nAssume you are given datapoints \\((x_i)_{i=1}^N\\) with \\(x_i\\in\\R\\) coming from a Exponential distribution. The probability density function of a exponential distribution is given by \\(f(x) = \\la \\exp(-\\la x)\\) with \\(x\\in\\R\\). Derive the maximum likelihood estimator of the parameter \\(\\la\\)."
  },
  {
    "objectID": "ex/w07/exercises07.html#exercise-2---convolutional-layers",
    "href": "ex/w07/exercises07.html#exercise-2---convolutional-layers",
    "title": "CSC477 - Fall 2024",
    "section": "Exercise 2 - Convolutional Layers",
    "text": "Exercise 2 - Convolutional Layers\nConsider the following \\(4\\times 4 \\times 1\\) input X and a \\(2\\times 2 \\times 1\\) convolutional kernel K with no bias term\n\\[\nX =\n\\bpmat\n1 & 2 & -1 & 1 \\\\\n1 & 0 & 1 & 0 \\\\\n0 & 1 & 0 & 2 \\\\\n2 & 1 & 0 & -1\n\\epmat, \\qquad\n%\nK = \\bpmat\n1, & 0 \\\\\n2, & 1 \\\\\n\\epmat\n\\]\n\nWhat is the output of the convolutional layer for the case of stride 1 and no padding?\nWhat if we have stride 2 and no padding?\nWhat if we have stride 2 and zero-padding of size 1?"
  },
  {
    "objectID": "ex/w07/exercises07.html#exercise-3---computational-parameter-counting",
    "href": "ex/w07/exercises07.html#exercise-3---computational-parameter-counting",
    "title": "CSC477 - Fall 2024",
    "section": "Exercise 3 - Computational Parameter Counting",
    "text": "Exercise 3 - Computational Parameter Counting\nUse PyTorch to load the vgg11 model and automatically compute its number of parameters. Output the number of parameters for each layer and the total number of parameters in the model."
  },
  {
    "objectID": "ex/w07/exercises07.html#exercise-4---influence-functions",
    "href": "ex/w07/exercises07.html#exercise-4---influence-functions",
    "title": "CSC477 - Fall 2024",
    "section": "Exercise 4 - Influence Functions",
    "text": "Exercise 4 - Influence Functions\nLet \\(\\hte\\) and \\(\\hte(\\ve)\\) be as defined in class. Show that the first order Taylor expansion of \\(\\hte(\\ve)\\) around \\(\\ve=0\\) is given by the equation given in class, i.e. by \\[\\begin{align*}\n\\hat{\\theta}({\\epsilon}) \\approx \\hat{\\theta} + \\epsilon\\frac{d\\hat{\\theta}(\\epsilon)}{d\\epsilon} {\\Bigr |}_{\\epsilon=0} .\n\\end{align*}\\]"
  },
  {
    "objectID": "ex/w07/exercises07-notes.html",
    "href": "ex/w07/exercises07-notes.html",
    "title": "Solution Computations",
    "section": "",
    "text": "This file is for internal use where we compute the solutions to the exercises. I also compute the solutions for seemingly easy exercises here to use the code later for automation of the process."
  },
  {
    "objectID": "ex/w07/exercises07-notes.html#cnn-size",
    "href": "ex/w07/exercises07-notes.html#cnn-size",
    "title": "Solution Computations",
    "section": "CNN Size",
    "text": "CNN Size\n\nimport torch\nimport torch.nn as nn\n\nconv = nn.Conv2d(1, 1, kernel_size=2, stride=1, padding=0, bias=False)\n\nx = torch.tensor([\n    [ 1, 2, -1, 1],\n    [ 1, 0, 1, 0],\n    [ 0, 1, 0, 2],\n    [ 2, 1, 0, -1]\n])\n\nk = torch.tensor([\n    [ 1, 0],\n    [ 2, 1]\n])\n\nconv.weight.data = k.view(1, 1, 2, 2).float()\n\nprint(conv(x.view(1, 1, 4, 4).float()))\n\ntensor([[[[ 3.,  3.,  1.],\n          [ 2.,  2.,  3.],\n          [ 5.,  3., -1.]]]], grad_fn=&lt;ConvolutionBackward0&gt;)\n\n\n\nconv = nn.Conv2d(1, 1, kernel_size=2, stride=2, padding=0, bias=False)\nconv.weight.data = k.view(1, 1, 2, 2).float()\nprint(conv(x.view(1, 1, 4, 4).float()))\n\ntensor([[[[ 3.,  1.],\n          [ 5., -1.]]]], grad_fn=&lt;ConvolutionBackward0&gt;)\n\n\n\n# PyTorch uses zero-padding by default\nconv = nn.Conv2d(1, 1, kernel_size=2, stride=2, padding=1, bias=False)\nconv.weight.data = k.view(1, 1, 2, 2).float()\nprint(conv(x.view(1, 1, 4, 4).float()))\n\ntensor([[[[ 1.,  3.,  2.],\n          [ 0.,  2.,  4.],\n          [ 0.,  1., -1.]]]], grad_fn=&lt;ConvolutionBackward0&gt;)"
  },
  {
    "objectID": "ex/w07/exercises07-notes.html#vgg-weight-count",
    "href": "ex/w07/exercises07-notes.html#vgg-weight-count",
    "title": "Solution Computations",
    "section": "VGG weight count",
    "text": "VGG weight count\n\nimport torchvision\nvgg11 = torchvision.models.vgg.vgg11(pretrained=False)\n\nprint(\n    f\"Total number of parameters: {sum(p.numel() for p in vgg11.parameters())}\")\n\nprint(\"\\n\\nParameter Overview in the backbone:\")\nfor layer in vgg11.features:\n    print(f\"{layer}: {sum(p.numel() for p in layer.parameters())}\")\n\nprint(\"\\n\\nParameter Ovewview in the head:\")\nfor layer in vgg11.classifier:\n    print(layer, sum(p.numel() for p in layer.parameters()))\n\n/Users/igor/Conda/envs/csc413f23/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/Users/igor/Conda/envs/csc413f23/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n  warnings.warn(msg)\n\n\nTotal number of parameters: 132863336\n\n\nParameter Overview in the backbone:\nConv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)): 1792\nReLU(inplace=True): 0\nMaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False): 0\nConv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)): 73856\nReLU(inplace=True): 0\nMaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False): 0\nConv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)): 295168\nReLU(inplace=True): 0\nConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)): 590080\nReLU(inplace=True): 0\nMaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False): 0\nConv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)): 1180160\nReLU(inplace=True): 0\nConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)): 2359808\nReLU(inplace=True): 0\nMaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False): 0\nConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)): 2359808\nReLU(inplace=True): 0\nConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)): 2359808\nReLU(inplace=True): 0\nMaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False): 0\n\n\nParameter Ovewview in the head:\nLinear(in_features=25088, out_features=4096, bias=True) 102764544\nReLU(inplace=True) 0\nDropout(p=0.5, inplace=False) 0\nLinear(in_features=4096, out_features=4096, bias=True) 16781312\nReLU(inplace=True) 0\nDropout(p=0.5, inplace=False) 0\nLinear(in_features=4096, out_features=1000, bias=True) 4097000"
  },
  {
    "objectID": "ex/w07/questions/cnn-by_hand-sol.html",
    "href": "ex/w07/questions/cnn-by_hand-sol.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "Here, we simply apply the convolutional kernel over each \\(2\\times 2\\) patch of the input. There are 9 such patches. The output \\(Y\\) is then\n\n\\[\nY = \\bpmat\n3 &  3 &  1 \\\\\n2 &  2 &  3 \\\\\n5 &  3 & -1\n\\epmat\n\\]\n\nSame idea except that we skip every other patch resulting in only 4 patches. The output \\(Y\\) is then\n\n\\[\nY = \\bpmat\n3 & 1 \\\\\n5 & -1\n\\epmat\n\\]\n\nNow, we have added zeros on each side of the input. The resulting \\(6\\times 6\\) padded input \\(X_\\mathrm{padded}\\) and corresponding output \\(Y\\) are\n\n\\[\nX_\\mathrm{padded} =\n\\bpmat\n0 & 0 & 0 &  0 &  0 & 0 \\\\\n0 & 1 & 2 & -1 &  1 & 0 \\\\\n0 & 1 & 0 &  1 &  0 & 0 \\\\\n0 & 0 & 1 &  0 &  2 & 0 \\\\\n0 & 2 & 1 &  0 & -1 & 0 \\\\\n0 & 0 & 0 &  0 &  0 & 0\n\\epmat,\n\\qquad\nY = \\bpmat\n1 & 3 & 2 \\\\\n0 & 2 & 4 \\\\\n0 & 1 & -1\n\\epmat\n\\]"
  },
  {
    "objectID": "ex/w07/questions/pytorch-parameter_count.html",
    "href": "ex/w07/questions/pytorch-parameter_count.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "Use PyTorch to load the vgg11 model and automatically compute its number of parameters. Output the number of parameters for each layer and the total number of parameters in the model."
  },
  {
    "objectID": "ex/w07/questions/pytorch-parameter_count-sol.html",
    "href": "ex/w07/questions/pytorch-parameter_count-sol.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "First, we hvae to load the vgg11 model which is part of torchvision as has been shown in the lecture:\nimport torchvision\nvgg11 = torchvision.models.vgg.vgg11(pretrained=False)\nThe number of parameters for the entire model, is the easier part: We can simply use the parameters() iterator which returns the set of parameters for each module. Those can then be counted using the numel() method resulting in\nsum(p.numel() for p in vgg11.parameters())\nObtaining the number of parameters for each of the layer requires looking into the source code of the vgg11 model. All VGG models are ultimately instantiated by using the VGG class. Its forward pass looks like this:\nx = self.features(x)\nx = self.avgpool(x)\nx = torch.flatten(x, 1)\nx = self.classifier(x)\nA closer look at the implementation reveals that we can obtain the individual layers parameter by simply iterating over the self.features and self.classifier modules. The self.avgpool module does not have any parameters. The following code snippet shows how to obtain the number of parameters for each layer of the convolutional backbone:\nfor layer in vgg11.features:\n    print(layer, sum(p.numel() for p in layer.parameters()))\nTo get the number of paramers in the cnn head, simply update the code snippet to iterate over vgg11.classifier instead of vgg11.features."
  },
  {
    "objectID": "ex/w07/questions/prob-mle_exp_dist.html",
    "href": "ex/w07/questions/prob-mle_exp_dist.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "Assume you are given datapoints \\((x_i)_{i=1}^N\\) with \\(x_i\\in\\R\\) coming from a Exponential distribution. The probability density function of a exponential distribution is given by \\(f(x) = \\la \\exp(-\\la x)\\) with \\(x\\in\\R\\). Derive the maximum likelihood estimator of the parameter \\(\\la\\)."
  },
  {
    "objectID": "ex/w09/exercises09_solution.html",
    "href": "ex/w09/exercises09_solution.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "You are given a set of vectors \\[\n\\fh_1 = (1,2,3)^\\top,\\quad\n\\fh_2 = (1,2,1)^\\top,\\quad\n\\fh_3 = (0,1,-1)^\\top\n\\] and an alignment source vector \\(\\fs=(1,2,1)^\\top\\). Compute the resulting dot-product attention weights \\(\\alpha_i\\) for \\(i=1,2,3\\) and the resulting context vector \\(\\fc\\).\n\n\nFirst we compute the dot products between \\(\\fs\\) and the \\(\\fh_i\\) and apply softmax resulting in: \\[\n\\fa = \\text{Softmax}\\li(\n\\bpmat\n1 & 1 & 0\\\\\n2 & 2 & 1\\\\\n3 & 1 & -1\n\\epmat^\\top\n\\cdot\n\\bpmat 1\\\\ 2\\\\ 1\\epmat\n\\ri)\n\\approx\n\\bpmat 0.88\\\\ 0.12\\\\ 0.00\\epmat\n\\] The resulting context vector is then computed as a weighted sum of the \\(\\fh_i\\): \\[\n\\fc\n  = a_1 \\fh_1 + a_2 \\fh_2 + a_3 \\fh_3\n  \\approx \\bpmat 1.00\\\\ 2.00\\\\ 2.76\\epmat\n\\]\nA simple implementation yielding the solution is as follows:\nimport torch\nh = torch.tensor([[1, 2, 3], [1,2,1], [0,1,-1]])\ns = torch.tensor([1,2,1])\na = torch.matmul(h,s).float()\na = torch.exp(a)/torch.sum(torch.exp(a)) # Softmax\nc = a[0] * h[0] + a[1] * h[1] + a[2] * h[2]\nprint(c)"
  },
  {
    "objectID": "ex/w09/exercises09_solution.html#exercise-1---dot-product-attention",
    "href": "ex/w09/exercises09_solution.html#exercise-1---dot-product-attention",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "You are given a set of vectors \\[\n\\fh_1 = (1,2,3)^\\top,\\quad\n\\fh_2 = (1,2,1)^\\top,\\quad\n\\fh_3 = (0,1,-1)^\\top\n\\] and an alignment source vector \\(\\fs=(1,2,1)^\\top\\). Compute the resulting dot-product attention weights \\(\\alpha_i\\) for \\(i=1,2,3\\) and the resulting context vector \\(\\fc\\).\n\n\nFirst we compute the dot products between \\(\\fs\\) and the \\(\\fh_i\\) and apply softmax resulting in: \\[\n\\fa = \\text{Softmax}\\li(\n\\bpmat\n1 & 1 & 0\\\\\n2 & 2 & 1\\\\\n3 & 1 & -1\n\\epmat^\\top\n\\cdot\n\\bpmat 1\\\\ 2\\\\ 1\\epmat\n\\ri)\n\\approx\n\\bpmat 0.88\\\\ 0.12\\\\ 0.00\\epmat\n\\] The resulting context vector is then computed as a weighted sum of the \\(\\fh_i\\): \\[\n\\fc\n  = a_1 \\fh_1 + a_2 \\fh_2 + a_3 \\fh_3\n  \\approx \\bpmat 1.00\\\\ 2.00\\\\ 2.76\\epmat\n\\]\nA simple implementation yielding the solution is as follows:\nimport torch\nh = torch.tensor([[1, 2, 3], [1,2,1], [0,1,-1]])\ns = torch.tensor([1,2,1])\na = torch.matmul(h,s).float()\na = torch.exp(a)/torch.sum(torch.exp(a)) # Softmax\nc = a[0] * h[0] + a[1] * h[1] + a[2] * h[2]\nprint(c)"
  },
  {
    "objectID": "ex/w09/exercises09_solution.html#exercise-2---attention-in-transformers",
    "href": "ex/w09/exercises09_solution.html#exercise-2---attention-in-transformers",
    "title": "CSC477 - Fall 2024",
    "section": "Exercise 2 - Attention in Transformers",
    "text": "Exercise 2 - Attention in Transformers\nTransformers use a scaled dot product attention mechanism given by \\[\nC\n= \\text{attention}(Q, K, V)\n= \\text{softmax}\\left(\\fr{QK^\\top}{\\sqrt{d}}\\right) V,\n\\] where \\(Q\\in\\R^{n_q\\times d_k}\\), \\(K\\in\\R^{n_k\\times d_k}\\), \\(V\\in\\R^{n_k\\times d_v}\\).\n\nIs the softmax function here applied row-wise or column-wise? What is the shape of the result?\nWhat is the value of \\(d\\)? Why is it needed?\nWhat is the computational complexity of this attention mechanism? How many additions and multiplications are required? Assume the canonical matrix multiplcation and not counting \\(\\exp(x)\\) towards computational cost.\nIn the masked variant of the module, a masking matrix is added before the softmax function is applied. What are its values and its shape? For simplicity, assume \\(n_q=n_k\\).\n\n\nSolution\n\nThe softmax function is applied row-wise and the shape of the result is \\(n_q\\times n_k\\). One way to see this is by looking at the shape of the dot product \\(QK^\\top\\) which is \\(n_q\\times n_k\\). Each row represents the pre-softmax scores of all keys and a given query. Because we need to normalize our attention weights per query, the normalization happens along the rows.\nThe value of \\(d\\) is \\(d_k\\). It is needed to scale the dot product so that the gradient of the softmax function does not vanish.\nTo obtain the computational complexity, let’s look at all the operations individually:\n\n\\(QK^\\top\\) requires \\(n_q n_k d_k\\) multiplications and \\(n_qn_k(d_k-1)\\) additions.\nDividing by \\(\\sqrt{d_k}\\) needs to be carried out \\(n_q n_k\\) times.\nApplying the softmax function can be implemented in \\(n_q n_k\\) divisions and \\(n_q(n_k-1)\\) additions.\nThe final matrix multiplication requires \\(n_qd_vn_k\\) multiplications and \\(n_q d_v (n_k-1)\\) additions.\n\nThe masking matrix is a triangular matrix with \\(-\\infty\\) on its top right half. This results in softmax weights being \\(0\\) for all key-query combinations to which \\(-\\infty\\) is added."
  },
  {
    "objectID": "ex/w09/exercises09_solution.html#exercise-3---scaled-dot-product-attention-by-hand",
    "href": "ex/w09/exercises09_solution.html#exercise-3---scaled-dot-product-attention-by-hand",
    "title": "CSC477 - Fall 2024",
    "section": "Exercise 3 - Scaled Dot-Product Attention by Hand",
    "text": "Exercise 3 - Scaled Dot-Product Attention by Hand\nConsider the matrices \\(Q\\), \\(K\\), \\(V\\) given by \\[\nQ = \\bpmat\n1 & 2\\\\\n3 & 1\n\\epmat,\\quad\nK = \\bpmat\n2 & 1\\\\\n1 & 1\\\\\n0 & 1\n\\epmat,\\quad\nV=\\bpmat\n1 & 2 & -2\\\\\n1 & 1 & 2 \\\\\n0 & 1 & -1\n\\epmat.\n\\] Compute the context matrix \\(C\\) using the scaled dot product attention.\n\nSolution\nThe resulting context matrix is given by: \\[\nC\\approx\n\\bpmat\n0.86 & 1.58 & -0.72\\\\\n0.99 & 1.88 & -1.56\n\\epmat\n\\] A simple implementation would look as follows:\nimport torch\nQ = torch.tensor([[1, 2], [3, 1]]).float()\nK = torch.tensor([[2, 1], [1, 1], [0, 1]]).float()\nV = torch.tensor([[1, 2, -2], [1, 1, 2], [0, 1, -1]]).float()\nd_k = torch.tensor(K.shape[1])\nM = torch.matmul(Q, K.transpose(0, 1)) / torch.sqrt(d_k)\nS = torch.exp(M) / torch.sum(torch.exp(M), dim=1).view(-1,1)\ntorch.matmul(S, V)\nPytorch also provides a function for scaled dot product attention:\nimport torch.nn.functional as F\nF.scaled_dot_product_attention(Q, K, V)"
  },
  {
    "objectID": "ex/w09/questions/attn-transformers_by_hand-sol.html",
    "href": "ex/w09/questions/attn-transformers_by_hand-sol.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "The resulting context matrix is given by: \\[\nC\\approx\n\\bpmat\n0.86 & 1.58 & -0.72\\\\\n0.99 & 1.88 & -1.56\n\\epmat\n\\] A simple implementation would look as follows:\nimport torch\nQ = torch.tensor([[1, 2], [3, 1]]).float()\nK = torch.tensor([[2, 1], [1, 1], [0, 1]]).float()\nV = torch.tensor([[1, 2, -2], [1, 1, 2], [0, 1, -1]]).float()\nd_k = torch.tensor(K.shape[1])\nM = torch.matmul(Q, K.transpose(0, 1)) / torch.sqrt(d_k)\nS = torch.exp(M) / torch.sum(torch.exp(M), dim=1).view(-1,1)\ntorch.matmul(S, V)\nPytorch also provides a function for scaled dot product attention:\nimport torch.nn.functional as F\nF.scaled_dot_product_attention(Q, K, V)"
  },
  {
    "objectID": "ex/w09/questions/attn-dot_product_attention.html",
    "href": "ex/w09/questions/attn-dot_product_attention.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "You are given a set of vectors \\[\n\\fh_1 = (1,2,3)^\\top,\\quad\n\\fh_2 = (1,2,1)^\\top,\\quad\n\\fh_3 = (0,1,-1)^\\top\n\\] and an alignment source vector \\(\\fs=(1,2,1)^\\top\\). Compute the resulting dot-product attention weights \\(\\alpha_i\\) for \\(i=1,2,3\\) and the resulting context vector \\(\\fc\\)."
  },
  {
    "objectID": "ex/w09/questions/attn-dot_product_attention-sol.html",
    "href": "ex/w09/questions/attn-dot_product_attention-sol.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "First we compute the dot products between \\(\\fs\\) and the \\(\\fh_i\\) and apply softmax resulting in: \\[\n\\fa = \\text{Softmax}\\li(\n\\bpmat\n1 & 1 & 0\\\\\n2 & 2 & 1\\\\\n3 & 1 & -1\n\\epmat^\\top\n\\cdot\n\\bpmat 1\\\\ 2\\\\ 1\\epmat\n\\ri)\n\\approx\n\\bpmat 0.88\\\\ 0.12\\\\ 0.00\\epmat\n\\] The resulting context vector is then computed as a weighted sum of the \\(\\fh_i\\): \\[\n\\fc\n  = a_1 \\fh_1 + a_2 \\fh_2 + a_3 \\fh_3\n  \\approx \\bpmat 1.00\\\\ 2.00\\\\ 2.76\\epmat\n\\]\nA simple implementation yielding the solution is as follows:\nimport torch\nh = torch.tensor([[1, 2, 3], [1,2,1], [0,1,-1]])\ns = torch.tensor([1,2,1])\na = torch.matmul(h,s).float()\na = torch.exp(a)/torch.sum(torch.exp(a)) # Softmax\nc = a[0] * h[0] + a[1] * h[1] + a[2] * h[2]\nprint(c)"
  },
  {
    "objectID": "ex/w09/questions/attn-dot_product_attention-notes.html",
    "href": "ex/w09/questions/attn-dot_product_attention-notes.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "import torch\nimport torch.nn\nimport torch.nn.functional as F\n\n\nh = torch.tensor([[1, 2, 3], [1,2,1], [0,1,-1]])\ns = torch.tensor([1,2,1])\na = torch.matmul(h,s).float()\na = torch.exp(a)/torch.sum(torch.exp(a)) # Softmax\nc = a[0] * h[0] + a[1] * h[1] + a[2] * h[2]\nprint(c, a)\n\ntensor([0.9992, 1.9992, 2.7586]) tensor([8.8009e-01, 1.1911e-01, 8.0254e-04])"
  },
  {
    "objectID": "ex/w08/exercises08_solution.html",
    "href": "ex/w08/exercises08_solution.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "Suppose we are training a vanilla RNN like below to determine whether a sentence expresses positive or negative sentiment. This RNN will be a character-level RNN where \\(x^{(1)}, \\ldots, x^{(T)}\\) is the sequence of input characters. The RNN is given as follows: \\[\\begin{align*}\nh^{(t)} &= \\tanh\\li(U x^{(t)} + W h^{(t-1)} + b\\ri) \\\\\ny &= \\sigma\\li(V h^{(T)} + d\\ri)\n\\end{align*}\\]\n\nHow many times do we need to apply the weight matrix \\(U\\), \\(W\\), and \\(V\\)?\nWhat are the shapes of the matrices \\(U\\), \\(W\\), and \\(V\\)?\nHow many addition and multiplication operations are required to make a prediction? You can assume that no addition and multiplications are performed when applying the tanh and sigmoid activation functions.\n\n\n\n\nWe will need to compute \\(h^{(t)}\\) for \\(t = 1, \\ldots, T\\). Each of this computation requires applying the weight matrices \\(W\\) and \\(T\\) once. The matrix \\(V\\) is only applied once at the end. Therefore, we need to apply \\(W\\) and \\(U\\) \\(T\\) times each and \\(V\\) once.\nThe shape of \\(U\\) is \\(d_h \\times d_x\\), the shape of \\(W\\) is \\(d_h \\times d_h\\), and the shape of \\(V\\) is \\(d_y \\times d_h\\), where \\(d_h\\) is the dimensionality of the \\(h^{(i)}\\) (i.e. \\(h^{(i)}\\in\\R^{d_h}\\)), \\(d_x\\) is the dimensionality of the inputs \\(x^{(i)}\\), and \\(d_y\\) is the dimensionality of the ouput \\(y\\).\nFor each of the \\(T\\) steps, we need to perform two matrix-vector multiplications (one for \\(Ux^{(i)}\\) and one for \\(Uh^{(i)}\\)) and two vector additions. To compute the output, we need one additional matrix-vector multiplication and one vector addition."
  },
  {
    "objectID": "ex/w08/exercises08_solution.html#exercise-1---rnn-for-sentiment-analysis",
    "href": "ex/w08/exercises08_solution.html#exercise-1---rnn-for-sentiment-analysis",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "Suppose we are training a vanilla RNN like below to determine whether a sentence expresses positive or negative sentiment. This RNN will be a character-level RNN where \\(x^{(1)}, \\ldots, x^{(T)}\\) is the sequence of input characters. The RNN is given as follows: \\[\\begin{align*}\nh^{(t)} &= \\tanh\\li(U x^{(t)} + W h^{(t-1)} + b\\ri) \\\\\ny &= \\sigma\\li(V h^{(T)} + d\\ri)\n\\end{align*}\\]\n\nHow many times do we need to apply the weight matrix \\(U\\), \\(W\\), and \\(V\\)?\nWhat are the shapes of the matrices \\(U\\), \\(W\\), and \\(V\\)?\nHow many addition and multiplication operations are required to make a prediction? You can assume that no addition and multiplications are performed when applying the tanh and sigmoid activation functions.\n\n\n\n\nWe will need to compute \\(h^{(t)}\\) for \\(t = 1, \\ldots, T\\). Each of this computation requires applying the weight matrices \\(W\\) and \\(T\\) once. The matrix \\(V\\) is only applied once at the end. Therefore, we need to apply \\(W\\) and \\(U\\) \\(T\\) times each and \\(V\\) once.\nThe shape of \\(U\\) is \\(d_h \\times d_x\\), the shape of \\(W\\) is \\(d_h \\times d_h\\), and the shape of \\(V\\) is \\(d_y \\times d_h\\), where \\(d_h\\) is the dimensionality of the \\(h^{(i)}\\) (i.e. \\(h^{(i)}\\in\\R^{d_h}\\)), \\(d_x\\) is the dimensionality of the inputs \\(x^{(i)}\\), and \\(d_y\\) is the dimensionality of the ouput \\(y\\).\nFor each of the \\(T\\) steps, we need to perform two matrix-vector multiplications (one for \\(Ux^{(i)}\\) and one for \\(Uh^{(i)}\\)) and two vector additions. To compute the output, we need one additional matrix-vector multiplication and one vector addition."
  },
  {
    "objectID": "ex/w08/exercises08_solution.html#exercise-2---scalar-rnn",
    "href": "ex/w08/exercises08_solution.html#exercise-2---scalar-rnn",
    "title": "CSC477 - Fall 2024",
    "section": "Exercise 2 - Scalar RNN",
    "text": "Exercise 2 - Scalar RNN\n\nSuppose we have the following vanilla RNN network, where the inputs and hidden units are scalars. \\[\\begin{align*}\nh^{(t)} &= \\tanh\\li(w \\cdot h^{(t-1)} + u \\cdot x^{(t-1)} + b_h\\ri) \\\\\ny &= \\sigma\\li(v \\cdot h^{(T)} + b_y\\ri)\n\\end{align*}\\]\n\nShow that if \\(|w| &lt; 1\\), and the number of time steps \\(T\\) is large, then the gradient \\(\\frac{\\partial y}{\\partial x^{(0)}}\\) vanishes.\nWhy is the result from Part (a) troubling?\n\n\nSolution\n\nTo make the sequence length \\(T\\) explicit in the notation, we will write \\(y\\) instead of \\(y_T\\). Formally, what we have to show is \\[\n|w|&lt;1 \\implies \\lim_{T\\to\\infty} \\fr{\\partial y_T}{\\partial x^{(0)}} = 0 .\n\\] For the proof, we expand the derivative of \\(y_T\\) with respect to \\(x^{(0)}\\) using the chain rule: \\[\n\\begin{aligned}\n\\fr{\\partial y_T}{\\partial x^{(0)}}\n  & = \\si'\\li(v \\cdot h^{(T)} + b_y\\ri)\n    \\cdot v \\cdot \\fr{\\partial h^{(T)}}{\\partial x^{(0)}} \\\\\n  & =\n  \\si'\\li(v \\cdot h^{(T)} + b_y\\ri)\n    \\cdot v\n    \\cdot\n      \\underbrace{\n        \\tanh'\\li(w \\cdot h^{(T-1)} + u \\cdot x^{(T-1)} + b_h\\ri)\n      }_{A_{T-1}(x^{(0)})}\n    \\cdot w \\cdot \\fr{\\partial h^{(T-1)}}{\\partial x^{(0)}} \\\\\n& = \\ldots \\\\\n& = \\si'\\li(v \\cdot h^{(T)} + b_y\\ri)\n    \\cdot v\n    \\cdot \\prod_{t=2}^{T-1} A_{t}(x^{(0)})\n    \\cdot w^{T-1} \\cdot \\fr{\\partial h^{(1)}}{\\partial x^{(0)}} .\\\\\n\\end{aligned}\n\\] Using this, we can analyze the absolute value of the derivative \\(\\partial y_T/\\partial x^{(0)}\\). For \\(\\tanh\\) and \\(\\si\\), the absolute value of their respective derivatives is bounded by \\(1\\). Thus, we have \\[\n\\begin{aligned}\n\\li|\\fr{\\partial y_T}{\\partial x^{(0)}} \\ri|\n& =\n\\underbrace{\n  \\li|\\si'\\li(v \\cdot h^{(T)} + b_y\\ri) \\ri|\n}_{\\leq 1}\n\\cdot |v|\n\\cdot \\prod_{t=2}^{T-1}\n  \\underbrace{\\li| A_{t}(x^{(0)}) \\ri|}_{\\leq 1}\n\\cdot \\li|w^{T-1}\\ri|\n\\cdot \\li|\\fr{\\partial h^{(1)}}{\\partial x^{(0)}}\\ri| \\\\\n& \\leq |v|\n\\cdot \\li|w^{T-1}\\ri|\n\\cdot \\li|\\fr{\\partial h^{(1)}}{\\partial x^{(0)}}\\ri| \\\\\n\\end{aligned}\n\\] Because \\(|w|&lt;1\\), this converges to \\(0\\) as \\(T\\to\\infty\\) and thus \\(|\\partial y_T/\\partial x^{(0)}|\\) also converges to \\(0\\), i.e. the gradient vanishes.\nIt implies that in the considered setting, the input has no impact on the output."
  },
  {
    "objectID": "ex/w08/exercises08_solution.html#exercise-3---rnn-addition",
    "href": "ex/w08/exercises08_solution.html#exercise-3---rnn-addition",
    "title": "CSC477 - Fall 2024",
    "section": "Exercise 3 - RNN Addition",
    "text": "Exercise 3 - RNN Addition\n\nIn this problem, you will implement a recurrent neural network which implements binary addition. The inputs are given as binary sequences, starting with the significant binary digit. (It is easier to start from the least significant bit, just like how you did addition in grade school.) The sequences will be padded with at least one zero as the most significant digit, so that the output length is the same as the input length. For example, the problem \\(100111 + 110010\\), whose target output value is \\(1011001\\), will be represented as follows: \\[\\begin{align*}\n\\bf{x}^{(1)} = \\begin{bmatrix}1 \\\\ 0\\end{bmatrix},\n\\bf{x}^{(2)} = \\begin{bmatrix}1 \\\\ 1\\end{bmatrix},\n\\bf{x}^{(3)} = \\begin{bmatrix}1 \\\\ 0\\end{bmatrix},\n\\bf{x}^{(4)} = \\begin{bmatrix}0 \\\\ 0\\end{bmatrix},\n\\bf{x}^{(5)} = \\begin{bmatrix}0 \\\\ 1\\end{bmatrix},\n\\bf{x}^{(6)} = \\begin{bmatrix}1 \\\\ 1\\end{bmatrix},\n\\bf{x}^{(7)} = \\begin{bmatrix}0 \\\\ 0\\end{bmatrix}\n\\end{align*}\\]\nWith the target output: \\[\\begin{align*}\ny^{(1)} = 1,\ny^{(2)} = 0,\ny^{(3)} = 0,\ny^{(4)} = 1,\ny^{(5)} = 1,\ny^{(6)} = 0,\ny^{(7)} = 1,\n\\end{align*}\\]\nThere are two input units corresponding to the two inputs, and one output unit. Therefore, the pattern of inputs and outputs for this example would be:\n\nDesign, by hand, the weights and biases for an RNN which has two input units, three hidden units, and one output unit, which implements binary addition as discussed above. All of the units use the hard threshold activation function (\\(f(x) = 1\\) if \\(x &gt; 0\\) and \\(0\\) otherwise). In particular, specify weight matrices \\(\\mathbf{U}\\), \\(\\mathbf{v}\\), and \\(\\mathbf{W}\\), bias vector \\(\\mathbf{b}_{\\mathbf{h}}\\), and scalar bias \\(b_y\\) for the following architecture: \\[\\begin{align*}\nh^{(t)} &= f(\\bf{W}h^{(t-1)} + \\bf{U}\\bf{x}^{(t)} + \\bf{b_h}) \\\\\ny^{(t)} &= f(\\bf{v}^T h^{(t)} + b_y)\n\\end{align*}\\]\n\nWhat are the shapes of \\(\\mathbf{U}\\), \\(\\mathbf{v}\\), \\(\\mathbf{W}\\), and \\(\\mathbf{b}_{\\mathbf{h}}\\)?\nCome up with values for \\(\\mathbf{U}\\), \\(\\mathbf{W}\\), and \\(\\mathbf{b}_{\\mathbf{h}}\\). Justify your answer. Hint: When performing binary addition, in addition to adding up two digits in a column, we need to track whether there is a digit from the previous column. We will choose one of the three units in \\(\\bf{h}^{(t)}\\), say \\(\\bf{h}_2^{(t)}\\), to represent this carry digit. You may also find it helpful to set \\(\\bf{h}_1\\) to activate if the sum of the 3 digits is at least 1, \\(\\bf{h}_2\\) to activate if the sum is at least 2, and \\(\\bf{h}_3\\) to activate if the sum is at least 3.\nCome up with the values of \\(\\bf{v}\\) and \\(b_y\\). Justify your answer.\n\n\nSolution\n\nSince the inputs \\(\\bf{x}^{(t)}\\) are \\(2 \\times 1\\) and the hidden units \\(\\bf{h}^{(t)}\\) are \\(2 \\times 1\\), we should have:\n\n\\(\\bf{W}\\) is \\(3 \\times 3\\)\n\\(\\bf{U}\\) is \\(3 \\times 2\\)\n\\(\\bf{b}_h\\) is \\(3 \\times 1\\)\n\\(\\bf{v}\\) is \\(3 \\times 1\\)\n\nWe will follow the hint and implement the addition in our RNN such that:\n\nThe first of our hidden units \\(h_1^{(t)}\\) is 1 if and only if the sum \\(S^{(t)} \\doteq x_1^{(t)} + x_2^{(t)} + c^{(t-1)} \\geq 1\\), where by \\(c^{(t-1)}\\) we denote a carry (\\(\\bf{h_2}^{(t-1)}\\) from the previous addition). Note, these \\(S^{(t)}\\) and \\(c^{(t-1)}\\) are not variables of the model, merely our notation to help us to work out the solution.\nThe \\(h_2^{(t)}\\) is 1 iff the sum \\(S^{(t)} \\geq 2\\),\nand \\(h_3^{(t)}\\) is 1 iff the sum \\(S^{(t)}\\) is 3.\n\nNotice that the carry \\(c^{(t-1)}\\) is going to be 1 iff \\(h_2^{(t-1)}=1\\) and 0 otherwise, i.e. when the previous addition was 2 or 3. Therefore to compute \\(h_i^{(t)}\\) we need to first compute the sum \\(S^{(t)} = x_1^{(t)} + x_2^{(t)} + h_2^{(t-1)}\\) and then offset it by \\(-i+1\\) so that after applying the hard threshold function we get the desired value as specified above. This can be achieved with the following set of parameters: \\[\n\\mathbf{U}= \\begin{bmatrix}\n    1 & 1 \\\\\n    1 & 1 \\\\\n    1 & 1 \\end{bmatrix},\\quad\n\\mathbf{W}=\\begin{bmatrix}\n    0 & 1 & 0 \\\\\n    0 & 1 & 0 \\\\\n    0 & 1 & 0\\end{bmatrix},\\quad\n\\mathbf{b_h}= \\begin{bmatrix}\n    -0.5 \\\\\n    -1.5 \\\\\n    -2.5 \\end{bmatrix}.\n\\]\nTo compute the output \\(y^{(t)}\\) we need to check if the \\(S^{(t)}\\) is 1 or 3, that is, if either \\(h_1^{(t)} = 1\\) while all other hidden units are zero or all hidden units are 1. We can accomplish this by setting: \\(\\mathbf{v}=\\begin{bmatrix} 1, -1, 1 \\end{bmatrix}\\) and \\(b_y = -0.5\\)."
  },
  {
    "objectID": "ex/w08/questions/rnn-sentiment.html",
    "href": "ex/w08/questions/rnn-sentiment.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "Suppose we are training a vanilla RNN like below to determine whether a sentence expresses positive or negative sentiment. This RNN will be a character-level RNN where \\(x^{(1)}, \\ldots, x^{(T)}\\) is the sequence of input characters. The RNN is given as follows: \\[\\begin{align*}\nh^{(t)} &= \\tanh\\li(U x^{(t)} + W h^{(t-1)} + b\\ri) \\\\\ny &= \\sigma\\li(V h^{(T)} + d\\ri)\n\\end{align*}\\]\n\nHow many times do we need to apply the weight matrix \\(U\\), \\(W\\), and \\(V\\)?\nWhat are the shapes of the matrices \\(U\\), \\(W\\), and \\(V\\)?\nHow many addition and multiplication operations are required to make a prediction? You can assume that no addition and multiplications are performed when applying the tanh and sigmoid activation functions."
  },
  {
    "objectID": "ex/w08/questions/rnn-scalar-sol.html",
    "href": "ex/w08/questions/rnn-scalar-sol.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "To make the sequence length \\(T\\) explicit in the notation, we will write \\(y\\) instead of \\(y_T\\). Formally, what we have to show is \\[\n|w|&lt;1 \\implies \\lim_{T\\to\\infty} \\fr{\\partial y_T}{\\partial x^{(0)}} = 0 .\n\\] For the proof, we expand the derivative of \\(y_T\\) with respect to \\(x^{(0)}\\) using the chain rule: \\[\n\\begin{aligned}\n\\fr{\\partial y_T}{\\partial x^{(0)}}\n  & = \\si'\\li(v \\cdot h^{(T)} + b_y\\ri)\n    \\cdot v \\cdot \\fr{\\partial h^{(T)}}{\\partial x^{(0)}} \\\\\n  & =\n  \\si'\\li(v \\cdot h^{(T)} + b_y\\ri)\n    \\cdot v\n    \\cdot\n      \\underbrace{\n        \\tanh'\\li(w \\cdot h^{(T-1)} + u \\cdot x^{(T-1)} + b_h\\ri)\n      }_{A_{T-1}(x^{(0)})}\n    \\cdot w \\cdot \\fr{\\partial h^{(T-1)}}{\\partial x^{(0)}} \\\\\n& = \\ldots \\\\\n& = \\si'\\li(v \\cdot h^{(T)} + b_y\\ri)\n    \\cdot v\n    \\cdot \\prod_{t=2}^{T-1} A_{t}(x^{(0)})\n    \\cdot w^{T-1} \\cdot \\fr{\\partial h^{(1)}}{\\partial x^{(0)}} .\\\\\n\\end{aligned}\n\\] Using this, we can analyze the absolute value of the derivative \\(\\partial y_T/\\partial x^{(0)}\\). For \\(\\tanh\\) and \\(\\si\\), the absolute value of their respective derivatives is bounded by \\(1\\). Thus, we have \\[\n\\begin{aligned}\n\\li|\\fr{\\partial y_T}{\\partial x^{(0)}} \\ri|\n& =\n\\underbrace{\n  \\li|\\si'\\li(v \\cdot h^{(T)} + b_y\\ri) \\ri|\n}_{\\leq 1}\n\\cdot |v|\n\\cdot \\prod_{t=2}^{T-1}\n  \\underbrace{\\li| A_{t}(x^{(0)}) \\ri|}_{\\leq 1}\n\\cdot \\li|w^{T-1}\\ri|\n\\cdot \\li|\\fr{\\partial h^{(1)}}{\\partial x^{(0)}}\\ri| \\\\\n& \\leq |v|\n\\cdot \\li|w^{T-1}\\ri|\n\\cdot \\li|\\fr{\\partial h^{(1)}}{\\partial x^{(0)}}\\ri| \\\\\n\\end{aligned}\n\\] Because \\(|w|&lt;1\\), this converges to \\(0\\) as \\(T\\to\\infty\\) and thus \\(|\\partial y_T/\\partial x^{(0)}|\\) also converges to \\(0\\), i.e. the gradient vanishes.\nIt implies that in the considered setting, the input has no impact on the output."
  },
  {
    "objectID": "ex/w08/questions/rnn-addition-sol.html",
    "href": "ex/w08/questions/rnn-addition-sol.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "Since the inputs \\(\\bf{x}^{(t)}\\) are \\(2 \\times 1\\) and the hidden units \\(\\bf{h}^{(t)}\\) are \\(2 \\times 1\\), we should have:\n\n\\(\\bf{W}\\) is \\(3 \\times 3\\)\n\\(\\bf{U}\\) is \\(3 \\times 2\\)\n\\(\\bf{b}_h\\) is \\(3 \\times 1\\)\n\\(\\bf{v}\\) is \\(3 \\times 1\\)\n\nWe will follow the hint and implement the addition in our RNN such that:\n\nThe first of our hidden units \\(h_1^{(t)}\\) is 1 if and only if the sum \\(S^{(t)} \\doteq x_1^{(t)} + x_2^{(t)} + c^{(t-1)} \\geq 1\\), where by \\(c^{(t-1)}\\) we denote a carry (\\(\\bf{h_2}^{(t-1)}\\) from the previous addition). Note, these \\(S^{(t)}\\) and \\(c^{(t-1)}\\) are not variables of the model, merely our notation to help us to work out the solution.\nThe \\(h_2^{(t)}\\) is 1 iff the sum \\(S^{(t)} \\geq 2\\),\nand \\(h_3^{(t)}\\) is 1 iff the sum \\(S^{(t)}\\) is 3.\n\nNotice that the carry \\(c^{(t-1)}\\) is going to be 1 iff \\(h_2^{(t-1)}=1\\) and 0 otherwise, i.e. when the previous addition was 2 or 3. Therefore to compute \\(h_i^{(t)}\\) we need to first compute the sum \\(S^{(t)} = x_1^{(t)} + x_2^{(t)} + h_2^{(t-1)}\\) and then offset it by \\(-i+1\\) so that after applying the hard threshold function we get the desired value as specified above. This can be achieved with the following set of parameters: \\[\n\\mathbf{U}= \\begin{bmatrix}\n    1 & 1 \\\\\n    1 & 1 \\\\\n    1 & 1 \\end{bmatrix},\\quad\n\\mathbf{W}=\\begin{bmatrix}\n    0 & 1 & 0 \\\\\n    0 & 1 & 0 \\\\\n    0 & 1 & 0\\end{bmatrix},\\quad\n\\mathbf{b_h}= \\begin{bmatrix}\n    -0.5 \\\\\n    -1.5 \\\\\n    -2.5 \\end{bmatrix}.\n\\]\nTo compute the output \\(y^{(t)}\\) we need to check if the \\(S^{(t)}\\) is 1 or 3, that is, if either \\(h_1^{(t)} = 1\\) while all other hidden units are zero or all hidden units are 1. We can accomplish this by setting: \\(\\mathbf{v}=\\begin{bmatrix} 1, -1, 1 \\end{bmatrix}\\) and \\(b_y = -0.5\\)."
  },
  {
    "objectID": "ex/w06/exercises06.html",
    "href": "ex/w06/exercises06.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "Show that for two independent random variables, \\(X,Y\\) and arbitraty \\(a,b\\in\\R\\), the following equality holds \\[\\Var(aX+bY) = a^2\\cdot\\Var(X) + b^2\\cdot\\Var(Y).\\]"
  },
  {
    "objectID": "ex/w06/exercises06.html#exercise-1---variance",
    "href": "ex/w06/exercises06.html#exercise-1---variance",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "Show that for two independent random variables, \\(X,Y\\) and arbitraty \\(a,b\\in\\R\\), the following equality holds \\[\\Var(aX+bY) = a^2\\cdot\\Var(X) + b^2\\cdot\\Var(Y).\\]"
  },
  {
    "objectID": "ex/w06/exercises06.html#exercise-2---variance-bias-decomposistion",
    "href": "ex/w06/exercises06.html#exercise-2---variance-bias-decomposistion",
    "title": "CSC477 - Fall 2024",
    "section": "Exercise 2 - Variance / Bias Decomposistion",
    "text": "Exercise 2 - Variance / Bias Decomposistion\nLet \\(D=\\lbrace (x_i,y_i) | i=1 \\ldots n\\rbrace\\) be a dataset obtained from the true underlying data distribution \\(P\\), i.e. \\(D\\sim P^n\\). And let \\(h_D(\\cdot)\\) be a classifier trained on \\(D\\). Show the variance bias decomposition \\[\n\\underbrace{\\mathbb{E}_{D,x,y} \\li[ (h_D(x) - y)^2 \\ri]}_{\\text{Expected test error}}  \n  = \\underbrace{\\mathbb{E}_{D,x} \\li[ (h_D(x) - \\hat{h}(x))^2 \\ri]}_{\\text{Variance}} +\n  \\underbrace{\\mathbb{E}_{x,y} \\li[ (\\hat{y}(x) - y)^2 \\ri]}_{\\text{Noise}}  +  \\underbrace{\\mathbb{E}_{x} \\li[ (\\hat{h}(x) - \\hat{y}(x))^2 \\ri]}_{\\text{Bias}^2}\n\\] where \\(\\hat{h}(x) = \\mathbb{E}_{D \\sim P^n}[h_D(x)]\\) is the expected regressor over possible training sets, given the learning algorithm \\(\\mathcal{A}\\) and \\(\\hat{y}(x) = \\mathbb{E}_{y|x}[y]\\) is the expected label given \\(x\\). As mentioned in the lecture, labels might not be deterministic given x. To carry out the proof, proceed in the following steps:\n\nShow that the following identity holds \\[\\begin{align}\n\\E_{D,x,y}\\li[\\li[h_{D}(x) - y\\ri]^{2}\\ri]\n= \\E_{D, x}\\li[(\\hh_{D}(x) - \\hh(x))^{2}\\ri] + \\E_{x, y} \\li[\\li(\\hh(x) - y\\ri)^{2}\\ri].\n   \\end{align}\\]\nNext, show \\[\\begin{align}\nE_{x, y} \\li[ \\li(\\hh(x) - y \\ri)^{2}\\ri]\n=E_{x, y} \\li[\\li(\\hy(x) - y\\ri)^{2}\\ri] + E_{x} \\li[\\li(\\hh(x) - \\hy(x)\\ri)^{2}\\ri]\n\\end{align}\\] which completes the proof by substituting (2) into (1)."
  },
  {
    "objectID": "ex/w06/exercises06.html#exercise-3---ensembling",
    "href": "ex/w06/exercises06.html#exercise-3---ensembling",
    "title": "CSC477 - Fall 2024",
    "section": "Exercise 3 - Ensembling",
    "text": "Exercise 3 - Ensembling\nDownload the file exercises06-ensembling.ipynb from quercus. It contains basic Pytorch code training a classifier on MNIST. Modify that code such that it trains an ensemble of 5-10 neural networks and computes their average prediction once trained."
  },
  {
    "objectID": "ex/w06/exercises06_solution.html",
    "href": "ex/w06/exercises06_solution.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "Show that for two independent random variables, \\(X,Y\\) and arbitraty \\(a,b\\in\\R\\), the following equality holds \\[\\Var(aX+bY) = a^2\\cdot\\Var(X) + b^2\\cdot\\Var(Y).\\]\n\n\nFirst, we use the definition of variance and rewrite the left hand side as \\[\n\\Var(aX+bY) = \\E\\li[ (aX+bY)^2 \\ri] - \\E[aX+bY]^2.\n\\] Next, we expand the squares for each of the terms on the right hand side: \\[\\begin{align*}\n\\E\\li[ (aX+bY)^2 \\ri]\n  &= \\E\\li[ a^2X^2 + 2abXY + b^2Y^2 \\ri] \\\\\n  &= a^2\\E\\li[ X^2 \\ri] + 2ab\\E[ XY ] + b^2\\E\\li[ Y^2 \\ri] \\\\\n  &= a^2\\E\\li[ X^2 \\ri] + 2ab\\E[ X]\\,\\E[Y] + b^2\\E\\li[ Y^2 \\ri] ,\\\\\n\\E[aX+bY]^2\n  &= \\li(a\\E[X] + b\\E[Y]\\ri)^2 \\\\\n  &= a^2\\E[X]^2 + 2ab\\E[X]\\,\\E[Y] + b^2\\E[Y]^2.\n\\end{align*}\\] Subtracting the two terms, we get \\[\\begin{align*}\n& \\E\\li[ (aX+bY)^2 \\ri] - \\E[aX+bY]^2 \\\\\n& \\quad = a^2\\E\\li[ X^2 \\ri] + 2ab\\E[ X]\\,\\E[Y] + b^2\\E\\li[ Y^2 \\ri]\n    - a^2\\E[X]^2 - 2ab\\E[X]\\,\\E[Y] - b^2\\E[Y]^2 \\\\\n& \\quad = a^2\\E\\li[ X^2 \\ri] - a^2\\E[X]^2\n   + b^2\\E\\li[ Y^2 \\ri] - b^2\\E[Y]^2 \\\\\n& \\quad = a^2(\\E\\li[ X^2 \\ri] - \\E[X]^2)\n   + b^2\\li(\\E\\li[ Y^2 \\ri] - \\E[Y]^2\\ri) \\\\\n& \\quad = a^2\\cdot\\Var(X) + b^2\\cdot\\Var(Y).\n\\end{align*}\\]"
  },
  {
    "objectID": "ex/w06/exercises06_solution.html#exercise-1---variance",
    "href": "ex/w06/exercises06_solution.html#exercise-1---variance",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "Show that for two independent random variables, \\(X,Y\\) and arbitraty \\(a,b\\in\\R\\), the following equality holds \\[\\Var(aX+bY) = a^2\\cdot\\Var(X) + b^2\\cdot\\Var(Y).\\]\n\n\nFirst, we use the definition of variance and rewrite the left hand side as \\[\n\\Var(aX+bY) = \\E\\li[ (aX+bY)^2 \\ri] - \\E[aX+bY]^2.\n\\] Next, we expand the squares for each of the terms on the right hand side: \\[\\begin{align*}\n\\E\\li[ (aX+bY)^2 \\ri]\n  &= \\E\\li[ a^2X^2 + 2abXY + b^2Y^2 \\ri] \\\\\n  &= a^2\\E\\li[ X^2 \\ri] + 2ab\\E[ XY ] + b^2\\E\\li[ Y^2 \\ri] \\\\\n  &= a^2\\E\\li[ X^2 \\ri] + 2ab\\E[ X]\\,\\E[Y] + b^2\\E\\li[ Y^2 \\ri] ,\\\\\n\\E[aX+bY]^2\n  &= \\li(a\\E[X] + b\\E[Y]\\ri)^2 \\\\\n  &= a^2\\E[X]^2 + 2ab\\E[X]\\,\\E[Y] + b^2\\E[Y]^2.\n\\end{align*}\\] Subtracting the two terms, we get \\[\\begin{align*}\n& \\E\\li[ (aX+bY)^2 \\ri] - \\E[aX+bY]^2 \\\\\n& \\quad = a^2\\E\\li[ X^2 \\ri] + 2ab\\E[ X]\\,\\E[Y] + b^2\\E\\li[ Y^2 \\ri]\n    - a^2\\E[X]^2 - 2ab\\E[X]\\,\\E[Y] - b^2\\E[Y]^2 \\\\\n& \\quad = a^2\\E\\li[ X^2 \\ri] - a^2\\E[X]^2\n   + b^2\\E\\li[ Y^2 \\ri] - b^2\\E[Y]^2 \\\\\n& \\quad = a^2(\\E\\li[ X^2 \\ri] - \\E[X]^2)\n   + b^2\\li(\\E\\li[ Y^2 \\ri] - \\E[Y]^2\\ri) \\\\\n& \\quad = a^2\\cdot\\Var(X) + b^2\\cdot\\Var(Y).\n\\end{align*}\\]"
  },
  {
    "objectID": "ex/w06/exercises06_solution.html#exercise-2---variance-bias-decomposistion",
    "href": "ex/w06/exercises06_solution.html#exercise-2---variance-bias-decomposistion",
    "title": "CSC477 - Fall 2024",
    "section": "Exercise 2 - Variance / Bias Decomposistion",
    "text": "Exercise 2 - Variance / Bias Decomposistion\nLet \\(D=\\lbrace (x_i,y_i) | i=1 \\ldots n\\rbrace\\) be a dataset obtained from the true underlying data distribution \\(P\\), i.e. \\(D\\sim P^n\\). And let \\(h_D(\\cdot)\\) be a classifier trained on \\(D\\). Show the variance bias decomposition \\[\n\\underbrace{\\mathbb{E}_{D,x,y} \\li[ (h_D(x) - y)^2 \\ri]}_{\\text{Expected test error}}  \n  = \\underbrace{\\mathbb{E}_{D,x} \\li[ (h_D(x) - \\hat{h}(x))^2 \\ri]}_{\\text{Variance}} +\n  \\underbrace{\\mathbb{E}_{x,y} \\li[ (\\hat{y}(x) - y)^2 \\ri]}_{\\text{Noise}}  +  \\underbrace{\\mathbb{E}_{x} \\li[ (\\hat{h}(x) - \\hat{y}(x))^2 \\ri]}_{\\text{Bias}^2}\n\\] where \\(\\hat{h}(x) = \\mathbb{E}_{D \\sim P^n}[h_D(x)]\\) is the expected regressor over possible training sets, given the learning algorithm \\(\\mathcal{A}\\) and \\(\\hat{y}(x) = \\mathbb{E}_{y|x}[y]\\) is the expected label given \\(x\\). As mentioned in the lecture, labels might not be deterministic given x. To carry out the proof, proceed in the following steps:\n\nShow that the following identity holds \\[\\begin{align}\n\\E_{D,x,y}\\li[\\li[h_{D}(x) - y\\ri]^{2}\\ri]\n= \\E_{D, x}\\li[(\\hh_{D}(x) - \\hh(x))^{2}\\ri] + \\E_{x, y} \\li[\\li(\\hh(x) - y\\ri)^{2}\\ri].\n   \\end{align}\\]\nNext, show \\[\\begin{align}\nE_{x, y} \\li[ \\li(\\hh(x) - y \\ri)^{2}\\ri]\n=E_{x, y} \\li[\\li(\\hy(x) - y\\ri)^{2}\\ri] + E_{x} \\li[\\li(\\hh(x) - \\hy(x)\\ri)^{2}\\ri]\n\\end{align}\\] which completes the proof by substituting (2) into (1).\n\n\nSolution\n\nFirst, we reformulate (1) as \\[\\begin{align*}\n  \\E_{D,x,y}\\li[\\li[h_{D}(x) - y\\ri]^{2}\\ri]\n&= \\E_{D,x,y}\\li[\\li[\\li(h_{D}(x) - \\hh(x)\\ri) + \\li(\\hh(x) - y\\ri)\\ri]^{2}\\ri] \\nonumber \\\\\n&= \\E_{x, D}\\li[(\\hh_{D}(x) - \\hh(x))^{2}\\ri] + 2 \\mathrm{\\;} \\E_{x, y, D} \\li[\\li(h_{D}(x) - \\hh(x)\\ri)\\li(\\hh(x) - y\\ri)\\ri] + \\E_{x, y} \\li[\\li(\\hh(x) - y\\ri)^{2}\\ri]\n\\end{align*}\\] Next, we note that the second term in the above equation is zero because \\[\\begin{align*}\n\\E_{D,x, y} \\li[\\li(h_{D}(x) - \\hh(x)\\ri) \\li(\\hh(x) - y\\ri)\\ri] &= \\E_{x, y} \\li[\\E_{D} \\li[ h_{D}(x) - \\hh(x)\\ri] \\li(\\hh(x) - y\\ri) \\ri] \\\\\n&= \\E_{x, y} \\li[ \\li( \\E_{D} \\li[ h_{D}(x) \\ri] - \\hh(x) \\ri) \\li(\\hh(x) - y \\ri)\\ri] \\\\\n&= \\E_{x, y} \\li[ \\li(\\hh(x) - \\hh(x) \\ri) \\li(\\hh(x) - y \\ri)\\ri] \\\\\n&= \\E_{x, y} \\li[ 0 \\ri] \\\\\n&= 0\\ .\n\\end{align*}\\]\nThe proof here, is similar. We start by reformulating the second term in (2) as \\[\\begin{align*}\n\\E_{x, y} \\li[ \\li(\\hh(x) - y \\ri)^{2}\\ri] &= \\E_{x, y} \\li[ \\li(\\hh(x) -\\bar y(x) )+(\\bar y(x) - y \\ri)^{2}\\ri]  \\\\\n  &=\\E_{x, y} \\li[\\li(\\hy(x) - y\\ri)^{2}\\ri] + \\E_{x} \\li[\\li(\\hh(x) - \\hy(x)\\ri)^{2}\\ri] + 2 \\mathrm{\\;} \\E_{x, y} \\li[ \\li(\\hh(x) - \\hy(x)\\ri)\\li(\\hy(x) - y\\ri)\\ri]\n  \\end{align*}\\] Here, the third term is zero which follows from an analogous derivation as in (a). Thus, we have \\[\\begin{align*}\n\\E_{x, y} \\li[\\li(\\hh(x) - \\hy(x)\\ri)\\li(\\hy(x) - y\\ri)\\ri] &= \\E_{x}\\li[\\E_{y \\mid x} \\li[\\hy(x) - y \\ri] \\li(\\hh(x) - \\hy(x) \\ri) \\ri] \\\\\n&= \\E_{x} \\li[ \\E_{y \\mid x} \\li[ \\hy(x) - y\\ri] \\li(\\hh(x) - \\hy(x)\\ri)\\ri] \\\\\n&= \\E_{x} \\li[ \\li( \\hy(x) - \\E_{y \\mid x} \\li [ y \\ri]\\ri) \\li(\\hh(x) - \\hy(x)\\ri)\\ri] \\\\\n&= \\E_{x} \\li[ \\li( \\hy(x) - \\hy(x) \\ri) \\li(\\hh(x) - \\hy(x)\\ri)\\ri] \\\\\n&= \\E_{x} \\li[ 0 \\ri] \\\\\n&= 0\n  \\end{align*}\\]"
  },
  {
    "objectID": "ex/w06/exercises06_solution.html#exercise-3---ensembling",
    "href": "ex/w06/exercises06_solution.html#exercise-3---ensembling",
    "title": "CSC477 - Fall 2024",
    "section": "Exercise 3 - Ensembling",
    "text": "Exercise 3 - Ensembling\nDownload the file ex06-ensembling.ipynb from quercus. It contains basic Pytorch code training a classifier on MNIST. Modify that code such that it trains an ensemble of 5-10 neural networks and computes their average prediction once trained."
  },
  {
    "objectID": "ex/w06/questions/prob-variance.html",
    "href": "ex/w06/questions/prob-variance.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "Show that for two independent random variables, \\(X,Y\\) and arbitraty \\(a,b\\in\\R\\), the following equality holds \\[\\Var(aX+bY) = a^2\\cdot\\Var(X) + b^2\\cdot\\Var(Y).\\]"
  },
  {
    "objectID": "ex/w06/questions/ml-variance_bias_decomposition-sol.html",
    "href": "ex/w06/questions/ml-variance_bias_decomposition-sol.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "First, we reformulate (1) as \\[\\begin{align*}\n  \\E_{D,x,y}\\li[\\li[h_{D}(x) - y\\ri]^{2}\\ri]\n&= \\E_{D,x,y}\\li[\\li[\\li(h_{D}(x) - \\hh(x)\\ri) + \\li(\\hh(x) - y\\ri)\\ri]^{2}\\ri] \\nonumber \\\\\n&= \\E_{x, D}\\li[(\\hh_{D}(x) - \\hh(x))^{2}\\ri] + 2 \\mathrm{\\;} \\E_{x, y, D} \\li[\\li(h_{D}(x) - \\hh(x)\\ri)\\li(\\hh(x) - y\\ri)\\ri] + \\E_{x, y} \\li[\\li(\\hh(x) - y\\ri)^{2}\\ri]\n\\end{align*}\\] Next, we note that the second term in the above equation is zero because \\[\\begin{align*}\n\\E_{D,x, y} \\li[\\li(h_{D}(x) - \\hh(x)\\ri) \\li(\\hh(x) - y\\ri)\\ri] &= \\E_{x, y} \\li[\\E_{D} \\li[ h_{D}(x) - \\hh(x)\\ri] \\li(\\hh(x) - y\\ri) \\ri] \\\\\n&= \\E_{x, y} \\li[ \\li( \\E_{D} \\li[ h_{D}(x) \\ri] - \\hh(x) \\ri) \\li(\\hh(x) - y \\ri)\\ri] \\\\\n&= \\E_{x, y} \\li[ \\li(\\hh(x) - \\hh(x) \\ri) \\li(\\hh(x) - y \\ri)\\ri] \\\\\n&= \\E_{x, y} \\li[ 0 \\ri] \\\\\n&= 0\\ .\n\\end{align*}\\]\nThe proof here, is similar. We start by reformulating the second term in (2) as \\[\\begin{align*}\n\\E_{x, y} \\li[ \\li(\\hh(x) - y \\ri)^{2}\\ri] &= \\E_{x, y} \\li[ \\li(\\hh(x) -\\bar y(x) )+(\\bar y(x) - y \\ri)^{2}\\ri]  \\\\\n  &=\\E_{x, y} \\li[\\li(\\hy(x) - y\\ri)^{2}\\ri] + \\E_{x} \\li[\\li(\\hh(x) - \\hy(x)\\ri)^{2}\\ri] + 2 \\mathrm{\\;} \\E_{x, y} \\li[ \\li(\\hh(x) - \\hy(x)\\ri)\\li(\\hy(x) - y\\ri)\\ri]\n  \\end{align*}\\] Here, the third term is zero which follows from an analogous derivation as in (a). Thus, we have \\[\\begin{align*}\n\\E_{x, y} \\li[\\li(\\hh(x) - \\hy(x)\\ri)\\li(\\hy(x) - y\\ri)\\ri] &= \\E_{x}\\li[\\E_{y \\mid x} \\li[\\hy(x) - y \\ri] \\li(\\hh(x) - \\hy(x) \\ri) \\ri] \\\\\n&= \\E_{x} \\li[ \\E_{y \\mid x} \\li[ \\hy(x) - y\\ri] \\li(\\hh(x) - \\hy(x)\\ri)\\ri] \\\\\n&= \\E_{x} \\li[ \\li( \\hy(x) - \\E_{y \\mid x} \\li [ y \\ri]\\ri) \\li(\\hh(x) - \\hy(x)\\ri)\\ri] \\\\\n&= \\E_{x} \\li[ \\li( \\hy(x) - \\hy(x) \\ri) \\li(\\hh(x) - \\hy(x)\\ri)\\ri] \\\\\n&= \\E_{x} \\li[ 0 \\ri] \\\\\n&= 0\n  \\end{align*}\\]"
  },
  {
    "objectID": "course-overview.html",
    "href": "course-overview.html",
    "title": "Fall 2024 - CSC 2626: Imitation Learning for Robotics",
    "section": "",
    "text": "Course Overview\nIn the next few decades we are going to witness millions of people, from various backgrounds and levels of technical expertise, needing to effectively interact with robotic technologies on a daily basis. As such, people will need to modify the behavior of their robots without explicitly writing code, but by providing only a small number of kinesthetic or visual demonstrations, or even natural language commands. At the same time, robots should try to infer and predict the human’s intentions and internal objectives from past interactions, in order to provide assistance before it is explicitly asked. This graduate-level course will examine some of the most important papers in imitation learning for robot control, placing more emphasis on developments in the last 10 years. Its purpose is to familiarize students with the frontiers of this research area, to help them identify open problems, and to enable them to make a research contribution.\nThis course will broadly cover the following areas:\n\nImitating the policies of demonstrators (people, expensive algorithms, optimal controllers)\nConnections between imitation learning, optimal control, and reinforcement learning\nLearning the cost functions that best explain a set of demonstrations\nShared autonomy between humans and robots for real-time control\n\n\n\nPrerequisites\nYou need to be comfortable with: introductory machine learning concepts (such as from CSC411/CSC413/ECE521 or equivalent), linear algebra, basic multivariable calculus, intro to probability. You also need to have strong programming skills in Python. Note: if you don’t meet all the prerequisites above please contact the instructor by email. Optional, but recommended: experience with neural networks, such as from CSC321, introductory-level familiarity with reinforcement learning and control.\n\n\nCourse Delivery Details\n\nLectures: In-person, Mondays @ 1pm-4pm ET, Carr Hall 404\nAnnouncements will be posted on Quercus\nDiscussions will take place on Piazza\nZoom recordings will be posted on Quercus after lectures\nAnonymous feedback form for suggested improvements"
  },
  {
    "objectID": "course-team.html",
    "href": "course-team.html",
    "title": "Teaching team",
    "section": "",
    "text": "Instructor\n\nFlorian Shkurti is an assistant professor in computer science at the University of Toronto, where he leads the Robot Vision and Learning lab. He is a faculty member of the University of Toronto Robotics Institute, the Acceleration Consortium, and a faculty affiliate at Vector Institute. His research group develops methods that enable robots to learn to perceive, reason, plan, and act effectively and safely, particularly in dynamic environments and alongside humans. Application areas of his research include field robotics for environmental monitoring, visual navigation for autonomous vehicles, and mobile manipulation.\n\n\n\n\n\n\n\nOffice Hours\nLocation\n\n\n\n\nWednesdays 3-5pm ET\nZoom, In person office hours can be arranged by appointment\n\n\n\n\n\nTeaching Assistants\n\n\n\n\n\n\n\n\nTAs\nOffice Hours\nLocation\n\n\n\n\nYewoon Lee\nTuesday 11am - noon ET\nZoom\n\n\nYasasa Abeysirigoonawardena\nFriday 11am-noon ET\nZoom\n\n\nRadian Gondokaryono\nTBA\nZoom\n\n\n\nEmail : csc477-tas@cs.toronto.edu",
    "crumbs": [
      "Teaching Staff"
    ]
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html",
    "href": "supplemental/model-diagnostics-matrix.html",
    "title": "Model Diagnostics",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr. Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\nThis document discusses some of the mathematical details of the model diagnostics - leverage, standardized residuals, and Cook’s distance. We assume the reader knowledge of the matrix form for multiple linear regression. Please see Matrix Form of Linear Regression for a review."
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#introduction",
    "href": "supplemental/model-diagnostics-matrix.html#introduction",
    "title": "Model Diagnostics",
    "section": "Introduction",
    "text": "Introduction\nSuppose we have \\(n\\) observations. Let the \\(i^{th}\\) be \\((x_{i1}, \\ldots, x_{ip}, y_i)\\), such that \\(x_{i1}, \\ldots, x_{ip}\\) are the explanatory variables (predictors) and \\(y_i\\) is the response variable. We assume the data can be modeled using the least-squares regression model, such that the mean response for a given combination of explanatory variables follows the form in Equation 1.\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_p x_p\n\\tag{1}\\]\nWe can write the response for the \\(i^{th}\\) observation as shown in Equation 2.\n\\[\ny_i = \\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip} + \\epsilon_i\n\\tag{2}\\]\nsuch that \\(\\epsilon_i\\) is the amount \\(y_i\\) deviates from \\(\\mu\\{y|x_{i1}, \\ldots, x_{ip}\\}\\), the mean response for a given combination of explanatory variables. We assume each \\(\\epsilon_i \\sim N(0,\\sigma^2)\\), where \\(\\sigma^2\\) is a constant variance for the distribution of the response \\(y\\) for any combination of explanatory variables \\(x_1, \\ldots, x_p\\)."
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#matrix-form-for-the-regression-model",
    "href": "supplemental/model-diagnostics-matrix.html#matrix-form-for-the-regression-model",
    "title": "Model Diagnostics",
    "section": "Matrix Form for the Regression Model",
    "text": "Matrix Form for the Regression Model\nWe can represent the Equation 1 and Equation 2 using matrix notation. Let\n\\[\n\\mathbf{Y} = \\begin{bmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\y_n\\end{bmatrix}\n\\hspace{15mm}\n\\mathbf{X} = \\begin{bmatrix}x_{11} & x_{12} & \\dots & x_{1p} \\\\\nx_{21} & x_{22} & \\dots & x_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\nx_{n1} & x_{n2} & \\dots & x_{np} \\end{bmatrix}\n\\hspace{15mm}\n\\boldsymbol{\\beta}= \\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{bmatrix}\n\\hspace{15mm}\n\\boldsymbol{\\epsilon}= \\begin{bmatrix}\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix}\n\\tag{3}\\]\nThus,\n\\[\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{\\epsilon}\\]\nTherefore the estimated response for a given combination of explanatory variables and the associated residuals can be written as\n\\[\n\\hat{\\mathbf{Y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\hspace{10mm} \\mathbf{e} = \\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\n\\tag{4}\\]"
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#hat-matrix-leverage",
    "href": "supplemental/model-diagnostics-matrix.html#hat-matrix-leverage",
    "title": "Model Diagnostics",
    "section": "Hat Matrix & Leverage",
    "text": "Hat Matrix & Leverage\nRecall from the notes Matrix Form of Linear Regression that \\(\\hat{\\boldsymbol{\\beta}}\\) can be written as the following:\n\\[\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\n\\tag{5}\\]\nCombining Equation 4 and Equation 5, we can write \\(\\hat{\\mathbf{Y}}\\) as the following:\n\\[\n\\begin{aligned}\n\\hat{\\mathbf{Y}} &= \\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\[10pt]\n&= \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\\\\n\\end{aligned}\n\\tag{6}\\]\nWe define the hat matrix as an \\(n \\times n\\) matrix of the form \\(\\mathbf{H} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\). Thus Equation 6 becomes\n\\[\n\\hat{\\mathbf{Y}} = \\mathbf{H}\\mathbf{Y}\n\\tag{7}\\]\nThe diagonal elements of the hat matrix are a measure of how far the predictor variables of each observation are from the means of the predictor variables. For example, \\(h_{ii}\\) is a measure of how far the values of the predictor variables for the \\(i^{th}\\) observation, \\(x_{i1}, x_{i2}, \\ldots, x_{ip}\\), are from the mean values of the predictor variables, \\(\\bar{x}_1, \\bar{x}_2, \\ldots, \\bar{x}_p\\). In the case of simple linear regression, the \\(i^{th}\\) diagonal, \\(h_{ii}\\), can be written as\n\\[\nh_{ii} =  \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\sum_{j=1}^{n}(x_j-\\bar{x})^2}\n\\]\nWe call these diagonal elements, the leverage of each observation.\nThe diagonal elements of the hat matrix have the following properties:\n\n\\(0 \\leq h_ii \\leq 1\\)\n\\(\\sum\\limits_{i=1}^{n} h_{ii} = p+1\\), where \\(p\\) is the number of predictor variables in the model.\nThe mean hat value is \\(\\bar{h} = \\frac{\\sum\\limits_{i=1}^{n} h_{ii}}{n} = \\frac{p+1}{n}\\).\n\nUsing these properties, we consider a point to have high leverage if it has a leverage value that is more than 2 times the average. In other words, observations with leverage greater than \\(\\frac{2(p+1)}{n}\\) are considered to be high leverage points, i.e. outliers in the predictor variables. We are interested in flagging high leverage points, because they may have an influence on the regression coefficients.\nWhen there are high leverage points in the data, the regression line will tend towards those points; therefore, one property of high leverage points is that they tend to have small residuals. We will show this by rewriting the residuals from Equation 4 using Equation 7.\n\\[\n\\begin{aligned}\n\\mathbf{e} &= \\mathbf{Y} - \\hat{\\mathbf{Y}} \\\\[10pt]\n& = \\mathbf{Y} - \\mathbf{H}\\mathbf{Y} \\\\[10pt]\n&= (1-\\mathbf{H})\\mathbf{Y}\n\\end{aligned}\n\\tag{8}\\]\nNote that the identity matrix and hat matrix are idempotent, i.e. \\(\\mathbf{I}\\mathbf{I} = \\mathbf{I}\\), \\(\\mathbf{H}\\mathbf{H} = \\mathbf{H}\\). Thus, \\((\\mathbf{I} - \\mathbf{H})\\) is also idempotent. These matrices are also symmetric. Using these properties and Equation 8, we have that the variance-covariance matrix of the residuals \\(\\boldsymbol{e}\\), is\n\\[\n\\begin{aligned}\nVar(\\mathbf{e}) &= \\mathbf{e}\\mathbf{e}^T \\\\[10pt]\n&=  (1-\\mathbf{H})Var(\\mathbf{Y})^T(1-\\mathbf{H})^T \\\\[10pt]\n&= (1-\\mathbf{H})\\hat{\\sigma}^2(1-\\mathbf{H})^T  \\\\[10pt]\n&= \\hat{\\sigma}^2(1-\\mathbf{H})(1-\\mathbf{H})  \\\\[10pt]\n&= \\hat{\\sigma}^2(1-\\mathbf{H})\n\\end{aligned}\n\\tag{9}\\]\nwhere \\(\\hat{\\sigma}^2 = \\frac{\\sum_{i=1}^{n}e_i^2}{n-p-1}\\) is the estimated regression variance. Thus, the variance of the \\(i^{th}\\) residual is \\(Var(e_i) = \\hat{\\sigma}^2(1-h_{ii})\\). Therefore, the higher the leverage, the smaller the variance of the residual. Because the expected value of the residuals is 0, we conclude that points with high leverage tend to have smaller residuals than points with lower leverage."
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#standardized-residuals",
    "href": "supplemental/model-diagnostics-matrix.html#standardized-residuals",
    "title": "Model Diagnostics",
    "section": "Standardized Residuals",
    "text": "Standardized Residuals\nIn general, we standardize a value by shifting by the expected value and rescaling by the standard deviation (or standard error). Thus, the \\(i^{th}\\) standardized residual takes the form\n\\[\nstd.res_i = \\frac{e_i - E(e_i)}{SE(e_i)}\n\\]\nThe expected value of the residuals is 0, i.e. \\(E(e_i) = 0\\). From Equation 9), the standard error of the residual is \\(SE(e_i) = \\hat{\\sigma}\\sqrt{1-h_{ii}}\\). Therefore,\n\\[\nstd.res_i = \\frac{e_i}{\\hat{\\sigma}\\sqrt{1-h_{ii}}}\n\\tag{10}\\]"
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#cooks-distance",
    "href": "supplemental/model-diagnostics-matrix.html#cooks-distance",
    "title": "Model Diagnostics",
    "section": "Cook’s Distance",
    "text": "Cook’s Distance\nCook’s distance is a measure of how much each observation influences the model coefficients, and thus the predicted values. The Cook’s distance for the \\(i^{th}\\) observation can be written as\n\\[\nD_i = \\frac{(\\hat{\\mathbf{Y}} -\\hat{\\mathbf{Y}}_{(i)})^T(\\hat{\\mathbf{Y}} -\\hat{\\mathbf{Y}}_{(i)})}{(p+1)\\hat{\\sigma}}\n\\tag{11}\\]\nwhere \\(\\hat{\\mathbf{Y}}_{(i)}\\) is the vector of predicted values from the model fitted when the \\(i^{th}\\) observation is deleted. Cook’s Distance can be calculated without deleting observations one at a time, since Equation 12 below is mathematically equivalent to Equation 11.\n\\[\nD_i = \\frac{1}{p+1}std.res_i^2\\Bigg[\\frac{h_{ii}}{(1-h_{ii})}\\Bigg] = \\frac{e_i^2}{(p+1)\\hat{\\sigma}^2(1-h_{ii})}\\Bigg[\\frac{h_{ii}}{(1-h_{ii})}\\Bigg]\n\\tag{12}\\]"
  },
  {
    "objectID": "supplemental/log-transformations.html",
    "href": "supplemental/log-transformations.html",
    "title": "Log Transformations in Linear Regression",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr. Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\nThis document provides details about the model interpretation when the predictor and/or response variables are log-transformed. For simplicity, we will discuss transformations for the simple linear regression model as shown in Equation 1.\n\\[\n\\label{orig}\ny = \\beta_0 + \\beta_1 x\n\\tag{1}\\]\nAll results and interpretations can be easily extended to transformations in multiple regression models.\nNote: log refers to the natural logarithm."
  },
  {
    "objectID": "supplemental/log-transformations.html#log-transformation-on-the-response-variable",
    "href": "supplemental/log-transformations.html#log-transformation-on-the-response-variable",
    "title": "Log Transformations in Linear Regression",
    "section": "Log-transformation on the response variable",
    "text": "Log-transformation on the response variable\nSuppose we fit a linear regression model with \\(\\log(y)\\), the log-transformed \\(y\\), as the response variable. Under this model, we assume a linear relationship exists between \\(x\\) and \\(\\log(y)\\), such that \\(\\log(y) \\sim N(\\beta_0 + \\beta_1 x, \\sigma^2)\\) for some \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\). In other words, we can model the relationship between \\(x\\) and \\(\\log(y)\\) using the model in Equation 2.\n\\[\n\\log(y) = \\beta_0 + \\beta_1 x\n\\tag{2}\\]\nIf we interpret the model in terms of \\(\\log(y)\\), then we can use the usual interpretations for slope and intercept. When reporting results, however, it is best to give all interpretations in terms of the original response variable \\(y\\), since interpretations using log-transformed variables are often more difficult to truly understand.\nIn order to get back on the original scale, we need to use the exponential function (also known as the anti-log), \\(\\exp\\{x\\} = e^x\\). Therefore, we use the model in Equation 2 for interpretations and predictions, we will use Equation 3 to state our conclusions in terms of \\(y\\).\n\\[\n\\begin{aligned}\n&\\exp\\{\\log(y)\\} = \\exp\\{\\beta_0 + \\beta_1 x\\} \\\\[10pt]\n\\Rightarrow &y = \\exp\\{\\beta_0 + \\beta_1 x\\} \\\\[10pt]\n\\Rightarrow &y = \\exp\\{\\beta_0\\}\\exp\\{\\beta_1 x\\}\n\\end{aligned}\n\\tag{3}\\]\nIn order to interpret the slope and intercept, we need to first understand the relationship between the mean, median and log transformations.\n\nMean, Median, and Log Transformations\nSuppose we have a dataset y that contains the following observations:\n\n\n[1] 3 5 6 7 8\n\n\nIf we log-transform the values of y then calculate the mean and median, we have\n\n\n\n\n\nmean_log_y\nmedian_log_y\n\n\n\n\n1.70503\n1.79176\n\n\n\n\n\nIf we calculate the mean and median of y, then log-transform the mean and median, we have\n\n\n\n\n\nlog_mean\nlog_median\n\n\n\n\n1.75786\n1.79176\n\n\n\n\n\nThis is a simple illustration to show\n\n\\(\\text{Mean}[{\\log(y)}] \\neq \\log[\\text{Mean}(y)]\\) - the mean and log are not commutable\n\\(\\text{Median}[\\log(y)] = \\log[\\text{Median}(y)]\\) - the median and log are commutable\n\n\n\nInterpretaton of model coefficients\nUsing Equation 2, the mean \\(\\log(y)\\) for any given value of \\(x\\) is \\(\\beta_0 + \\beta_1 x\\); however, this does not indicate that the mean of \\(y = \\exp\\{\\beta_0 + \\beta_1 x\\}\\) (see previous section). From the assumptions of linear regression, we assume that for any given value of \\(x\\), the distribution of \\(\\log(y)\\) is Normal, and therefore symmetric. Thus the median of \\(\\log(y)\\) is equal to the mean of \\(\\log(y)\\), i.e \\(\\text{Median}(\\log(y)) = \\beta_0 + \\beta_1 x\\).\nSince the log and the median are commutable, \\(\\text{Median}(\\log(y)) = \\beta_0 + \\beta_1 x \\Rightarrow \\text{Median}(y) = \\exp\\{\\beta_0 + \\beta_1 x\\}\\). Thus, when we log-transform the response variable, the interpretation of the intercept and slope are in terms of the effect on the median of \\(y\\).\nIntercept: The intercept is expected median of \\(y\\) when the predictor variable equals 0. Therefore, when \\(x=0\\),\n\\[\n\\begin{aligned}\n&\\log(y) = \\beta_0 + \\beta_1 \\times 0 = \\beta_0 \\\\[10pt]\n\\Rightarrow &y = \\exp\\{\\beta_0\\}\n\\end{aligned}\n\\]\nInterpretation: When \\(x=0\\), the median of \\(y\\) is expected to be \\(\\exp\\{\\beta_0\\}\\).\nSlope: The slope is the expected change in the median of \\(y\\) when \\(x\\) increases by 1 unit. The change in the median of \\(y\\) is\n\\[\n\\exp\\{[\\beta_0 + \\beta_1 (x+1)] - [\\beta_0 + \\beta_1 x]\\} = \\frac{\\exp\\{\\beta_0 + \\beta_1 (x+1)\\}}{\\exp\\{\\beta_0 + \\beta_1 x\\}} = \\frac{\\exp\\{\\beta_0\\}\\exp\\{\\beta_1 x\\}\\exp\\{\\beta_1\\}}{\\exp\\{\\beta_0\\}\\exp\\{\\beta_1 x\\}} = \\exp\\{\\beta_1\\}\n\\]\nThus, the median of \\(y\\) for \\(x+1\\) is \\(\\exp\\{\\beta_1\\}\\) times the median of \\(y\\) for \\(x\\).\nInterpretation: When \\(x\\) increases by one unit, the median of \\(y\\) is expected to multiply by a factor of \\(\\exp\\{\\beta_1\\}\\)."
  },
  {
    "objectID": "supplemental/log-transformations.html#log-transformation-on-the-predictor-variable",
    "href": "supplemental/log-transformations.html#log-transformation-on-the-predictor-variable",
    "title": "Log Transformations in Linear Regression",
    "section": "Log-transformation on the predictor variable",
    "text": "Log-transformation on the predictor variable\nSuppose we fit a linear regression model with \\(\\log(x)\\), the log-transformed \\(x\\), as the predictor variable. Under this model, we assume a linear relationship exists between \\(\\log(x)\\) and \\(y\\), such that \\(y \\sim N(\\beta_0 + \\beta_1 \\log(x), \\sigma^2)\\) for some \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\). In other words, we can model the relationship between \\(\\log(x)\\) and \\(y\\) using the model in #eq-log-x.\n\\[\ny = \\beta_0 + \\beta_1 \\log(x)\n\\tag{4}\\]\nIntercept: The intercept is the mean of \\(y\\) when \\(\\log(x) = 0\\), i.e. \\(x = 1\\).\nInterpretation: When \\(x = 1\\) \\((\\log(x) = 0)\\), the mean of \\(y\\) is expected to be \\(\\beta_0\\).\nSlope: The slope is interpreted in terms of the change in the mean of \\(y\\) when \\(x\\) is multiplied by a factor of \\(C\\), since \\(\\log(Cx) = \\log(x) + \\log(C)\\). Thus, when \\(x\\) is multiplied by a factor of \\(C\\), the change in the mean of \\(y\\) is\n\\[\n\\begin{aligned}\n[\\beta_0 + \\beta_1 \\log(Cx)] - [\\beta_0 + \\beta_1 \\log(x)] &= \\beta_1 [\\log(Cx) - \\log(x)] \\\\[10pt]\n& = \\beta_1[\\log(C) + \\log(x) - \\log(x)] \\\\[10pt]\n& = \\beta_1 \\log(C)\n\\end{aligned}\n\\]\nThus the mean of \\(y\\) changes by \\(\\beta_1 \\log(C)\\) units.\nInterpretation: When \\(x\\) is multiplied by a factor of \\(C\\), the mean of \\(y\\) is expected to change by \\(\\beta_1 \\log(C)\\) units. For example, if \\(x\\) is doubled, then the mean of \\(y\\) is expected to change by \\(\\beta_1 \\log(2)\\) units."
  },
  {
    "objectID": "supplemental/log-transformations.html#log-transformation-on-the-the-response-and-predictor-variable",
    "href": "supplemental/log-transformations.html#log-transformation-on-the-the-response-and-predictor-variable",
    "title": "Log Transformations in Linear Regression",
    "section": "Log-transformation on the the response and predictor variable",
    "text": "Log-transformation on the the response and predictor variable\nSuppose we fit a linear regression model with \\(\\log(x)\\), the log-transformed \\(x\\), as the predictor variable and \\(\\log(y)\\), the log-transformed \\(y\\), as the response variable. Under this model, we assume a linear relationship exists between \\(\\log(x)\\) and \\(\\log(y)\\), such that \\(\\log(y) \\sim N(\\beta_0 + \\beta_1 \\log(x), \\sigma^2)\\) for some \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\). In other words, we can model the relationship between \\(\\log(x)\\) and \\(\\log(y)\\) using the model in Equation 5.\n\\[\n\\log(y) = \\beta_0 + \\beta_1 \\log(x)\n\\tag{5}\\]\nBecause the response variable is log-transformed, the interpretations on the original scale will be in terms of the median of \\(y\\) (see the section on the log-transformed response variable for more detail).\nIntercept: The intercept is the mean of \\(y\\) when \\(\\log(x) = 0\\), i.e. \\(x = 1\\). Therefore, when \\(\\log(x) = 0\\),\n\\[\n\\begin{aligned}\n&\\log(y) = \\beta_0 + \\beta_1 \\times 0 = \\beta_0 \\\\[10pt]\n\\Rightarrow &y = \\exp\\{\\beta_0\\}\n\\end{aligned}\n\\]\nInterpretation: When \\(x = 1\\) \\((\\log(x) = 0)\\), the median of \\(y\\) is expected to be \\(\\exp\\{\\beta_0\\}\\).\nSlope: The slope is interpreted in terms of the change in the median \\(y\\) when \\(x\\) is multiplied by a factor of \\(C\\), since \\(\\log(Cx) = \\log(x) + \\log(C)\\). Thus, when \\(x\\) is multiplied by a factor of \\(C\\), the change in the median of \\(y\\) is\n\\[\n\\begin{aligned}\n\\exp\\{[\\beta_0 + \\beta_1 \\log(Cx)] - [\\beta_0 + \\beta_1 \\log(x)]\\} &=\n\\exp\\{\\beta_1 [\\log(Cx) - \\log(x)]\\} \\\\[10pt]\n& = \\exp\\{\\beta_1[\\log(C) + \\log(x) - \\log(x)]\\} \\\\[10pt]\n& = \\exp\\{\\beta_1 \\log(C)\\} = C^{\\beta_1}\n\\end{aligned}\n\\]\nThus, the median of \\(y\\) for \\(Cx\\) is \\(C^{\\beta_1}\\) times the median of \\(y\\) for \\(x\\).\nInterpretation: When \\(x\\) is multiplied by a factor of \\(C\\), the median of \\(y\\) is expected to multiple by a factor of \\(C^{\\beta_1}\\). For example, if \\(x\\) is doubled, then the median of \\(y\\) is expected to multiply by \\(2^{\\beta_1}\\)."
  },
  {
    "objectID": "computing-pipelines.html",
    "href": "computing-pipelines.html",
    "title": "Pipelines",
    "section": "",
    "text": "library(palmerpenguins)\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✓ ggplot2 3.3.5     ✓ purrr   0.3.4\n✓ tibble  3.1.6     ✓ dplyr   1.0.7\n✓ tidyr   1.1.4     ✓ stringr 1.4.0\n✓ readr   2.1.1     ✓ forcats 0.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\nlibrary(tidymodels)\n\nRegistered S3 method overwritten by 'tune':\n  method                   from   \n  required_pkgs.model_spec parsnip\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 0.1.4 ──\n\n\n✓ broom        0.7.10         ✓ rsample      0.1.1     \n✓ dials        0.0.10         ✓ tune         0.1.6     \n✓ infer        1.0.1.9000     ✓ workflows    0.2.4     \n✓ modeldata    0.1.1          ✓ workflowsets 0.1.0     \n✓ parsnip      0.1.7          ✓ yardstick    0.0.9     \n✓ recipes      0.2.0          \n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\nx scales::discard() masks purrr::discard()\nx dplyr::filter()   masks stats::filter()\nx recipes::fixed()  masks stringr::fixed()\nx dplyr::lag()      masks stats::lag()\nx yardstick::spec() masks readr::spec()\nx recipes::step()   masks stats::step()\n• Search for functions across packages at https://www.tidymodels.org/find/\n\nlibrary(knitr)"
  },
  {
    "objectID": "computing-pipelines.html#simple-linear-regression",
    "href": "computing-pipelines.html#simple-linear-regression",
    "title": "Pipelines",
    "section": "Simple linear regression",
    "text": "Simple linear regression\n\nModel fitting\nFit model:\n\npenguins_fit &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(body_mass_g ~ flipper_length_mm, data = penguins)\n\nTidy model output:\n\ntidy(penguins_fit)\n\n# A tibble: 2 × 5\n  term              estimate std.error statistic   p.value\n  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)        -5781.     306.       -18.9 5.59e- 55\n2 flipper_length_mm     49.7      1.52      32.7 4.37e-107\n\n\nFormat model output as table:\n\ntidy(penguins_fit) %&gt;%\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5780.831\n305.815\n-18.903\n0\n\n\nflipper_length_mm\n49.686\n1.518\n32.722\n0\n\n\n\n\n\nAugment data with model:\n\naugment(penguins_fit$fit)\n\n# A tibble: 342 × 9\n   .rownames body_mass_g flipper_length_… .fitted  .resid    .hat .sigma .cooksd\n   &lt;chr&gt;           &lt;int&gt;            &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n 1 1                3750              181   3212.  538.   0.00881   394. 8.34e-3\n 2 2                3800              186   3461.  339.   0.00622   394. 2.33e-3\n 3 3                3250              195   3908. -658.   0.00344   393. 4.83e-3\n 4 5                3450              193   3808. -358.   0.00385   394. 1.60e-3\n 5 6                3650              190   3659.   -9.43 0.00469   395. 1.35e-6\n 6 7                3625              181   3212.  413.   0.00881   394. 4.91e-3\n 7 8                4675              195   3908.  767.   0.00344   393. 6.56e-3\n 8 9                3475              193   3808. -333.   0.00385   394. 1.39e-3\n 9 10               4250              190   3659.  591.   0.00469   394. 5.31e-3\n10 11               3300              186   3461. -161.   0.00622   395. 5.23e-4\n# … with 332 more rows, and 1 more variable: .std.resid &lt;dbl&gt;\n\n\n\n\nStatistical inference"
  },
  {
    "objectID": "course-schedule.html",
    "href": "course-schedule.html",
    "title": "Fall 2024 - CSC477/CSC2630: Introduction to Mobile Robotics",
    "section": "",
    "text": "This page contains an outline of the topics, content, and assignments for the semester. Note that this schedule will be updated as the semester progresses, with all changes documented here.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek\nDate\nTopic\nPrepare\nSlides\nNotebooks\nAssignments\nProject\n\n\n\n\n1\nSep 14\nIntroduction\n📖\n🖥️\n\n\n\n\n\n\n\nSensors and Actuators\n📖\n\n\n\n\n\n\n2\nSep 21\nKinematics\n📖\n🖥️\n\n\n\n\n\n\n\nDynamics\n📖\n\n\n\n\n\n\n3\nSep 28\nPID Control\n📖\n[🖥️]\n\n\n\n\n\n\n\nArtificial Potential Fields and Obstacle Avoidance\n📖\n\n📋\n\n\n\n\n4\nOct 5\nPlanning\n📖\n[🖥️]\n📋\n\n\n\n\n\nOct 12\nReading Week\n\n\n\n\n\n\n\n5\nOct 19\nLinear Quadratic Regulator (LQR)\n\n🖥️\n\n\n\n\n\n6\nOct 26\nMap Representations and Map Alignment\n\n🖥️\n\n\n📂\n\n\n\n\nOccupancy Grid Mapping With Known Robot Poses\n\n\n\n\n\n\n\n7\nNov 2\nMaximum Likelihood, Least Squares Estimation, Maximum A Posteriori Estimation\n📖\n🖥️\n📋\n\n\n\n\n\n\nGraphSLAM\n📖\n\n📋\n\n\n\n\n8\nNov 9\nKalman Filter\n\n🖥️\n\n\n\n\n\n\n\nBayes’ Filter and Kalman Filter\n\n\n\n\n\n\n\n9\nNov 16\nExtended Kalman Filter (EKF)\n📖\n🖥️\n📋\n\n\n\n\n10\nNov 23\nParticle Filter\n📖\n🖥️\n📋\n\n\n\n\n\n\nImitation with a human in the loop\n📖\n\n📋\n\n\n\n\n\n\nTeleoperation\n📖\n\n📋\n\n\n\n\n11\nNov 30\nCamera Optics and Multi-view Geometry\n📖\n🖥️\n📋\n\n\n\n\n\n\nCausal confusion in imitation learning\n📖\n\n📋\n\n\n\n\n12\nDec 7\nVisual odometry and Visual SLAM\n📖\n🖥️\n📋\n\n\n\n\n\n\nGeneralization and safety guarantees for imitation\n📖\n\n📋\n\n\n\n\n13\nDec 8\nStudy break, beginning of exams",
    "crumbs": [
      "Schedule"
    ]
  },
  {
    "objectID": "exams/exam-1.html",
    "href": "exams/exam-1.html",
    "title": "Exam 1",
    "section": "",
    "text": "Exam 1 will be released on Friday, Feb 4 at 9 am ET and must be completed by Mon, Feb 7 at 11:59 pm.\nThe exam will consist of two parts:\n\nPart 1 - Conceptual: Multiple choice questions on Sakai test and quizzes. This portion may only be submitted one time. Go here to complete Part 1 of the exam.\nPart 2 - Applied: Data analysis in RStudio and submitted in Gradescope (like a usual lab and homework). Go to the GitHub organization for the course and find the exam-1- repo to complete Part 3 of your exam. Add your answers to the exam-1.qmd file in your repo.\n\nBoth portions must be completed and submitted by Mon, Feb 7 at 11:59 pm. Late work will only be accepted in the case of of extenuating circumstances and notification from your academic dean.\n🍀 Good luck! 🍀\n\n\n\nBy taking this exam, you pledge to uphold the Duke Community Standard:\n\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors; and\nI will act if the Standard is compromised.\n\n\n\n\n\nThis is an individual assignment. Everything in your repository and on Sakai is for your eyes only.\nYou may not collaborate or communicate anything about this exam to anyone except the instructor and the head TA (Rick Presman). For example, you may not communicate with other students, other TAs, or post/solicit help on the internet, email or via any other method of communication.\nThe exam is open-book, open-note, so you may use any materials from class as you take the exam.\nOnly head TA and professor office hours will be held during the exam period.\nYou may not email the TAs questions about the exam.\nIf you have questions, direct message the professor or the head TA (Rick Presman) on Slack or post on Sakai Conversations for “Instructors in this site” only or attend my office hours (Monday, 10:30 am - 11:30 am).\n\n\n\n\n\nPart 1 - Conceptual: The answers to the multiple choice/short answer questions should be submitted under Test & Quizzes in Sakai.\nPart 2 - Applied: The PDF document for the data analysis portion must be uploaded to Gradescope.\n\nYou must submit a PDF to Gradescope that corresponds to the .qmd file on your GitHub repository in order to receive credit for this portion of the exam.\nYou must upload a PDF, not HTML. Any non-PDF submissions will not be graded.\nMark the pages associated with each question. If any answer for a question spans multiple pages, mark all associated pages.\nMake sure to associate the “Workflow & formatting” section with the first page.\nFailure to mark the pages in Gradescope will result in point deductions. Any pages that are not marked will not be graded.\nMake sure that your uploaded PDF document matches your .qmd and the PDF in your GitHub repository exactly. Points will be deducted for documents that are not reproducible."
  },
  {
    "objectID": "exams/exam-1.html#overview",
    "href": "exams/exam-1.html#overview",
    "title": "Exam 1",
    "section": "",
    "text": "Exam 1 will be released on Friday, Feb 4 at 9 am ET and must be completed by Mon, Feb 7 at 11:59 pm.\nThe exam will consist of two parts:\n\nPart 1 - Conceptual: Multiple choice questions on Sakai test and quizzes. This portion may only be submitted one time. Go here to complete Part 1 of the exam.\nPart 2 - Applied: Data analysis in RStudio and submitted in Gradescope (like a usual lab and homework). Go to the GitHub organization for the course and find the exam-1- repo to complete Part 3 of your exam. Add your answers to the exam-1.qmd file in your repo.\n\nBoth portions must be completed and submitted by Mon, Feb 7 at 11:59 pm. Late work will only be accepted in the case of of extenuating circumstances and notification from your academic dean.\n🍀 Good luck! 🍀"
  },
  {
    "objectID": "exams/exam-1.html#academic-integrity",
    "href": "exams/exam-1.html#academic-integrity",
    "title": "Exam 1",
    "section": "",
    "text": "By taking this exam, you pledge to uphold the Duke Community Standard:\n\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors; and\nI will act if the Standard is compromised."
  },
  {
    "objectID": "exams/exam-1.html#rules-notes",
    "href": "exams/exam-1.html#rules-notes",
    "title": "Exam 1",
    "section": "",
    "text": "This is an individual assignment. Everything in your repository and on Sakai is for your eyes only.\nYou may not collaborate or communicate anything about this exam to anyone except the instructor and the head TA (Rick Presman). For example, you may not communicate with other students, other TAs, or post/solicit help on the internet, email or via any other method of communication.\nThe exam is open-book, open-note, so you may use any materials from class as you take the exam.\nOnly head TA and professor office hours will be held during the exam period.\nYou may not email the TAs questions about the exam.\nIf you have questions, direct message the professor or the head TA (Rick Presman) on Slack or post on Sakai Conversations for “Instructors in this site” only or attend my office hours (Monday, 10:30 am - 11:30 am)."
  },
  {
    "objectID": "exams/exam-1.html#submission",
    "href": "exams/exam-1.html#submission",
    "title": "Exam 1",
    "section": "",
    "text": "Part 1 - Conceptual: The answers to the multiple choice/short answer questions should be submitted under Test & Quizzes in Sakai.\nPart 2 - Applied: The PDF document for the data analysis portion must be uploaded to Gradescope.\n\nYou must submit a PDF to Gradescope that corresponds to the .qmd file on your GitHub repository in order to receive credit for this portion of the exam.\nYou must upload a PDF, not HTML. Any non-PDF submissions will not be graded.\nMark the pages associated with each question. If any answer for a question spans multiple pages, mark all associated pages.\nMake sure to associate the “Workflow & formatting” section with the first page.\nFailure to mark the pages in Gradescope will result in point deductions. Any pages that are not marked will not be graded.\nMake sure that your uploaded PDF document matches your .qmd and the PDF in your GitHub repository exactly. Points will be deducted for documents that are not reproducible."
  },
  {
    "objectID": "exams/exam-2.html",
    "href": "exams/exam-2.html",
    "title": "Exam 2",
    "section": "",
    "text": "Exam 1 will be released on Friday, Feb 25 at 9 am ET and must be completed by Mon, Feb 28 at 11:59 pm.\nThe exam will consist of two parts:\n\nPart 1 - Conceptual: Multiple choice questions on Sakai test and quizzes. This portion may only be submitted one time. Go here to complete Part 1 of the exam.\nPart 2 - Applied: Data analysis in RStudio and submitted in Gradescope (like a usual lab and homework). Go to the GitHub organization for the course and find the exam-2- repo to complete Part 2 of your exam. Add your answers to the exam-2.qmd file in your repo.\n\nBoth portions must be completed and submitted by Mon, Feb 28 at 11:59 pm. Late work will only be accepted in the case of of extenuating circumstances and notification from your academic dean.\n🍀 Good luck! 🍀\n\n\n\nBy taking this exam, you pledge to uphold the Duke Community Standard:\n\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors; and\nI will act if the Standard is compromised.\n\n\n\n\n\nThis is an individual assignment. Everything in your repository and on Sakai is for your eyes only.\nYou may not collaborate or communicate anything about this exam to anyone except the instructor and the head TA (Rick Presman). For example, you may not communicate with other students, other TAs, or post/solicit help on the internet, email or via any other method of communication.\nThe exam is open-book, open-note, so you may use any materials from class as you take the exam.\nOnly head TA and professor office hours will be held during the exam period.\nYou may not email the TAs questions about the exam.\nIf you have questions, direct message the professor or the head TA (Rick Presman) on Slack or post on Sakai Conversations for “Instructors in this site” only or attend my office hours (Monday, 10:30 am - 11:30 am).\n\n\n\n\n\nPart 1 - Conceptual: The answers to the multiple choice/short answer questions should be submitted under Test & Quizzes in Sakai.\nPart 2 - Applied: The PDF document for the data analysis portion must be uploaded to Gradescope.\n\nYou must submit a PDF to Gradescope that corresponds to the .qmd file on your GitHub repository in order to receive credit for this portion of the exam.\nYou must upload a PDF, not HTML. Any non-PDF submissions will not be graded.\nMark the pages associated with each question. If any answer for a question spans multiple pages, mark all associated pages.\nMake sure to associate the “Workflow & formatting” section with the first page.\nFailure to mark the pages in Gradescope will result in point deductions. Any pages that are not marked will not be graded.\nMake sure that your uploaded PDF document matches your .qmd and the PDF in your GitHub repository exactly. Points will be deducted for documents that are not reproducible."
  },
  {
    "objectID": "exams/exam-2.html#overview",
    "href": "exams/exam-2.html#overview",
    "title": "Exam 2",
    "section": "",
    "text": "Exam 1 will be released on Friday, Feb 25 at 9 am ET and must be completed by Mon, Feb 28 at 11:59 pm.\nThe exam will consist of two parts:\n\nPart 1 - Conceptual: Multiple choice questions on Sakai test and quizzes. This portion may only be submitted one time. Go here to complete Part 1 of the exam.\nPart 2 - Applied: Data analysis in RStudio and submitted in Gradescope (like a usual lab and homework). Go to the GitHub organization for the course and find the exam-2- repo to complete Part 2 of your exam. Add your answers to the exam-2.qmd file in your repo.\n\nBoth portions must be completed and submitted by Mon, Feb 28 at 11:59 pm. Late work will only be accepted in the case of of extenuating circumstances and notification from your academic dean.\n🍀 Good luck! 🍀"
  },
  {
    "objectID": "exams/exam-2.html#academic-integrity",
    "href": "exams/exam-2.html#academic-integrity",
    "title": "Exam 2",
    "section": "",
    "text": "By taking this exam, you pledge to uphold the Duke Community Standard:\n\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors; and\nI will act if the Standard is compromised."
  },
  {
    "objectID": "exams/exam-2.html#rules-notes",
    "href": "exams/exam-2.html#rules-notes",
    "title": "Exam 2",
    "section": "",
    "text": "This is an individual assignment. Everything in your repository and on Sakai is for your eyes only.\nYou may not collaborate or communicate anything about this exam to anyone except the instructor and the head TA (Rick Presman). For example, you may not communicate with other students, other TAs, or post/solicit help on the internet, email or via any other method of communication.\nThe exam is open-book, open-note, so you may use any materials from class as you take the exam.\nOnly head TA and professor office hours will be held during the exam period.\nYou may not email the TAs questions about the exam.\nIf you have questions, direct message the professor or the head TA (Rick Presman) on Slack or post on Sakai Conversations for “Instructors in this site” only or attend my office hours (Monday, 10:30 am - 11:30 am)."
  },
  {
    "objectID": "exams/exam-2.html#submission",
    "href": "exams/exam-2.html#submission",
    "title": "Exam 2",
    "section": "",
    "text": "Part 1 - Conceptual: The answers to the multiple choice/short answer questions should be submitted under Test & Quizzes in Sakai.\nPart 2 - Applied: The PDF document for the data analysis portion must be uploaded to Gradescope.\n\nYou must submit a PDF to Gradescope that corresponds to the .qmd file on your GitHub repository in order to receive credit for this portion of the exam.\nYou must upload a PDF, not HTML. Any non-PDF submissions will not be graded.\nMark the pages associated with each question. If any answer for a question spans multiple pages, mark all associated pages.\nMake sure to associate the “Workflow & formatting” section with the first page.\nFailure to mark the pages in Gradescope will result in point deductions. Any pages that are not marked will not be graded.\nMake sure that your uploaded PDF document matches your .qmd and the PDF in your GitHub repository exactly. Points will be deducted for documents that are not reproducible."
  },
  {
    "objectID": "course-syllabus.html",
    "href": "course-syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Click here to download a PDF copy of the syllabus."
  },
  {
    "objectID": "lecs/w02/lec02.html#section",
    "href": "lecs/w02/lec02.html#section",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "",
    "text": "Today’s slides borrow parts of Paul Furgale’s “Representing robot pose” presentation:\nhttp://paulfurgale.info/news/2014/6/9/representing-robot-pose-the-good-the-bad-and-the-ugly\nYou should absolutely read it."
  },
  {
    "objectID": "lecs/w02/lec02.html#todays-agenda",
    "href": "lecs/w02/lec02.html#todays-agenda",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Today’s Agenda",
    "text": "Today’s Agenda\n• Frames of reference\n• Ways to represent rotations\n• Simplified models of vehicles\n• Forward and inverse kinematics"
  },
  {
    "objectID": "lecs/w02/lec02.html#d-frames-of-reference-are-everywhere-in-robotics",
    "href": "lecs/w02/lec02.html#d-frames-of-reference-are-everywhere-in-robotics",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "3D frames of reference are everywhere in robotics",
    "text": "3D frames of reference are everywhere in robotics"
  },
  {
    "objectID": "lecs/w02/lec02.html#right-handed-vs-left-handed-frames",
    "href": "lecs/w02/lec02.html#right-handed-vs-left-handed-frames",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Right-handed vs left-handed frames",
    "text": "Right-handed vs left-handed frames\n\n\nUnless otherwise specified,\nwe use right-handed\nframes in robotics"
  },
  {
    "objectID": "lecs/w02/lec02.html#why-do-we-need-to-use-so-many-frames",
    "href": "lecs/w02/lec02.html#why-do-we-need-to-use-so-many-frames",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Why do we need to use so many frames?",
    "text": "Why do we need to use so many frames?\n\n\n\nBecause we want to reason and express quantities relative to their local configuration.\nFor example: “grab the bottle behind the cereal bowl”\nThis lecture is about defining and representing frames of reference and reasoning about how to express quantities in one frame to quantities in the other."
  },
  {
    "objectID": "lecs/w02/lec02.html#rigid-body-motion",
    "href": "lecs/w02/lec02.html#rigid-body-motion",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Rigid-body motion",
    "text": "Rigid-body motion\n\nMotion that can be described by a rotation and translation.\nAll the parts making up the body move in unison, and there are no deformations.\nRepresenting rotations, translations, and vectors in a given frame of reference is often a source of frustration and bugs in robot software because there are so many options."
  },
  {
    "objectID": "lecs/w02/lec02.html#the-answer-is-meaningless-unless-i-provide-a-definition-of-the-coordinate-frames",
    "href": "lecs/w02/lec02.html#the-answer-is-meaningless-unless-i-provide-a-definition-of-the-coordinate-frames",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "The answer is meaningless unless I provide a definition of the coordinate frames",
    "text": "The answer is meaningless unless I provide a definition of the coordinate frames"
  },
  {
    "objectID": "lecs/w02/lec02.html#section-2",
    "href": "lecs/w02/lec02.html#section-2",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "",
    "text": "Color convention\nfor frames\n\n\nMoving body (robot) frame\n\n\nFixed world frame"
  },
  {
    "objectID": "lecs/w02/lec02.html#always-provide-a-frame-diagram",
    "href": "lecs/w02/lec02.html#always-provide-a-frame-diagram",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Always provide a frame diagram",
    "text": "Always provide a frame diagram"
  },
  {
    "objectID": "lecs/w02/lec02.html#inertial-frames-of-reference",
    "href": "lecs/w02/lec02.html#inertial-frames-of-reference",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Inertial frames of reference",
    "text": "Inertial frames of reference\n\nG, the global frame of reference is fixed, i.e. with zero velocity in our previous example.\nBut, in general it can move as long as it has zero acceleration. Such a frame is called an “inertial” frame of reference.\nNewton’s laws hold for inertial reference frames only. For reference frames with non-constant velocity we need the theory of General Relativity.\nSo, make sure that your global frame of reference is inertial, preferably fixed."
  },
  {
    "objectID": "lecs/w02/lec02.html#todays-agenda-1",
    "href": "lecs/w02/lec02.html#todays-agenda-1",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Today’s Agenda",
    "text": "Today’s Agenda\n\n\nFrames of reference\n\n\n\nWays to represent rotations\nSimplified models of vehicles\nForward and inverse kinematics"
  },
  {
    "objectID": "lecs/w02/lec02.html#representing-rotations-in-3d-euler-angles",
    "href": "lecs/w02/lec02.html#representing-rotations-in-3d-euler-angles",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Representing Rotations in 3D: Euler Angles",
    "text": "Representing Rotations in 3D: Euler Angles"
  },
  {
    "objectID": "lecs/w02/lec02.html#specification-ambiguities-in-euler-angles",
    "href": "lecs/w02/lec02.html#specification-ambiguities-in-euler-angles",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Specification ambiguities in Euler Angles",
    "text": "Specification ambiguities in Euler Angles\n\nNeed to specify the axes which each angle refers to.\nThere are 12 different valid combinations of fundamental rotations. Here are the possible axes:\nz-x-z, x-y-x, y-z-y, z-y-z, x-z-x, y-x-y\nx-y-z, y-z-x, z-y-x, x-z-y, z-y-x, y-x-z\n\n\n\nE.g.: x-y-z rotation with Euler angles \\((\\theta, \\phi, \\psi)\\) means the rotation can be expressed as a sequence of simple rotations \\(R_x(\\theta) R_y(\\phi) R_z(\\psi)\\)"
  },
  {
    "objectID": "lecs/w02/lec02.html#specification-ambiguities-in-euler-angles-1",
    "href": "lecs/w02/lec02.html#specification-ambiguities-in-euler-angles-1",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Specification ambiguities in Euler Angles",
    "text": "Specification ambiguities in Euler Angles\nSimple rotations can be counter-clockwise or clockwise. This gives another 2 possibilities.\n\\[\n\\mathbf{R}_z(\\alpha) :=\n\\begin{bmatrix}\n\\cos\\alpha & -\\sin\\alpha & 0 \\\\\n\\sin\\alpha & \\cos\\alpha  & 0 \\\\\n0          & 0           & 1\n\\end{bmatrix}\n\\qquad\n\\mathbf{C}_z(\\alpha) :=\n\\begin{bmatrix}\n\\cos\\alpha & \\sin\\alpha  & 0 \\\\\n-\\sin\\alpha & \\cos\\alpha & 0 \\\\\n0           & 0          & 1\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "lecs/w02/lec02.html#specification-ambiguities-in-euler-angles-2",
    "href": "lecs/w02/lec02.html#specification-ambiguities-in-euler-angles-2",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Specification ambiguities in Euler Angles",
    "text": "Specification ambiguities in Euler Angles\n\n\nYou need to specify whether the rotation rotates from the world frame to the body frame, or the other way around.\nAnother 2 possibilities. More possibilities if you have more frames.\nDegrees or radians? Another 2 possibilities"
  },
  {
    "objectID": "lecs/w02/lec02.html#specification-ambiguities-in-euler-angles-3",
    "href": "lecs/w02/lec02.html#specification-ambiguities-in-euler-angles-3",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Specification ambiguities in Euler Angles",
    "text": "Specification ambiguities in Euler Angles\n\nNeed to specify the ordering of the three parameters.\n1-2-3, 1-3-2, 2-1-3, 2-3-1, 3-1-2, 3-2-1\nAnother 6 different valid combinations"
  },
  {
    "objectID": "lecs/w02/lec02.html#another-problem-with-euler-angles-gimbal-lock",
    "href": "lecs/w02/lec02.html#another-problem-with-euler-angles-gimbal-lock",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Another problem with Euler angles: Gimbal Lock",
    "text": "Another problem with Euler angles: Gimbal Lock"
  },
  {
    "objectID": "lecs/w02/lec02.html#another-problem-with-euler-angles-gimbal-lock-1",
    "href": "lecs/w02/lec02.html#another-problem-with-euler-angles-gimbal-lock-1",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Another problem with Euler angles: Gimbal Lock",
    "text": "Another problem with Euler angles: Gimbal Lock\n\nWhy should roboticists care about this?\nBecause when it happens Euler angle representations lose one degree of freedom.\nThey cannot represent the entire range of rotations any more.\nThey get “locked” into a subset of the space of possible rotations."
  },
  {
    "objectID": "lecs/w02/lec02.html#so-we-need-other-representations-aside-from-euler-angles.-even-though-they-are-a-minimal-representation.",
    "href": "lecs/w02/lec02.html#so-we-need-other-representations-aside-from-euler-angles.-even-though-they-are-a-minimal-representation.",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "So, we need other representations aside from Euler angles. Even though they are a minimal representation.",
    "text": "So, we need other representations aside from Euler angles. Even though they are a minimal representation."
  },
  {
    "objectID": "lecs/w02/lec02.html#representing-rotations-in-3d-axis-angle",
    "href": "lecs/w02/lec02.html#representing-rotations-in-3d-axis-angle",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Representing Rotations in 3D: Axis-Angle",
    "text": "Representing Rotations in 3D: Axis-Angle\n\n\n\n\n\n4-number representation (angle, 3D axis)\n2 ambiguities: (-angle, -axis) is the same as (angle, axis)"
  },
  {
    "objectID": "lecs/w02/lec02.html#representing-rotations-in-3d-rotation-matrix",
    "href": "lecs/w02/lec02.html#representing-rotations-in-3d-rotation-matrix",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Representing Rotations in 3D: Rotation Matrix",
    "text": "Representing Rotations in 3D: Rotation Matrix\n\nThe royalty of rotation representations\n3x3-number representation, very redundant\nNo ambiguities, as long as source frame and target frame are specified correctly. For example, define your notation this way:\nRotation from Body frame to World frame: \\(\\mathbf{R}_{BW}\\)\nOr you can define it this way: \\(_B^W \\mathbf{R}\\)"
  },
  {
    "objectID": "lecs/w02/lec02.html#inverse-rotation-matrix",
    "href": "lecs/w02/lec02.html#inverse-rotation-matrix",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Inverse Rotation Matrix",
    "text": "Inverse Rotation Matrix\n\\[\n_B^W\\mathbf{R}^{-1} = _B^W\\mathbf{R}^t = _W^B\\mathbf{R}\n\\]\nRotation matrices are orthogonal matrices: their transpose is their inverse and they do not change the length of a vector, they just rotate it in space.\n\\[{}^W_B\\mathbf{R}^{t} {}^W_B\\mathbf{R} = \\mathbf{I}\\]"
  },
  {
    "objectID": "lecs/w02/lec02.html#converting-axis-angle-to-rotation-matrix",
    "href": "lecs/w02/lec02.html#converting-axis-angle-to-rotation-matrix",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Converting axis-angle to rotation matrix",
    "text": "Converting axis-angle to rotation matrix\n\nGiven angle theta and axis v the equivalent rotation matrix is\n\n\\[\n\\mathbf{R} = \\mathbf{I}\\cos\\theta + (1 - \\cos\\theta)\\mathbf{v}\\mathbf{v}^t + [\\mathbf{v}]_\\times\n\\]\n\nWhere I is the 3x3 identity and \\[[\\mathbf{a}]_\\times \\stackrel{\\text{def}}{=} \\begin{bmatrix} 0 & -a_3 & a_2 \\\\ a_3 & 0 & -a_1 \\\\ -a_2 & a_1 & 0 \\end{bmatrix}\\]\nThis is called the “Rodrigues formula”"
  },
  {
    "objectID": "lecs/w02/lec02.html#example-finding-a-rotation-matrix-that-rotates-one-vector-to-another",
    "href": "lecs/w02/lec02.html#example-finding-a-rotation-matrix-that-rotates-one-vector-to-another",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Example: finding a rotation matrix that rotates one vector to another",
    "text": "Example: finding a rotation matrix that rotates one vector to another\n\n\n\n\n\\[\n_C^D\\mathbf{R} = \\begin{bmatrix} 0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ 1 & 0 & 0 \\end{bmatrix}\n\\]\n\nThis matrix transforms the x-axis of frame C to the z-axis of frame D. Same for y and z axes."
  },
  {
    "objectID": "lecs/w02/lec02.html#rotation-multiplication-vs-addition-3d-vs-2d",
    "href": "lecs/w02/lec02.html#rotation-multiplication-vs-addition-3d-vs-2d",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Rotation multiplication vs addition: 3D vs 2D",
    "text": "Rotation multiplication vs addition: 3D vs 2D\n\nIn 2D adding angles with wraparound at 360 degrees is a valid operation.\nRotation matrices can be added, but the result is not necessarily a valid rotation. Rotations are not closed under the operation of addition.\nRotations are closed under the operation of multiplication. To compose a sequence of simple rotations we need to multiply them."
  },
  {
    "objectID": "lecs/w02/lec02.html#compound-rotations",
    "href": "lecs/w02/lec02.html#compound-rotations",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Compound rotations",
    "text": "Compound rotations\n\n\\[\n_C^E \\mathbf{R} = _D^E \\mathbf{R} _C^D \\mathbf{R}\n\\]"
  },
  {
    "objectID": "lecs/w02/lec02.html#representing-rotations-in-3d-quaternions",
    "href": "lecs/w02/lec02.html#representing-rotations-in-3d-quaternions",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Representing Rotations in 3D: Quaternions",
    "text": "Representing Rotations in 3D: Quaternions\n\nBased on axis-angle representation, but more computationally efficient.\nThe main workhorse of rotation representations.\nUsed almost everywhere in robotics, aerospace, aviation.\nVery important to master in this course. You will need it for the first assignment and for working with ROS in general."
  },
  {
    "objectID": "lecs/w02/lec02.html#converting-axis-angle-to-quaternion",
    "href": "lecs/w02/lec02.html#converting-axis-angle-to-quaternion",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Converting axis-angle to quaternion",
    "text": "Converting axis-angle to quaternion\n\nGiven angle theta and axis v the equivalent quaternion representation is\n\n\\[\n\\mathbf{q} = [\\sin(\\theta/2)v_1, \\sin(\\theta/2)v_2, \\sin(\\theta/2)v_3, \\cos(\\theta/2)]\n\\]\n\\[\\mathbf{q} = x\\mathbf{i} + y\\mathbf{j} + z\\mathbf{k} + w\\]\n\nJust like in the case of rotation matrices we denote the source and target frames of the rotation quaternion: \\(_B^W \\mathbf{q}\\)\n\n\n\nWe always work with unit length (normalized) quaternions."
  },
  {
    "objectID": "lecs/w02/lec02.html#examples-of-quaternions",
    "href": "lecs/w02/lec02.html#examples-of-quaternions",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Examples of quaternions",
    "text": "Examples of quaternions\n\n90 degree rotation about the z-axis\n\n\\[\n\\mathbf{q} = [0, 0, \\sin(\\pi / 4)v_3, cos(\\pi / 4)]\n\\]"
  },
  {
    "objectID": "lecs/w02/lec02.html#quaternion-multiplication",
    "href": "lecs/w02/lec02.html#quaternion-multiplication",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Quaternion multiplication",
    "text": "Quaternion multiplication\n\nDefined algebraically by\n\\[ \\begin{align}\nQ = q_0 + q_1i + q_2j q_sk \\\\\ni^2 = j^2 = k^2 = ijk = -1 \\\\\nij = k, jk = i, ki = j\n\\end{align}\n\\]\nand usually denoted by the circular cross symbol. For example:\n\\[\n_F^W\\mathbf{q} = _C^W\\mathbf{q} \\otimes {} _F^C\\mathbf{q}\n\\]"
  },
  {
    "objectID": "lecs/w02/lec02.html#quaternion-multiplication-1",
    "href": "lecs/w02/lec02.html#quaternion-multiplication-1",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Quaternion multiplication",
    "text": "Quaternion multiplication\n\\[{}^W_F\\mathbf{q} = {}^W_C\\mathbf{q} \\otimes {}^C_F\\mathbf{q}\\]\nDirect correspondence with matrix multiplication:\n\\[\n_F^W\\mathbf{R(q)} = _C^W\\mathbf{R(q)} _F^C\\mathbf{R(q)}\n\\]\nNOTE: the quaternion to matrix conversion will not be given here.\nIt is usually present in all numerical algebra libraries. At the moment we’ll take it for granted."
  },
  {
    "objectID": "lecs/w02/lec02.html#quaternion-inversion",
    "href": "lecs/w02/lec02.html#quaternion-inversion",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Quaternion inversion",
    "text": "Quaternion inversion\n\\[\n\\mathbf{q}^{-1} = -x\\mathbf{i} - y\\mathbf{j} -z\\mathbf{k} + w\n\\]\n\n\\[\n[0,0,0,1] = \\mathbf{q}^{-1} \\otimes \\mathbf{q}\n\\]\nDirect correspondence with matrix inversion:  \\[\n\\mathbf{I} = \\mathbf{R(q^{-1})R(q)}\n\\]\n\\[\n\\mathbf{I} = \\mathbf{R(q)^{-1} R(q)}\n\\]"
  },
  {
    "objectID": "lecs/w02/lec02.html#example-updating-orientation-based-on-angular-velocity",
    "href": "lecs/w02/lec02.html#example-updating-orientation-based-on-angular-velocity",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Example: updating orientation based on angular velocity",
    "text": "Example: updating orientation based on angular velocity\n\nIf the angular velocity of the Body frame is \\(^Bw\\) and the body-to-world rotation at time t is \\(_B^W\\mathbf{q}(t)\\)\nThen, at time t+dt the new body-to-world rotation will be \\[{}^W_{B(t+dt)}\\mathbf{q} = {}^W_{B(t)}\\mathbf{q} \\otimes {}^{B(t)}_{B(t+dt)}\\mathbf{q}\\]\n\nwhere \\({}^{B(t)}_{B(t+dt)}\\mathbf{q}\\) has unit axis \\(\\frac{{}^B\\omega}{||{}^B\\omega||}\\) and angle \\(||{}^B\\omega|| dt\\)"
  },
  {
    "objectID": "lecs/w02/lec02.html#main-ambiguities-of-quaternion-representation",
    "href": "lecs/w02/lec02.html#main-ambiguities-of-quaternion-representation",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Main ambiguities of quaternion representation",
    "text": "Main ambiguities of quaternion representation\n\nThe ones inherited from the axis-angle representation, but also:"
  },
  {
    "objectID": "lecs/w02/lec02.html#be-clear-about-your-orientation-representation.",
    "href": "lecs/w02/lec02.html#be-clear-about-your-orientation-representation.",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Be clear about your orientation representation.",
    "text": "Be clear about your orientation representation."
  },
  {
    "objectID": "lecs/w02/lec02.html#suggested-minimum-documentation",
    "href": "lecs/w02/lec02.html#suggested-minimum-documentation",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Suggested minimum documentation",
    "text": "Suggested minimum documentation\n\nFrame diagram\nFull description of how to build a transformation matrix from the provided scalars and down to the scalar level.\nA clear statement of which transformation matrix it is."
  },
  {
    "objectID": "lecs/w02/lec02.html#lets-talk-about-code.",
    "href": "lecs/w02/lec02.html#lets-talk-about-code.",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Lets talk about code.",
    "text": "Lets talk about code.\n\nCode has the same requirements as notation\nRotation matrices have two frame decorations:\n\nto\nfrom\n\nCoordinates of vectors have three decorations:\n\nto\nfrom\nexpressed in"
  },
  {
    "objectID": "lecs/w02/lec02.html#lets-talk-about-code.-1",
    "href": "lecs/w02/lec02.html#lets-talk-about-code.-1",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Lets talk about code.",
    "text": "Lets talk about code."
  },
  {
    "objectID": "lecs/w02/lec02.html#lets-talk-about-code.-2",
    "href": "lecs/w02/lec02.html#lets-talk-about-code.-2",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Lets talk about code.",
    "text": "Lets talk about code."
  },
  {
    "objectID": "lecs/w02/lec02.html#lets-talk-about-code.-3",
    "href": "lecs/w02/lec02.html#lets-talk-about-code.-3",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Lets talk about code.",
    "text": "Lets talk about code."
  },
  {
    "objectID": "lecs/w02/lec02.html#lets-talk-about-code.-4",
    "href": "lecs/w02/lec02.html#lets-talk-about-code.-4",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Lets talk about code.",
    "text": "Lets talk about code."
  },
  {
    "objectID": "lecs/w02/lec02.html#lets-talk-about-code.-5",
    "href": "lecs/w02/lec02.html#lets-talk-about-code.-5",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Lets talk about code.",
    "text": "Lets talk about code."
  },
  {
    "objectID": "lecs/w02/lec02.html#lets-talk-about-code.-6",
    "href": "lecs/w02/lec02.html#lets-talk-about-code.-6",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Lets talk about code.",
    "text": "Lets talk about code.\nChoose an expressive coding style.\nExplain it clearly.\nStick with it."
  },
  {
    "objectID": "lecs/w02/lec02.html#example-finding-quaternion-that-rotates-one-vector-into-another",
    "href": "lecs/w02/lec02.html#example-finding-quaternion-that-rotates-one-vector-into-another",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Example: finding quaternion that rotates one vector into another",
    "text": "Example: finding quaternion that rotates one vector into another\n\nSuppose you have a vector in frame A, and a vector in frame B\nYou want to find a quaternion that transforms \\(^A\\mathbf{v}\\) to \\(^B\\mathbf{v}\\)\nIdea: use axis-angle and convert it to quaternion\nCan rotate from \\(^A\\mathbf{v}\\) to \\(^B\\mathbf{v}\\) along an axis that is perpendicular to both of them. How do we find that?"
  },
  {
    "objectID": "lecs/w02/lec02.html#cross-product",
    "href": "lecs/w02/lec02.html#cross-product",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Cross Product",
    "text": "Cross Product\n\n\n\n\n\\[\n\\mathbf{a} \\times \\mathbf{b} = ||\\mathbf{a}|| ||\\mathbf{b}|| \\sin(\\theta) \\mathbf{n}\n\\]"
  },
  {
    "objectID": "lecs/w02/lec02.html#example-finding-quaternion-that-rotates-one-vector-into-another-1",
    "href": "lecs/w02/lec02.html#example-finding-quaternion-that-rotates-one-vector-into-another-1",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Example: finding quaternion that rotates one vector into another",
    "text": "Example: finding quaternion that rotates one vector into another\n\\(\\mathbf{v}_\\text{rot axis} = ^A\\mathbf{v} \\times ^B\\mathbf{v}\\) is perpendicular to both of them\n\\(\\theta_{\\text{rot angle}} = \\text{acos}(^A\\mathbf{v} \\cdot ^B\\mathbf{v})\\)\nAssuming the two vectors are unit length"
  },
  {
    "objectID": "lecs/w02/lec02.html#rotating-a-vector-via-a-quaternion",
    "href": "lecs/w02/lec02.html#rotating-a-vector-via-a-quaternion",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Rotating a vector via a quaternion",
    "text": "Rotating a vector via a quaternion\n\nLet \\(^A\\mathbf{v}\\) be given and a quaternion \\(_A^B\\mathbf{q}\\)\nTo obtain \\(^B\\mathbf{v}\\) you have two choices:\nEither use the rotation matrix \\(^B\\mathbf{v} = _A^B\\mathbf{R(q)} ^A\\mathbf{v}\\)\nOr use quaternion multiplication directly\n\n\\[\n[^B\\mathbf{v}, 0] = _A^B\\mathbf{q} \\otimes [^A\\mathbf{v}, 0] \\otimes _B^A\\mathbf{q}\n\\]"
  },
  {
    "objectID": "lecs/w02/lec02.html#transforming-points-from-one-frame-to-another",
    "href": "lecs/w02/lec02.html#transforming-points-from-one-frame-to-another",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Transforming points from one frame to another",
    "text": "Transforming points from one frame to another\n\n\nVERY IMPORTANT AND USEFUL\n\n\n\nSuppose you have a point in the Body frame,\\(^B\\mathbf{p}\\) which you want to transform/express in the World frame. Then you can do any of the two following options:\n\n\\[{}^W\\mathbf{p} = {}^W_B\\mathbf{R} {}^B\\mathbf{p} + {}^W\\mathbf{t}_{WB} \\qquad {}^W\\mathbf{p} = {}^W_B\\mathbf{R}({}^B\\mathbf{p} - {}^B\\mathbf{t}_{BW})\\]\n\nThink of it as first rotating the point to be in the World frame and then adding to it the translation from Body to World."
  },
  {
    "objectID": "lecs/w02/lec02.html#transforming-vectors-from-one-frame-to-another",
    "href": "lecs/w02/lec02.html#transforming-vectors-from-one-frame-to-another",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Transforming vectors from one frame to another",
    "text": "Transforming vectors from one frame to another\n\n\nVERY IMPORTANT AND USEFUL\n\n\n\nSuppose you have a vector in the Body frame, \\(^B\\mathbf{v}\\) which you want to transform/express in the World frame. Then\n\n\\[\n^W\\mathbf{v} = _B^W\\mathbf{R} ^B_\\mathbf{v}\n\\]"
  },
  {
    "objectID": "lecs/w02/lec02.html#combining-rotations-and-translation-into-one-transformation",
    "href": "lecs/w02/lec02.html#combining-rotations-and-translation-into-one-transformation",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Combining rotations and translation into one transformation",
    "text": "Combining rotations and translation into one transformation\n\n\nVERY IMPORTANT AND USEFUL\n\n\n\nMany times we combine the rotation and translation of a rigid motion into a 4x4 homogeneous matrix\n\n\\[\n_B^W\\mathbf{T} = \\begin{bmatrix}\n_B^W\\mathbf{R} & ^W\\mathbf{t}_{WB}  \\\\\n0 & 1   \\\\\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "lecs/w02/lec02.html#main-advantage-of-homogeneous-transformations-easy-composition",
    "href": "lecs/w02/lec02.html#main-advantage-of-homogeneous-transformations-easy-composition",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Main advantage of homogeneous transformations: easy composition",
    "text": "Main advantage of homogeneous transformations: easy composition\n\\[\n_B^W\\mathbf{T} = _A^W\\mathbf{T} _B^A\\mathbf{T}\n\\]\nComposing rigid motions now becomes a series of matrix multiplications"
  },
  {
    "objectID": "lecs/w02/lec02.html#inverting-a-homogeneous-transformation",
    "href": "lecs/w02/lec02.html#inverting-a-homogeneous-transformation",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Inverting a homogeneous transformation",
    "text": "Inverting a homogeneous transformation\n\nBe careful:\n\n\\[\n_B^W\\mathbf{T}^{-1} \\neq _A^W\\mathbf{T}^t\n\\]\nas was the case with rotation matrices."
  },
  {
    "objectID": "lecs/w02/lec02.html#physical-models-of-how-systems-move",
    "href": "lecs/w02/lec02.html#physical-models-of-how-systems-move",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Physical models of how systems move",
    "text": "Physical models of how systems move"
  },
  {
    "objectID": "lecs/w02/lec02.html#todays-agenda-2",
    "href": "lecs/w02/lec02.html#todays-agenda-2",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Today’s Agenda",
    "text": "Today’s Agenda\n\n\nFrames of reference\nWays to represent rotations\n\n\n\nSimplified models of vehicles\nForward and inverse kinematics"
  },
  {
    "objectID": "lecs/w02/lec02.html#why-simplified",
    "href": "lecs/w02/lec02.html#why-simplified",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Why simplified?",
    "text": "Why simplified?\n\n“All models are wrong, but some are useful” – George Box (statistician)\nModel: a function that describes a physical phenomenon or a system, i.e. how a set of input variables cause a set of output variables.\nModels are useful if they can predict reality up to some degree .\nMismatch between model prediction and reality = error / noise"
  },
  {
    "objectID": "lecs/w02/lec02.html#noise",
    "href": "lecs/w02/lec02.html#noise",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Noise",
    "text": "Noise\n\nAnything that we do not bother modelling with our model\nExample 1: “assume frictionless surface”\nExample 2: Taylor series expansion (only first few terms are dominant)\nWith models, can be thought of as approximation error."
  },
  {
    "objectID": "lecs/w02/lec02.html#simplified-physical-models-of-robotic-vehicles",
    "href": "lecs/w02/lec02.html#simplified-physical-models-of-robotic-vehicles",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Simplified physical models of robotic vehicles",
    "text": "Simplified physical models of robotic vehicles\n\nOmnidirectional motion\nDubins car\nDifferential drive steering\nAckerman steering\nUnicycle\nCartpole\nQuadcopter"
  },
  {
    "objectID": "lecs/w02/lec02.html#omnidirectional-robots",
    "href": "lecs/w02/lec02.html#omnidirectional-robots",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Omnidirectional Robots",
    "text": "Omnidirectional Robots"
  },
  {
    "objectID": "lecs/w02/lec02.html#omnidirectional-robots-1",
    "href": "lecs/w02/lec02.html#omnidirectional-robots-1",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Omnidirectional Robots",
    "text": "Omnidirectional Robots"
  },
  {
    "objectID": "lecs/w02/lec02.html#the-state-of-an-omnidirectional-robot",
    "href": "lecs/w02/lec02.html#the-state-of-an-omnidirectional-robot",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "The state of an omnidirectional robot",
    "text": "The state of an omnidirectional robot\nState := Configuration := \\(\\mathbf{X}\\) := vector of physical quantities of interest about the system\n\n\n\n\n\\[\n\\mathbf{X} = [^Gp_x, ^Gp_y, ^G\\theta]\n\\]\nState = [Position, Orientation]\nPosition of the robot’s frame of reference C with respect to a fixed frame of reference G, expressed in coordinates of frame G. Angle is the orientation of frame C with respect to frame G."
  },
  {
    "objectID": "lecs/w02/lec02.html#control-of-an-omnidirectional-robot",
    "href": "lecs/w02/lec02.html#control-of-an-omnidirectional-robot",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Control of an omnidirectional robot",
    "text": "Control of an omnidirectional robot\nControl := \\(\\mathbf{u}\\) := a vector of input commands that can modify the state of the system\n\n\n\n\n\\[\n\\mathbf{u} = [^Cv_x, ^Cv_y, ^Cw_z]\n\\]\nControl = [Linear velocity, Angular velocity]\nLinear and angular velocity of the robot’s frame of reference C with respect to a fixed frame of reference G, expressed in coordinates of frame C."
  },
  {
    "objectID": "lecs/w02/lec02.html#dynamics-of-an-omnidirectional-robot",
    "href": "lecs/w02/lec02.html#dynamics-of-an-omnidirectional-robot",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Dynamics of an omnidirectional robot",
    "text": "Dynamics of an omnidirectional robot\nDynamical System : = Dynamics := a function that describes the time evolution of the state in response to a control signal\n\n\n\n\nContinuous case:\n\\[\n\\begin{align}\n\\frac{dx}{dt} &= \\dot{x} = f(x, u) \\\\\n\\dot{p}_x &= v_x \\\\\n\\dot{p}_y &= v_y \\\\\n\\dot{\\theta} &= \\omega_z\n\\end{align}\n\\]\nNote: reference frames have been removed for readability."
  },
  {
    "objectID": "lecs/w02/lec02.html#the-state-of-a-simple-car",
    "href": "lecs/w02/lec02.html#the-state-of-a-simple-car",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "The state of a simple car",
    "text": "The state of a simple car\n\nState = [Position and orientation]\nPosition of the car’s frame of reference C with respect to a fixed frame of reference G, expressed in frame G.\nThe angle is the orientation of frame C with respect to G.\n\\[\n\\mathbf{x} = [^Gp_x, ^Gp_y, ^G\\theta]\n\\]"
  },
  {
    "objectID": "lecs/w02/lec02.html#the-controls-of-a-simple-car",
    "href": "lecs/w02/lec02.html#the-controls-of-a-simple-car",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "The controls of a simple car",
    "text": "The controls of a simple car\n\nControls = [Forward speed and angular velocity]\nLinear velocity and angular velocity of the car’s frame of reference C with respect to a fixed frame of reference G, expressed in coordinates of C.\n\\[\n\\mathbf{u} = [^Cv_x, ^Cw_z]\n\\]"
  },
  {
    "objectID": "lecs/w02/lec02.html#the-dynamical-system-of-a-simple-car",
    "href": "lecs/w02/lec02.html#the-dynamical-system-of-a-simple-car",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "The dynamical system of a simple car",
    "text": "The dynamical system of a simple car\n\n\n\n\n\\[\n\\begin{align}\n\\dot{p}_x &= v_x \\cos(\\theta) \\\\\n\\dot{p}_y &= v_x \\sin(\\theta) \\\\\n\\dot{\\theta} &= \\omega_z\n\\end{align}\n\\]\nNote: reference frames have been removed for readability."
  },
  {
    "objectID": "lecs/w02/lec02.html#kinematics-vs-dynamics",
    "href": "lecs/w02/lec02.html#kinematics-vs-dynamics",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Kinematics vs Dynamics",
    "text": "Kinematics vs Dynamics\n\nKinematics considers models of locomotion independently of external forces and control.\nFor example, it describes how the speed of a car affects the state without considering what the required control commands required to generate those speeds are.\nDynamics considers models of locomotion as functions of their control inputs and state."
  },
  {
    "objectID": "lecs/w02/lec02.html#special-case-of-simple-car-dubins-car",
    "href": "lecs/w02/lec02.html#special-case-of-simple-car-dubins-car",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Special case of simple car: Dubins car",
    "text": "Special case of simple car: Dubins car\n\n\n\nCan only go forward\nConstant speed\n\n\\[\n^Cv_x = \\text{const} &gt; 0\n\\]\n\nYou only control the angular velocity"
  },
  {
    "objectID": "lecs/w02/lec02.html#special-case-of-simple-car-dubins-car-1",
    "href": "lecs/w02/lec02.html#special-case-of-simple-car-dubins-car-1",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Special case of simple car: Dubins car",
    "text": "Special case of simple car: Dubins car\n\n\n\nCan only go forward\nConstant speed\n\n\\[\n^Cv_x = \\text{const} &gt; 0\n\\]\n\nYou only control the angular velocity"
  },
  {
    "objectID": "lecs/w02/lec02.html#dubins-car-motion-primitives",
    "href": "lecs/w02/lec02.html#dubins-car-motion-primitives",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Dubins car: motion primitives",
    "text": "Dubins car: motion primitives\nThe path of the car can be decomposed to L(eft), R(ight), S(traight) segments.\n\n\n\n\nRSR path"
  },
  {
    "objectID": "lecs/w02/lec02.html#instantaneous-center-of-rotation",
    "href": "lecs/w02/lec02.html#instantaneous-center-of-rotation",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Instantaneous Center of Rotation",
    "text": "Instantaneous Center of Rotation\n\n\n\n\n\n\nIC = Instantaneous Center of Rotation\nThe center of the circle circumscribed by the turning path.\nUndefined for straight path segments."
  },
  {
    "objectID": "lecs/w02/lec02.html#dubins-car-colorbluerightarrow-dubins-boat",
    "href": "lecs/w02/lec02.html#dubins-car-colorbluerightarrow-dubins-boat",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Dubins car \\(\\color{blue}{\\Rightarrow}\\) Dubins boat",
    "text": "Dubins car \\(\\color{blue}{\\Rightarrow}\\) Dubins boat\n\nWhy do we care about a car that can only go forward?\nBecause we can also model idealized airplanes and boats\nDubins boat = Dubins car"
  },
  {
    "objectID": "lecs/w02/lec02.html#dubins-car-colorbluerightarrow-dubins-airplane-in-3d",
    "href": "lecs/w02/lec02.html#dubins-car-colorbluerightarrow-dubins-airplane-in-3d",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Dubins car \\(\\color{blue}{\\Rightarrow}\\) Dubins airplane in 3D",
    "text": "Dubins car \\(\\color{blue}{\\Rightarrow}\\) Dubins airplane in 3D\n\nPitch angle \\(\\phi\\) and forward velocity determine descent rate\nYaw angle \\(\\theta\\) and forward velocity determine turning rate\n\n\n\n\n\n\\[\n\\begin{align}\n\\dot{p}_x &= v_x \\cos(\\theta) \\sin(\\phi) \\\\\n\\dot{p}_y &= v_x \\sin(\\theta) \\sin(\\phi) \\\\\n\\dot{p}_z &= v_x \\cos(\\phi) \\\\\n\\dot{\\theta} &= \\omega_z \\\\\n\\dot{\\phi} &= \\omega_y\n\\end{align}\n\\]\n\n\\(\\theta\\) is yaw\n\\(\\phi\\) is pitch"
  },
  {
    "objectID": "lecs/w02/lec02.html#the-state-of-a-unicycle",
    "href": "lecs/w02/lec02.html#the-state-of-a-unicycle",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "The state of a unicycle",
    "text": "The state of a unicycle\n\n\n\nTop view of a unicycle\n\n\\[\n\\mathbf{x} = [^Gp_x, ^Gp_y, ^G\\theta]\n\\]\nState = [Position, Orientation]\nPosition of the unicycle’s frame of reference U with respect to a fixed frame of reference G, expressed in coordinates of frame G. Angle is the orientation of frame U with respect to frame G.\nQ: Would you put the radius of the unicycle to be part of the state?\n\nA: Most likely not, because it is a constant quantity that we can measure beforehand. But, if we couldn’t measure it, we need to make it part of the state in order to estimate it."
  },
  {
    "objectID": "lecs/w02/lec02.html#controls-of-a-unicycle",
    "href": "lecs/w02/lec02.html#controls-of-a-unicycle",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Controls of a unicycle",
    "text": "Controls of a unicycle\n\n\n\n\n\\[\n\\mathbf{u} = [^Uw_z, ^Uw_y]\n\\]\nControls = [Yaw rate, and pedaling rate]\nYaw and pedaling rates describe the angular velocities of the respective axes of the unicycle’s frame of reference U with respect to a fixed frame of reference G, expressed in coordinates of U."
  },
  {
    "objectID": "lecs/w02/lec02.html#dynamics-of-a-unicycle",
    "href": "lecs/w02/lec02.html#dynamics-of-a-unicycle",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Dynamics of a unicycle",
    "text": "Dynamics of a unicycle\n\n\n\n\n\\[\n\\begin{align}\n\\dot{p}_x &= rw_y\\cos(\\theta) \\\\\n\\dot{p}_y &= rw_y\\sin(\\theta) \\\\\n\\dot{\\theta} &= w_z \\\\\n\\end{align}\n\\]\nr = the radius of the wheel\n\\(rw_y\\) is the forward velocity of the unicycle"
  },
  {
    "objectID": "lecs/w02/lec02.html#the-state-of-a-differential-drive-vehicle",
    "href": "lecs/w02/lec02.html#the-state-of-a-differential-drive-vehicle",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "The state of a differential drive vehicle",
    "text": "The state of a differential drive vehicle\n\n\n\n\n\\[\n\\mathbf{x} = [^Gp_x, ^Gp_y, ^G\\theta]\n\\]\nState = [Position, Orientation]\nPosition of the vehicle’s frame of reference D with respect to a fixed frame of reference G, expressed in coordinates of frame G. Angle is the orientation of frame D with respect to frame G."
  },
  {
    "objectID": "lecs/w02/lec02.html#controls-of-a-differential-drive-vehicle",
    "href": "lecs/w02/lec02.html#controls-of-a-differential-drive-vehicle",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Controls of a differential drive vehicle",
    "text": "Controls of a differential drive vehicle\n\n\n\nICR = Instantaneous Center of Rotation\n\n\\[\n\\mathbf{u} = [u_l, u_r]\n\\]\nControls = [Left wheel and right wheel turning rates]\nWheel turning rates determine the linear velocities of the respective wheels of the vehicle’s frame of reference D with respect to a fixed frame of reference G, expressed in coordinates of D.\n\\[\n\\begin{align}\nv_1 &= (W - H/2)w \\\\\nv_r &= (W + H/2)w \\\\\nv_x &= (v_1 + v_r)/2\n\\end{align}\n\\]\n\\(v_1 = Ru_l\\) R is the wheel radius\n\\(v_r = Ru_r\\)"
  },
  {
    "objectID": "lecs/w02/lec02.html#dynamics-of-a-differential-drive-vehicle",
    "href": "lecs/w02/lec02.html#dynamics-of-a-differential-drive-vehicle",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Dynamics of a differential drive vehicle",
    "text": "Dynamics of a differential drive vehicle\n\n\n\nICR = Instantaneous Center of Rotation\n\n\n\\[\n\\begin{bmatrix}\np_x(t+1) \\\\\np_y(t+1) \\\\\n\\theta(t+1)\n\\end{bmatrix} =\n\\begin{bmatrix}\n\\cos(\\omega \\delta t) & -\\sin(\\omega \\delta t) & 0 \\\\\n\\sin(\\omega \\delta t) & \\cos(\\omega \\delta t) & 0 \\\\\n0 & 0 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\np_x(t) - \\text{ICR}_x \\\\\np_y(t) - \\text{ICR}_y \\\\\n\\theta(t)\n\\end{bmatrix} +\n\\begin{bmatrix}\n\\text{ICR}_x \\\\\n\\text{ICR}_y \\\\\n\\omega \\delta t\n\\end{bmatrix}\n\\]\n\n\\[\n\\text{ICR} = [p_x - W\\sin\\theta, p_y + W\\cos\\theta]\n\\]"
  },
  {
    "objectID": "lecs/w02/lec02.html#dynamics-of-a-differential-drive-vehicle-1",
    "href": "lecs/w02/lec02.html#dynamics-of-a-differential-drive-vehicle-1",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Dynamics of a differential drive vehicle",
    "text": "Dynamics of a differential drive vehicle\n\n\n\n\n\n\\[\n\\begin{bmatrix}\np_x(t+1) \\\\\np_y(t+1) \\\\\n\\theta(t+1)\n\\end{bmatrix} =\n\\begin{bmatrix}\n\\cos(\\omega \\delta t) & -\\sin(\\omega \\delta t) & 0 \\\\\n\\sin(\\omega \\delta t) & \\cos(\\omega \\delta t) & 0 \\\\\n0 & 0 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\np_x(t) - \\text{ICR}_x \\\\\np_y(t) - \\text{ICR}_y \\\\\n\\theta(t)\n\\end{bmatrix} +\n\\begin{bmatrix}\n\\text{ICR}_x \\\\\n\\text{ICR}_y \\\\\n\\omega \\delta t\n\\end{bmatrix}\n\\]\n\n\\[\n\\text{ICR} = [p_x - W\\sin\\theta, p_y + W\\cos\\theta]\n\\]\nSpecial cases:\n\nmoving straight \\(v_l = v_r\\)\nin-place rotation \\(v_l = -v_r\\)\nrotation about the left wheel \\(v_l = 0\\)"
  },
  {
    "objectID": "lecs/w02/lec02.html#ackerman-steering",
    "href": "lecs/w02/lec02.html#ackerman-steering",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Ackerman steering",
    "text": "Ackerman steering\n\n\nhttps://www.youtube.com/watch?v=i6uBwudwA5o"
  },
  {
    "objectID": "lecs/w02/lec02.html#the-state-of-a-double-link-inverted-pendulum-a.k.a.-acrobot",
    "href": "lecs/w02/lec02.html#the-state-of-a-double-link-inverted-pendulum-a.k.a.-acrobot",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "The state of a double-link inverted pendulum (a.k.a. Acrobot)",
    "text": "The state of a double-link inverted pendulum (a.k.a. Acrobot)\n\n\n\n\n\\[\n\\mathbf{x} = [\\theta_1, \\theta_2, \\dot\\theta_1, \\dot\\theta_2]\n\\]\nState = [angle of joint 1, joint 2, joint velocities]\nAngle of joint 2 is expressed with respect to joint 1. Angle of joint 1 is expressed compared to down vector."
  },
  {
    "objectID": "lecs/w02/lec02.html#controls-of-a-double-link-inverted-pendulum-a.k.a.-acrobot",
    "href": "lecs/w02/lec02.html#controls-of-a-double-link-inverted-pendulum-a.k.a.-acrobot",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Controls of a double-link inverted pendulum (a.k.a. Acrobot)",
    "text": "Controls of a double-link inverted pendulum (a.k.a. Acrobot)\n\n\n\n\n\\[\n\\mathbf{u} = [\\tau_1]\n\\]\nControls = [torque applied to joint 1]"
  },
  {
    "objectID": "lecs/w02/lec02.html#dynamics-of-a-double-link-inverted-pendulum-a.k.a-acrobot",
    "href": "lecs/w02/lec02.html#dynamics-of-a-double-link-inverted-pendulum-a.k.a-acrobot",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Dynamics of a double-link inverted pendulum (a.k.a Acrobot)",
    "text": "Dynamics of a double-link inverted pendulum (a.k.a Acrobot)\n\\[\n\\begin{align}\n\\ddot{\\theta}_1 &= -d_1^{-1}(d_2\\ddot{\\theta}_2 + \\phi_1) \\\\\n\\ddot{\\theta}_2 &= \\left(m_2l_{c2}^2 + I_2 - \\frac{d_2^2}{d_1}\\right)^{-1}\\left(\\tau + \\frac{d_2}{d_1}\\phi_1 - m_2gl_1l_{c2}\\dot{\\theta}_1^2\\sin\\theta_2 - \\phi_2\\right) \\\\\nd_1 &= m_1l_{c1}^2 + m_2(l_1^2 + l_{c2}^2 + 2l_1l_{c2}\\cos\\theta_2) + I_1 + I_2) \\\\\nd_2 &= m_2(l_{c2}^2 + l_1l_{c2}\\cos\\theta_2) + I_2 \\\\\n\\phi_1 &= -m_2l_1l_{c2}\\dot{\\theta}_2^2\\sin\\theta_2 - 2m_2l_1l_{c2}\\dot{\\theta}_2\\dot{\\theta}_1\\sin\\theta_2\n+ (m_1l_{c1} + m_2l_1)g\\cos(\\theta_1 - \\pi/2) + \\phi_2 \\\\\n\\phi_2 &= m_2l_{c2}g\\cos(\\theta_1 + \\theta_2 - \\pi/2)\n\\end{align}\n\\]\n\nProvided here just for reference and completeness. You are not expected to know this."
  },
  {
    "objectID": "lecs/w02/lec02.html#dynamics-of-a-double-link-inverted-pendulum-a.k.a-acrobot-1",
    "href": "lecs/w02/lec02.html#dynamics-of-a-double-link-inverted-pendulum-a.k.a-acrobot-1",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Dynamics of a double-link inverted pendulum (a.k.a Acrobot)",
    "text": "Dynamics of a double-link inverted pendulum (a.k.a Acrobot)"
  },
  {
    "objectID": "lecs/w02/lec02.html#the-state-of-a-single-link-cartpole",
    "href": "lecs/w02/lec02.html#the-state-of-a-single-link-cartpole",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "The state of a single-link cartpole",
    "text": "The state of a single-link cartpole\n\n\n\n\n\\[\n\\mathbf{x} = [^Gp_x, ^G{\\dot p_x}, ^G\\theta, ^G\\dot\\theta]\n\\]\nState = [Position and velocity of cart, orientation and angular velocity of pole]"
  },
  {
    "objectID": "lecs/w02/lec02.html#controls-of-a-single-link-cartpole",
    "href": "lecs/w02/lec02.html#controls-of-a-single-link-cartpole",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Controls of a single-link cartpole",
    "text": "Controls of a single-link cartpole\n\n\n\n\n\\[\n\\mathbf{u} = [f]\n\\]\nControls = [Horizontal force applied to cart]"
  },
  {
    "objectID": "lecs/w02/lec02.html#balancing-a-triple-link-pendulum-on-a-cart",
    "href": "lecs/w02/lec02.html#balancing-a-triple-link-pendulum-on-a-cart",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Balancing a triple-link pendulum on a cart",
    "text": "Balancing a triple-link pendulum on a cart"
  },
  {
    "objectID": "lecs/w02/lec02.html#extreme-balancing",
    "href": "lecs/w02/lec02.html#extreme-balancing",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Extreme Balancing",
    "text": "Extreme Balancing"
  },
  {
    "objectID": "lecs/w02/lec02.html#the-state-of-a-double-integrator",
    "href": "lecs/w02/lec02.html#the-state-of-a-double-integrator",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "The state of a double integrator",
    "text": "The state of a double integrator\n\n\n\n\n\\[\n\\mathbf{x} = [^Gp_x]\n\\]\nState = [Position along x-axis]"
  },
  {
    "objectID": "lecs/w02/lec02.html#controls-of-a-double-integrator",
    "href": "lecs/w02/lec02.html#controls-of-a-double-integrator",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Controls of a double integrator",
    "text": "Controls of a double integrator\n\n\n\n\n\\[\n\\mathbf{x} = [^Gu_x]\n\\]\nControls = [Force along x-axis]"
  },
  {
    "objectID": "lecs/w02/lec02.html#dynamics-of-a-double-integrator",
    "href": "lecs/w02/lec02.html#dynamics-of-a-double-integrator",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Dynamics of a double integrator",
    "text": "Dynamics of a double integrator\n\n\n\n\n\\[\n\\ddot x = F\n\\] This corresponds to applying force to a brick of mass 1 to move on frictionless ice. Where is the brick going to end up? Similar to curling."
  },
  {
    "objectID": "lecs/w02/lec02.html#the-state-of-a-quadrotor",
    "href": "lecs/w02/lec02.html#the-state-of-a-quadrotor",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "The state of a quadrotor",
    "text": "The state of a quadrotor\n\n\n\n\n\n\\[\n\\mathbf{x} = [^G\\phi, ^G\\theta, ^G\\psi, ^G\\dot\\phi, ^G\\dot\\theta, ^G\\dot\\psi]\n\\]\nState = [Roll, pitch, yaw, and roll rate, pitch rate, roll rate]\nAngles are with respect to the global frame."
  },
  {
    "objectID": "lecs/w02/lec02.html#controls-of-a-quadrotor",
    "href": "lecs/w02/lec02.html#controls-of-a-quadrotor",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Controls of a quadrotor",
    "text": "Controls of a quadrotor\n\n\n\n\n\\[\n\\mathbf{u} = [T_1, T_2, T_3, T_4]\n\\]\nControls = [Thrusts of four motors]\nOR \\[\n\\mathbf{u} = [M_1, M_2, M_3, M_4]\n\\]\nControls = [Torques of four motors]\nNotice how adjacent motors spin in opposite ways. Why?"
  },
  {
    "objectID": "lecs/w02/lec02.html#what-if-all-four-motors-spin-the-same-direction",
    "href": "lecs/w02/lec02.html#what-if-all-four-motors-spin-the-same-direction",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "What if all four motors spin the same direction?",
    "text": "What if all four motors spin the same direction?"
  },
  {
    "objectID": "lecs/w02/lec02.html#dynamics-of-a-quadrotor",
    "href": "lecs/w02/lec02.html#dynamics-of-a-quadrotor",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Dynamics of a quadrotor",
    "text": "Dynamics of a quadrotor"
  },
  {
    "objectID": "lecs/w02/lec02.html#manipulators",
    "href": "lecs/w02/lec02.html#manipulators",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Manipulators",
    "text": "Manipulators\n\n\n\nRobot arms, industrial robot\n\nRigid bodies(links) connected by joints\nJoints: revolute or prismatic\nDrive: electric or hydraulic\nEnd-effector (tool) mounted on a flange or plate secured to the wrist joint of robot"
  },
  {
    "objectID": "lecs/w02/lec02.html#manipulators-1",
    "href": "lecs/w02/lec02.html#manipulators-1",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Manipulators",
    "text": "Manipulators"
  },
  {
    "objectID": "lecs/w02/lec02.html#manipulators-2",
    "href": "lecs/w02/lec02.html#manipulators-2",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Manipulators",
    "text": "Manipulators\n\nMotion Control Methods\n\nPoint to point control\n\na sequence of discrete points\nspot welding, pick-and-place, loading & unloading\n\nContinuous path control\n\nfollow a prescribed path, controlled-path motion\nSpray painting, Arc welding, Gluing"
  },
  {
    "objectID": "lecs/w02/lec02.html#manipulators-3",
    "href": "lecs/w02/lec02.html#manipulators-3",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Manipulators",
    "text": "Manipulators\n\n\n\nRobot Specifications\n\nNumber of Axes\n\nMajor axes, (1-3) =&gt; Position the wrist\nMinor axes, (4-6) =&gt; Orient the tool\nRedundant, (7-n) =&gt; reaching around obstacles, avoiding undesirable configuration\n\nDegree of Freedom (DOF)\nWorkspace\nPayload (load capacity)\nPrecision v.s. Repeatability\n\n\n\n\nWhich one is more important?"
  },
  {
    "objectID": "lecs/w02/lec02.html#forward-and-inverse-kinematics",
    "href": "lecs/w02/lec02.html#forward-and-inverse-kinematics",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Forward and Inverse Kinematics",
    "text": "Forward and Inverse Kinematics\n\n\nhttps://slideplayer.com/slide/4239432/"
  },
  {
    "objectID": "lecs/w02/lec02.html#controllability",
    "href": "lecs/w02/lec02.html#controllability",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Controllability",
    "text": "Controllability\n\nA system is controllable if there exist control sequences that can bring the system from any state to any other state, in finite time.\nFor example, even though cars are subject to non-holonomic constraints (can’t move sideways directly), they are controllable, They can reach sideways states by parallel parking."
  },
  {
    "objectID": "lecs/w02/lec02.html#passive-dynamics",
    "href": "lecs/w02/lec02.html#passive-dynamics",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Passive Dynamics",
    "text": "Passive Dynamics\n\nDynamics of systems that operate without drawing (a lot of) energy from a power supply.\n\n\n\n\nInteresting because biological locomotion systems are more efficient than current robotic systems."
  },
  {
    "objectID": "lecs/w02/lec02.html#passive-dynamics-1",
    "href": "lecs/w02/lec02.html#passive-dynamics-1",
    "title": "CSC477 Introduction to Mobile Robotics",
    "section": "Passive Dynamics",
    "text": "Passive Dynamics\n\nDynamics of systems that operate without drawing (a lot of) energy from a power supply.\n\n\n\n\nUsually propelled by their own weight.\nInteresting because biological locomotion systems are more efficient than current robotic systems.\n\n\n\nSteve Collins & Andy Ruina, Cornell, 2001"
  },
  {
    "objectID": "lecs/w01/notes.html",
    "href": "lecs/w01/notes.html",
    "title": "CSC477 - Fall 2024",
    "section": "",
    "text": "Missing resources\nVideo missing slides:\n\naerial package delivery\nRobot Surgery daVinci robot-assisted surgery\nORB SLAM video (can find the videos that have the exact same starting slide but content is diff https://www.youtube.com/results?search_query=ORB+-+SLAM+Ra%C3%BAl+Mur-Artal%2C+J.+M.+M.+Montiel+and+Juan+D.+Tard%C3%B3s+Universidad+Zaragoza , videos by this user : https://www.youtube.com/@raulmurartal2764 )\nBeyond the visible spectrum: infrared cameras\n (seems to look like a video thumbnail)\n : also seems to look like a video thumbnail\nnext slide after Inertial Sensors\nExample: flippers on the Aqua robot"
  }
]