---
title: "CSC477 / CSC2630 Introduction to Mobile Robotics"
subtitle: "Week #1: Introduction, Sensors & Actuators"
author: "Florian Shkurti"
format: 
  revealjs:
    slide-number: true
    smaller: true
    footer: '<a href="https://csc477.github.io/website_fall24" target="_blank" style="font-size:0.8em; bottom: -5px;">↩ Back to Course Website</a>'
    css: ../style.css
    chalkboard:
      buttons: true
      boardmarker-width: 2
      chalk-width: 2
      chalk-effect: 1.0
html-math-method:
  method: mathjax
  url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
---

## Today’s agenda

-   Introduction
-   Administrivia
    -   Office hours
    -   Tutorials
    -   Assignment descriptions
    -   Prerequisites
-   Topics covered by the course
-   Sensors and Actuators
-   Quiz about background and interests

## Your TAs

:::::: columns
::: {.column .small-font width="30%"}
Yewon Lee\
MSc student\
Computer Science, UofT\
[csc477-tas\@cs.toronto.edu](mailto:csc477-tas@cs.toronto.edu){.email}
:::

::: {.column .small-font width="30%"}
Yasasa Abeysirigoonawardena\
MSc student\
Computer Science, UofT\

[csc477-tas\@cs.toronto.edu](mailto:csc477-tas@cs.toronto.edu){.email}
:::

::: {.column .small-font width="30%"}
Radian Gondokaryono\
PhD student\
Computer Science, UofT\

[csc477-tas\@cs.toronto.edu](mailto:csc477-tas@cs.toronto.edu){.email}
:::
::::::

## My lab: Robot Vision and Learning (RVL)

::: {layout="[1,1,1]"}
![](img/rvl-lab1.png)

![](img/rvl-lab2.png)

![](img/rvl-lab3.png)
:::

Mission: create algorithms that enable robots to learn to act intelligently in outdoor environments and alongside humans

## How I became interested in robotics

::: {layout="[40, -20, 40]"}
![Mars Exploration Rover](img/mars-rover.png)

![RoboCup, small-sized league](img/robocup.png)
:::

## How I became interested in robotics

![](img/robotics.png)

## How I became interested in robotics

![](img/robotics-circles.png)

## Today you have

:::::: columns
::: {.column width="45%"}
![](img/tdy-robot1.png){height="250"}

![](img/tdy-robot3.png){height="250"}
:::

::: {.column width="5%"}
:::

::: {.column width="45%"}
![](img/tdy-robot2.png){height="250"}

![](img/tdy-robot4.png)
:::
::::::

## Factory Automation

::::: columns
::: {.column width="40%"}
![Autonomous warehouse robots at Amazon](img/amzn-robots.png)
:::

::: {.column width="60%"}
![Autonomous arms at Tesla](img/tsla-robotarms.png)
:::
:::::

## Pipe Inspection

![Manually-controlled inspection robots](img/pipe-inspection.png)

## Nuclear Disaster Cleanup

![Remote-controlled cleaning robot at Fukushima Daiichi, 2011](img/nuclear-cleanup.png)

## Nuclear Disaster Cleanup

![Remote-controlled cleaning robot at Fukushima Daiichi, 2011](img/fukushima-robot.png)

## Nuclear Disaster Cleanup

![Remote-controlled cleaning robot at Chernobyl, 1986](img/chernobyl-robot.png)

## Aerial Package Delivery

## Aerial First-Aid Delivery

![](img/first-aid-delivery.png)

## Smart Wheelchairs

![](img/wheelchair.png)

## Robot Surgery

daVinci robot-assisted surgery

## Precision Agriculture

![farmbot.io](img/farmbot.png)

## Self-driving Trucks

![](img/self-driving-truck.png)

## Mining Operations

![](img/mining-operation.png)

## Oil Spill Containment

![BP Deepwater Horizon Spill, Gulf of Mexico, 2010](img/deepwater-horizon.png)

## Autonomy vs. Remote Control

-   Q: When is full or partial autonomy necessary?

-   Q: When is remote control preferred?

## Today’s agenda

::: grey
-   Introduction
:::

-   Administrivia
    -   Office hours
    -   Tutorials
    -   Assignment descriptions
    -   Prerequisites
-   Topics covered by the course
-   Sensors and Actuators
-   Quiz about background and interests

## Prerequisites

::: medium-font
-   Software Engineering
    -   Loops, conditionals, classes, modularity
    -   Lists, hash maps/dictionaries, trees
    -   Threads, callbacks, remote procedure calls, serialization
-   Linear Algebra
    -   Matrix multiplication and inversion, determinant
    -   Solving systems of equations, Gaussian elimination
    -   Matrix decompositions: Cholesky, QR
    -   Least squares
-   Basic Probability Theory
    -   Multivariate distributions, especially Gaussians
    -   Conditional probability, Bayes’ rule
    -   Maximum likelihood estimation
:::

##  Prerequisites

Currently

Required: CSC209H5; STA256H5; MAT223H5/MAT240H5; MAT232H5; CSC376

Recommended: MAT224H5; CSC384H5; CSC311H5;

## 4 Assignments

-   \~80% coding and the rest theory

-   Starter code will be provided

-   Bonus questions will be provided

-   Accepted languages: Python, C++

-   You’re going to learn ROS (Robot Operating System) and use the Gazebo simulator

-   You’re also going to learn numpy and scipy

-   About 2 weeks to work on each

## ROS + Gazebo simulation

![](img/gazebo-simulation.png)

## 7 Quizzes

-   5-10 mins to complete them

-   Not cumulative in terms of material. They cover only one lecture

-   Meant to check whether you have understood basic concepts

## Evaluation

CSC477

-   4 assignments, 15% each = 60%

-   7 quizzes, 2% each = 14%

-   1 final exam = 26%

<br>

CSC2630

-   3 assignments, 15% each = 45%

-   7 quizzes, 2% each = 14%

-   1 final project = 41%

## Recommended Textbooks (optional)

::: {layout-ncol="3"}
![](img/rec-textbook1.png)

![](img/rec-textbook2.png)

![](img/rec-textbook3.png)
:::

## Recommended Online Courses (optional)

-   Material is related to 477 but not identical
-   I will post links on Quercus to specific lectures that are relevant

{{< pagebreak >}}

-   <https://www.udacity.com/course/artificial-intelligence-for-robotics--cs373>

-   <https://www.edx.org/course/autonomous-mobile-robots-ethx-amrx-1>

-   <https://underactuated.mit.edu/> (more advanced, little overlap with 477)

## Office Hours (Zoom)

-   Florian: Thursdays 3-4pm
-   Yewon: Tuesdays 11-12pm
-   Yasasa: Fridays 11-12pm

{{< pagebreak >}}

-   [Office hours will begin next week]{.underline}

## Online communication

-   Use Quercus

-   Please check your course-related email frequently

-   Email us at [csc477-instructor\@cs.toronto.edu](mailto:csc477-instructor@cs.toronto.edu){.email} and [csc477-tas\@cs.toronto.edu](mailto:csc477-tas@cs.toronto.edu){.email}

-   Anonymous feedback about anything course-related: <https://www.surveymonkey.com/r/H8QH65F>

## Today’s agenda

::: grey
-   Introduction
-   Administrivia
    -   Office hours
    -   Tutorials
    -   Assignment descriptions
    -   Prerequisites
:::

-   Topics covered by the course
-   Sensors and Actuators
-   Quiz about background and interests

## Main topics to be covered

![](img/main-topics.png){width="400" height="500"}

## Covered $\qquad \qquad \qquad$ Not Covered

:::::: columns
::: {.column width="40%"}
![](img/covered-topic.png)
:::

::: {.column width="20%"}
:::

::: {.column width="40%"}
![](img/not-covered.png)
:::
::::::

## Main topics to be covered

::::: columns
::: {.column width="40%"}
![](img/topic1.png)
:::

::: {.column width="60%"}
![](img/robotic-system-diagram.png)

[Main question: what is the next state given the curent state and controls?]{.orange}
:::
:::::

## Main topics to be covered

::::: columns
::: {.column width="40%"}
![](img/topics-2-3.png)
:::

::: {.column width="60%"}
![](img/robotic-system-diagram2.png)

[Main question: what are the controls that will take the system from state A to B?]{.orange}
:::
:::::

## Not covered in CSC477, but related: learning for control

![](img/learning-to-control.png)

## Not covered in CSC477, but related: learning for control

{{< video https://www.youtube.com/watch?v=VHmEdRXSi9M width=700 height=500 >}}

## Main topics to be covered

::::: columns
::: {.column width="40%"}
![](img/topic4.png)
:::

::: {.column width="60%"}
![](img/robotic-system-diagram3.png)
:::
:::::

## Main topics to be covered

-   Known: robot’s position and orientation
-   Want to estimate: a map of the environment from laser measurements

::::: columns
::: {.column width="40%"}
![](img/topic4.png)
:::

::: {.column width="60%"}
![](img/state-estimation-diagram.png)
:::
:::::

-   Occupancy grid mapping

## Main topics to be covered

::::: columns
::: {.column width="40%"}
![](img/topic5.png)
:::

::: {.column width="60%"}
![](img/robotic-system-diagram3.png)
:::
:::::

## orb slam video

https://www.youtube.com/watch?v=\_9VcvGybsDA

## Lecture topics $\qquad \qquad$ Tutorials

:::::: columns
::: {.column width="40%"}
![](img/main-topics.png)
:::

::: {.column width="20%"}
:::

::: {.column width="40%"}
Intro to the Robot Operating System (ROS)

Refresher on linear algebra and least squares

Refresher on basic probability and continuous distributions

How to align 3D pointclouds. Demo of the PCL library

How to implement a Kalman Filter

How to implement a Particle Filter

How to approximate functions
:::
::::::

## Assignments

:::::: columns
::: {.column width="40%"}
![](img/main-topics.png)
:::

::: {.column width="10%"}
![](img/bracket.png){height="180"}
:::

::: {.column width="50%"}
<br>

A1: Designing a feedback controller for wall-following
:::
::::::

## Assignments

:::::: columns
::: {.column width="40%"}
![](img/main-topics.png)
:::

::: {.column width="10%"}
![](img/bracket.png){height="160"}\
![](img/small-bracket.png){height="70"}
:::

::: {.column width="50%"}
<br>

A1: Designing a feedback controller for wall-following

<br>

A2: Implementing path-planning and feedback control algorithms
:::
::::::

## Assignments

:::::: columns
::: {.column width="40%"}
![](img/main-topics.png)
:::

::: {.column width="10%"}
![](img/bracket.png){height="160"}\
![](img/small-bracket.png){height="70"}   ![](img/bracket.png){height="160"}
:::

::: {.column width="50%"}
<br>

A1: Designing a feedback controller for wall-following

<br>

A2: Implementing path-planning and feedback control algorithms

<br>

A3: Occupancy grid mapping with known robot location

A4: Localization in a known map using particle filters
:::
::::::

## Today’s agenda

::: grey
-   Introduction
-   Administrivia
    -   Office hours
    -   Tutorials
    -   Assignment descriptions
    -   Prerequisites
-   Topics covered by the course
:::

-   Sensors and Actuators
-   Quiz about background and interests

## Sensors and Actuators

-   Sensors:
    -   Characteristics and types
    -   Measurement noise
    -   Required bandwidth

{{< pagebreak >}}

-   Actuators:
    -   Types of motors
    -   Pulse-Width Modulation

## Sensors

-   Devices that can sense and measure physical properties of the environment.

-   Key phenomenon is **transduction** (conversion of energy from one form to another). E.g.:

    -   Imaging sensors: light to pixel voltages
    -   Depth sensors: mechanical pressure to voltage

-   Measurements are **noisy**, and difficult to interpret

## Sensors: general characteristics

-   Sensitivity: (change of output) ÷ (change of input)
-   Linearity: constancy of (output ÷ input)
-   Measurement range: \[min, max\] or {min, max}
-   Response time: time required for input change to cause output change
-   Accuracy: difference between measurement and actual
-   Repeatability/Drift: difference between repeated measures
-   Resolution: smallest observable increment
-   Bandwidth: required rate of data transfer
-   SNR: signal-to-noise ratio

## Sensors: vision

::::: columns
::: {.column width="30%"}
![CCD image sensor](img/ccd-imgsensor.png)
:::

::: {.column .small-font width="70%"}
**CCD (charge-coupled device) imaging sensors:**

-   Capacitor array accumulates electric charge proportional to light intensity.
-   Each capacitor’s charge is transferred to its neighbor.
-   Last capacitor’s charge gets amplified and output as voltage.
-   (+) High-quality, low-noise images
-   (-) Higher power consumption
-   (-) Slow readout
-   (-) Specialized fabrication

voltage → analog-to-digital converter → pixel value in {0, 255}

**CMOS (complementary metal-oxide semi-conductor) imaging sensors:**

-   One amplifier per pixel
-   (+) Low power
-   (+) Fast readout
-   (+) Easier to fabricate
-   (-) Poor low-light sensitivity
-   (-) Higher noise
:::
:::::

## Global vs. Rolling Shutter

Shutter = mechanism that allows light to hit the imaging sensor

Shutter “speed” = Exposure time = time duration in which the sensor is exposed to light

::::: columns
::: {.column width="50%"}
![](img/shutters.png)
:::

::: {.column width="50%"}
![Rolling shutter](img/rolling-shutter.png)
:::
:::::

## Reading RGB images from a camera

::: {layout-ncol="3"}
[Each pixel contains an intensity value from 0…255]{.small-font}

![](img/parrot.png)
:::

::: {layout-ncol="3"}
![600 x 1000 pixels](img/parot-red.png)

![600 x 1000 pixels](img/parrot-green.png)

![600 x 1000 pixels](img/parrot-blue.png)
:::

## Reading RGB images from a camera

::: {layout-ncol="3"}
Each pixel contains an intensity value from 0…255

![](img/parrot.png)

$\to$ [A matrix of 600 x 1000 x 3 = \~ 1.8 million numbers]{.small-font}
:::

::: {layout-ncol="3"}
![600 x 1000 pixels](img/img-pixel.png)

![600 x 1000 pixels](img/img-pixel.png)

![600 x 1000 pixels](img/img-pixel.png)
:::

## Computer/robot vision

:::::: columns
::: {.column width="45%"}
![Structured numbers](img/img-pixel.png)
:::

::: {.column width="10%"}
![](img/blue-arrow.png)
:::

::: {.column width="45%"}
1.  I’m seeing a parrot

2.  I’m seeing a toy bicycle

3.  The parrot is riding the bicycle

4.  The bicycle is on top of a desk

5.  Is this physically plausible?

6.  Where is the parrot in 3D w.r.t. the camera?

7.  Where will the parrot go next?

8.  What is the speed of the parrot?

Conclusions/Inference/Deduction/Estimation
:::
::::::

## Camera lenses

::::: columns
::: {.column width="40%"}
-   Lens determines:
    -   image distortion
    -   focus
    -   sharpness or blur
-   Lens characteristics:
    -   focal length
    -   aperture
    -   depth-of-field
:::

::: {.column width="60%"}
![](img/camera-lens.png)
:::
:::::

## Pinhole Camera Model {.center}

![](img/pinhole-dog.png)

::: medium-font
We know **approximately** how a 3D point (X,Y,Z) projects to pixel (x,y)\
We call this the ***pinhole projection model***
:::

## (1) Perspective projection \[x,y\] = 𝜋(X,Y,Z)

::::: columns
::: {.column width="45%"}
![](img/perspective-projection.png){height="400"}

[<http://www.cim.mcgill.ca/%7Elanger/558.html>]{.small-font}
:::

::: {.column width="55%"}
By similar triangles: x/f = X/Z

So, x = f \* X/Z and similarly y = f \* Y/Z

Problem: we just lost depth (Z) information by doing this projection, i.e. depth is now uncertain.
:::
:::::

## (2) Lens distortion

::: {layout-ncol="2"}
![](img/lens-distortion1.png)

![](img/lens-distortion2.png)
:::

## (2) Estimating parameters of lens distortion

::: {layout="[45, 10, 45]" layout-valign="center"}
![](img/chessboard.png)

![](img/orange-arrow.png)

![](img/chessboard2.png)
:::

## Non-pinhole cameras: thin lens model

::: {layout="[[1, 1], [1, 1]]" layout-valign="bottom"}
![](img/non-pinhole-camera.png)

Unlike the pinhole camera, this is able to model blur.

![<http://www.cim.mcgill.ca/%7Elanger/558.html>](img/non-pinhole-camera2.png)
:::

## Beyond the visible spectrum: infrared cameras

![](img/infrared-cameras.png)

## Beyond the visible spectrum: infrared cameras

![Drawback: Doesn’t work underwater](img/infrared-camera2.png)

## Beyond the visible spectrum: infrared cameras

![](img/infrared-camera3.png)

## Beyond the visible spectrum: RGBD cameras

::::: columns
::: {.column width="50%"}
![](img/rgbd-camera.png)
:::

::: {.column width="50%"}
Main ideas:

-   Active sensing
-   Projector emits infrared light in the scene
-   Infrared sensor reads the infrared light
-   Deformation of the expected pattern allows computation of the depth
:::
:::::

## Beyond the visible spectrum: RGBD cameras

::::: columns
::: {.column width="50%"}
Drawbacks:

-   Does not work well outdoors, sunlight saturates its measurements
-   Maximum range is \[0.5, 8\] meters

Advantages:

-   Real-time depth estimation at 30Hz
-   Cheap
:::

::: {.column width="50%"}
![](img/rgbd-camera2.png)
:::
:::::

## Beyond the visible spectrum: RGBD cameras

::::: columns
::: {.column width="50%"}
Enabled a wave of research, applications, and video games, based on real-time skeleton tracking
:::

::: {.column width="50%"}
![](img/rgbd-camera3.png)
:::
:::::

## Beyond the visible spectrum: RGBD cameras

::::: columns
::: {.column width="40%"}
Despite their drawbacks RGBD sensors have been extensively used in robotics.
:::

::: {.column width="60%"}
{{< video https://www.youtube.com/watch?v=7vq-1TiXi3g width="100%" height=450 >}}
:::
:::::

## 3D LIDAR (Light detection and ranging)

::::: columns
::: {.column .small-font width="50%"}
Produces a pointcloud of 3D points and intensities

-   (x,y,z) in the laser’s frame of reference
-   Intensity is related to the material of the object that reflects the light

Works based on time-of-flight for each beam to return back to the scanner

![](img/3d-lidar-diagram.jpeg){height="100"}

Not very robust to adverse weather conditions: rain, snow, smoke, fog etc.

Used in most self-driving cars today for obstacle detection. Range \< 100m.
:::

::: {.column width="50%"}
![Usually around 1million points in a single pointcloud](img/3d-lidar.png)
:::
:::::

## 2D LIDAR (Light detection and ranging)

::::::: columns
:::: {.column .small-font width="50%"}
Produces a scan of 2D points and intensities

-   (x,y) in the laser’s frame of reference
-   Intensity is related to the material of the object that reflects the light

Certain surfaces are problematic for LIDAR: e.g. glass

::: {.fragment fragment-index="1"}
![](img/2d-lidar2.png){height="120"}

Lots of moving parts: motors quickly rotate the laser beam and once complete (angle bound reached) a scan is returned. I.e. points are not strictly speaking time-synchronized, even though we usually treat them as such.
:::
::::

:::: {.column .small-font width="50%"}
![](img/2d-lidar.png)

::: {.fragment .right-align fragment-index="1"}
Usually around 1024 points in a single scan.
:::
::::
:::::::

## Inertial Sensors

-   Gyroscopes, Accelerometers, Magnetometers

-   Inertial Measurement Unit (IMU)

-   Perhaps the most important sensor for 3D navigation, along with the GPS

-   Without IMUs, plane autopilots would be much harder, if not impossible, to build

## 

![](img/inertial-sensor.png)

## Gyroscopes

-   Measure angular velocity in the body frame

-   Often affected by noise and bias

$$
w_\text{measured}(t) = w_\text{true}(t) + b_g(t) + n_g(t)
$$

-   We integrate it to get 3D orientation (Euler angles, quaternions rotation matrices), but there is drift due to noise and bias

## Accelerometers

-   Measure linear acceleration relative to freefall (measured in g)

-   A free-falling accelerometer in a vacuum would measure zero g

-   An accelerometer resting on the surface of the earth would measure 1g

-   Also affected by bias and noise.

-   Double integration to get position is very noisy. Errors grow quadratically with time.

## Magnetometers

::::: columns
::: {.column width="35%"}
Drawbacks:

-   Needs careful calibration
-   Needs to be placed away from moving metal parts, motors

Advantages:

-   Can be used as a compass for absolute heading
:::

::: {.column width="65%"}
![](img/magnetometer.png)
:::
:::::

## Inertial Measurement Unit

-   Combines measurements from accelerometer, gyroscope, and magnetometer to output an estimate of orientation with reduced drift.

-   Does not typically provide a position estimate, due to double integration.

-   Runs at 100-1000Hz

-   Expect yaw drift of 5-10 deg/hour on most modern low-end IMUs

## Global Positioning System: Satellites

-   Each GPS satellite periodically transmits:

-   [\[Coarse/Acquisition code\]]{.red} A 1023-bit pseudorandom binary sequence (PRN code), which repeats every 1 ms, unique for each satellite (no correlation with other satellites).

-   [\[Navigation frame\]]{.red} A 1500-bit packet that contains

    -   GPS date, time, satellite health
    -   Detailed orbital data for the satellite, accurate for the next \~4hrs
    -   PRN codes and status of all satellites in the network
    -   Takes 12.5mins to transmit

-   [\[Precision code\]]{.red} A 6.2-terabit code for military use.

-   Carrier frequencies are 1575.42 MHz (L1) and 1227.60 MHz (L2)

## Global Positioning System: Receivers

-   Each (civilian) GPS receiver:
    -   Knows the PRN codes for each satellite in advance
    -   Correlates received PRN signal with database PRN signal → time shift → noisy distance to satellite
    -   If 4 or more satellite PRN codes are received, it does **trilateration** to compute latitude and longitude

## Global Positioning System: Receivers and Dilution of Precision

![](img/global-positioning-system.png){height="400"}

## Hall Effect Sensor

::::: columns
::: {.column width="80%"}
-   Varies its voltage in response to a magnetic field
-   Used as a proximity switch, to measure a full rotation of a wheel for example
-   Used to measure rate of rotation of wheels
:::

::: {.column width="20%"}
![](img/hall-effect-sensor.png){.absolute height="200" right="0" top="0"}
:::
:::::

## Rotary Encoder

-   Contains an analog to digital converter for encoding the angle of a shaft/motor/axle

-   Usually outputs the discretized absolute angle of the shaft/motor/axle

![](img/rotary-encoder.png){height="180"}

-   Useful in order to know where different shafts are relative to each to each other.

## Example: flippers on the Aqua robot

![](img/aqua-robot.png)

## Actuators

::: {layout="[[1,1,1], [1,1,1]]"}
![**DC (direct current) motor**](img/dc-motor.png)

![**Servo motor**](img/servo-motor.png)

![**Stepper motor**](img/stepper-motor.png)

[They turn continuously at high RPM (revolutions per minute) when voltage is applied. Used in quadrotors and planes, model cars etc.]{.small-font}

[Usually includes: DC motor, gears, control circuit, position feedback\
Precise control without free rotation (e.g. robot arms, boat rudders) Limited turning range: 180 degrees]{.small-font}

[Positioning feedback and no positioning errors.\
Rotates by a predefined step angle.\
Requires external control circuit.\
Precise control without free rotation.\
Constant holding torque without powering the motor (good for robot arms or weight-carrying systems).]{.small-font}
:::

## Pulse Width Modulation

![](img/duty-cycle.png){height="400"}

::: small-font
Used for creating analog/continuous behavior when voltage applied is discrete.\
Main idea: turn on and off the motor fast enough so average voltage is the desired target.\
Used in dimming LEDs, controlling the speed of DC motors, controlling the position of servo motors.
:::

## Today’s agenda

::: grey
-   Introduction
-   Administrivia
    -   Office hours
    -   Tutorials
    -   Assignment descriptions
    -   Prerequisites
-   Topics covered by the course
-   Sensors and Actuators
:::

-   Quiz about background and interests