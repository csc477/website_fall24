<!DOCTYPE html>
<html lang="en"><head>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-html/tabby.min.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-e1a5c8363afafaef2c763b6775fbf3ca.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.7.31">

  <meta name="author" content="Author">
  <title>CSC477 - Fall 2024 – Presentation Title</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/theme/quarto-2c1b5f745a11cfad616ebade4a4a7d24.css">
  <link href="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
<meta property="og:title" content="Presentation Title – CSC477 - Fall 2024">
<meta property="og:site_name" content="CSC477 - Fall 2024">
<meta name="twitter:title" content="Presentation Title – CSC477 - Fall 2024">
<meta name="twitter:card" content="summary">
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Presentation Title</h1>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Author 
</div>
</div>
</div>

</section>
<section class="slide level2">


<img data-src="img/Lecture8_0.jpg" class="r-stretch"></section>
<section id="csc477-introduction-to-mobile-robotics" class="title-slide slide level1 center">
<h1>CSC477Introduction to Mobile Robotics</h1>

</section>

<section id="florian-shkurti" class="title-slide slide level1 center">
<h1>Florian Shkurti</h1>
<p>Week #8: Bayes’ Filters and Kalman Filter</p>
</section>

<section id="recommended-reading" class="title-slide slide level1 center">
<h1>Recommended reading</h1>
<p>Chapters 2 and 3.2 from Probabilistic Robotics</p>
<p>Chapters 4.9 and 8.3 from Computational Principles of Mobile Robotics</p>
<p>Lesson 2 in <a href="https://www.udacity.com/course/artificial-intelligence-for-robotics--cs373">https://www.udacity.com/course/artificial-intelligence-for-robotics--cs373</a></p>
<p>This illustrative blog post:</p>
<p><a href="http://www.bzarg.com/p/how-a-kalman-filter-works-in-pictures/">http://www.bzarg.com/p/how-a-kalman-filter-works-in-pictures/</a></p>
<p>Careful: the figure between equations (9) and (10) is wrong. The blue Gaussian should be taller and</p>
<p>peakier than the other two Gaussians, the prior and the measurement models. This is not fixed as</p>
<p>of March 15, 2017.</p>
</section>

<section id="filtering-vs.-smoothing" class="title-slide slide level1 center">
<h1>Filtering vs.&nbsp;Smoothing</h1>
<p>Smoothing/Batch Estimation</p>
<p>Filtering Estimation</p>
<p><img data-src="img/Lecture8_1.png"></p>
<p><span style="color:#ff0000">Sequence </span></p>
<p><span style="color:#ff0000">of states</span></p>
<p><span style="color:#ff0000">Sequence of</span></p>
<p><span style="color:#ff0000">sensor</span></p>
<p><span style="color:#ff0000">measurements</span></p>
<p><span style="color:#ff0000">Sequence of</span></p>
<p><span style="color:#ff0000">commands</span></p>
<p><img data-src="img/Lecture8_2.png"></p>
</section>

<section id="whats-the-difference" class="title-slide slide level1 center">
<h1>What’s the difference?</h1>
<p>Smoothing/Batch Estimation</p>
<p>Filtering Estimation</p>
<p><span style="color:#ff0000">All measurements and </span></p>
<p><span style="color:#ff0000">controls are known </span></p>
<p><span style="color:#ff0000">in advance</span></p>
<p><img data-src="img/Lecture8_3.png"></p>
<p><span style="color:#ff0000">Measurements and controls </span></p>
<p><span style="color:#ff0000">are processed online as they come.</span></p>
<p><span style="color:#ff0000">Future measurements are unknown.</span></p>
<p><img data-src="img/Lecture8_4.png"></p>
</section>

<section id="why-do-we-use-filtering" class="title-slide slide level1 center">
<h1>Why do we use filtering?</h1>
<p>Online belief updates: filters provide a principled way to incorporate noisy information from sensor measurements, which can change our prior belief, in an online fashion.</p>
<p>Sensor fusion: filters enable us to combine measurements from multiple different noisy sensors into one coherent state estimate. E.g. camera + laser, camera + IMU, multiple cameras, sonar and IMU, GPS and IMU etc.</p>
<p><span style="color:#ff0000">Technically speaking, this is also </span></p>
<p><span style="color:#ff0000">true for smoothing estimators.</span></p>
</section>

<section id="bayes-filter" class="title-slide slide level1 center">
<h1>Bayes’ Filter</h1>
<ul>
<li class="fragment">A generic class of filters that make use of Bayes’ rule and assume the following:
<ul>
<li class="fragment"><strong>Markov Assumption For Dynamics</strong> : the state is conditionally independent of past states and controls, given the previous state . In other words, the dynamics model is assumed to satisfy</li>
<li class="fragment"><strong>Static World Assumption: </strong> the current observation is conditionally independent of past observations and controls, given the current state</li>
</ul></li>
</ul>
<p><img data-src="img/Lecture8_5.png"></p>
<p><img data-src="img/Lecture8_6.png"></p>
<p><img data-src="img/Lecture8_7.png"></p>
<p><img data-src="img/Lecture8_8.png"></p>
<ul>
<li class="fragment">A generic class of filters that make use of Bayes’ rule and assume the following:
<ul>
<li class="fragment"><strong>Markov Assumption For Dynamics</strong> : the state is conditionally independent of past states and controls, given the previous state . In other words, the dynamics model is assumed to satisfy</li>
<li class="fragment"><strong>Static World Assumption: </strong> the current observation is conditionally independent of past observations and controls, given the current state</li>
</ul></li>
</ul>
<p><img data-src="img/Lecture8_9.png"></p>
<p><img data-src="img/Lecture8_10.png"></p>
<p><img data-src="img/Lecture8_11.png"></p>
<p><img data-src="img/Lecture8_12.png"></p>
<p><span style="color:#ff0000">Note: the Markov assumption is </span></p>
<p><span style="color:#ff0000">more general than what we have </span></p>
<p><span style="color:#ff0000">presented here. </span></p>
</section>

<section id="bayes-filter-derivation" class="title-slide slide level1 center">
<h1>Bayes’ Filter: Derivation</h1>
<p><img data-src="img/Lecture8_13.png"></p>
<p><img data-src="img/Lecture8_14.png"></p>
<p><img data-src="img/Lecture8_15.png"></p>
<p><img data-src="img/Lecture8_16.png"></p>
<p><span style="color:#ff0000">Conditional Bayes’ Rule</span></p>
<p><img data-src="img/Lecture8_17.png"></p>
<p><img data-src="img/Lecture8_18.png"></p>
<p><span style="color:#ff0000">Normalizing factor that makes the integral/sum of the numerator in Bayes’ Rule be 1.</span></p>
<p><img data-src="img/Lecture8_19.png"></p>
<p><span style="color:#ff0000">Static World Assumption</span></p>
<p><img data-src="img/Lecture8_20.png"></p>
<p><span style="color:#ff0000">Marginalization, or law of total probability</span></p>
<p><img data-src="img/Lecture8_21.png"></p>
<p><span style="color:#ff0000">where the sum enumerates all possibilities over </span></p>
<p><span style="color:#ff0000">the variable Bi. If we see Bi as a set, then the </span></p>
<p><span style="color:#ff0000">collection of Bi’s must be pairwise disjoint. I.e.</span></p>
<p><span style="color:#ff0000">the collection of subsets Bi must be a partition of </span></p>
<p><span style="color:#ff0000">the sample space. </span></p>
<p><img data-src="img/Lecture8_22.jpg"></p>
<p><img data-src="img/Lecture8_23.png"></p>
<p><span style="color:#ff0000">Marginalization, or law of total probability</span></p>
<p><img data-src="img/Lecture8_24.png"></p>
<p><span style="color:#ff0000">Here we are actually using the law of total </span></p>
<p><span style="color:#ff0000">probability for conditional distributions, so</span></p>
<p><img data-src="img/Lecture8_25.png"></p>
<p><img data-src="img/Lecture8_26.png"></p>
<p><span style="color:#ff0000">Definition of conditional distribution</span></p>
<p><img data-src="img/Lecture8_27.png"></p>
<p><img data-src="img/Lecture8_28.png"></p>
<p><span style="color:#ff0000">Markov assumption for dynamics</span></p>
<p><img data-src="img/Lecture8_29.png"></p>
<p><img data-src="img/Lecture8_30.png"></p>
<p><img data-src="img/Lecture8_31.png"></p>
<p><img data-src="img/Lecture8_32.png"></p>
<p><img data-src="img/Lecture8_33.png"></p>
<p><span style="color:#ff0000">This is the belief at the previous time step!</span></p>
<p><span style="color:#ff0000">This means we can perform filtering recursively.</span></p>
<p><img data-src="img/Lecture8_34.png"></p>
<p><img data-src="img/Lecture8_35.png"></p>
<p><span style="color:#ff0000">Control at time t-1 only affects state at time t</span></p>
<p><img data-src="img/Lecture8_36.png"></p>
<p><img data-src="img/Lecture8_37.png"></p>
<p><img data-src="img/Lecture8_38.png"></p>
<p><img data-src="img/Lecture8_39.png"></p>
</section>

<section id="kalman-filter-an-instance-of-bayes-filter" class="title-slide slide level1 center">
<h1>Kalman Filter: an instance of Bayes’ Filter</h1>
<p><img data-src="img/Lecture8_40.png"></p>
<p><span style="color:#ff0000">Linear dynamics with Gaussian noise</span></p>
<p><img data-src="img/Lecture8_41.png"></p>
<p><span style="color:#ff0000">Linear observations with Gaussian noise</span></p>
<p><img data-src="img/Lecture8_42.png"></p>
<p><span style="color:#ff0000">Initial belief is Gaussian</span></p>
<p><img data-src="img/Lecture8_43.png"></p>
</section>

<section id="kalman-filter-assumptions" class="title-slide slide level1 center">
<h1>Kalman Filter: assumptions</h1>
<p>Two assumptions inherited from Bayes’ Filter</p>
<p>Linear dynamics and observation models</p>
<p>Initial belief is Gaussian</p>
<p>Noise variables and initial state</p>
<p>are jointly Gaussian and independent</p>
<p>Noise variables are independent and identically distributed</p>
<p>Noise variables are independent and identically distributed</p>
<p><img data-src="img/Lecture8_44.png"></p>
<p><img data-src="img/Lecture8_45.png"></p>
<p><img data-src="img/Lecture8_46.png"></p>
<p><img data-src="img/Lecture8_47.png"></p>
<p><img data-src="img/Lecture8_48.png"></p>
</section>

<section id="kalman-filter-why-so-many-assumptions" class="title-slide slide level1 center">
<h1>Kalman Filter: why so many assumptions?</h1>
<p>Two assumptions inherited from Bayes’ Filter</p>
<p>Linear dynamics and observation models</p>
<p>Initial belief is Gaussian</p>
<p>Noise variables and initial state</p>
<p>are jointly Gaussian and independent</p>
<p>Noise variables are independent and identically distributed</p>
<p>Noise variables are independent and identically distributed</p>
<p><span style="color:#ff0000">Without linearity there is no closed-form solution for the posterior belief in the Bayes’ Filter. Recall that if X is Gaussian then Y=</span> <span style="color:#ff0000">AX+b</span> <span style="color:#ff0000"> is also Gaussian. This is not true in general if Y=h(X). </span></p>
<p><span style="color:#ff0000">Also, we will see later that applying Bayes’ rule to a Gaussian prior and a Gaussian measurement likelihood results in a Gaussian posterior.</span></p>
<p><img data-src="img/Lecture8_49.png"></p>
<p><img data-src="img/Lecture8_50.png"></p>
<p><img data-src="img/Lecture8_51.png"></p>
<p><img data-src="img/Lecture8_52.png"></p>
<p><img data-src="img/Lecture8_53.png"></p>
<p>Two assumptions inherited from Bayes’ Filter</p>
<p>Linear dynamics and observation models</p>
<p>Initial belief is Gaussian</p>
<p>Noise variables and initial state</p>
<p>are jointly Gaussian and independent</p>
<p>Noise variables are independent and identically distributed</p>
<p>Noise variables are independent and identically distributed</p>
<p><span style="color:#ff0000">This results in the belief remaining Gaussian</span></p>
<p><span style="color:#ff0000">after each propagation and update step. This means that we only have to worry about how the mean and the covariance of the belief evolve recursively with each prediction step and update step </span></p>
<p><span style="color:#ff0000"> </span> <span style="color:#ff0000">COOL! </span></p>
<p><img data-src="img/Lecture8_54.png"></p>
<p><img data-src="img/Lecture8_55.png"></p>
<p><img data-src="img/Lecture8_56.png"></p>
<p><img data-src="img/Lecture8_57.png"></p>
<p><img data-src="img/Lecture8_58.png"></p>
<p>Two assumptions inherited from Bayes’ Filter</p>
<p>Linear dynamics and observation models</p>
<p>Initial belief is Gaussian</p>
<p>Noise variables and initial state</p>
<p>are jointly Gaussian and independent</p>
<p>Noise variables are independent and identically distributed</p>
<p>Noise variables are independent and identically distributed</p>
<p><img data-src="img/Lecture8_59.png"></p>
<p><img data-src="img/Lecture8_60.png"></p>
<p><img data-src="img/Lecture8_61.png"></p>
<p><img data-src="img/Lecture8_62.png"></p>
<p><img data-src="img/Lecture8_63.png"></p>
<p><span style="color:#ff0000">This makes the recursive updates of the mean and covariance much simpler.</span></p>
<p><span style="color:#ff0000">Assumptions guarantee that if the prior belief before the prediction</span></p>
<p><span style="color:#ff0000">step is Gaussian</span></p>
</section>

<section id="kalman-filter-an-instance-of-bayes-filter-1" class="title-slide slide level1 center">
<h1>Kalman Filter: an instance of Bayes’ Filter</h1>
<p><img data-src="img/Lecture8_64.png"></p>
<p><span style="color:#ff0000">then the prior belief after the prediction step will be Gaussian </span></p>
<p><span style="color:#ff0000">and the posterior belief (after the update step) will be Gaussian. </span></p>
<p><img data-src="img/Lecture8_65.png"></p>
<p><span style="color:#ff0000">Belief after prediction step (to simplify notation)</span></p>
<p><span style="color:#ff0000">So, under the Kalman Filter assumptions we get</span></p>
<p><img data-src="img/Lecture8_66.png"></p>
<p><img data-src="img/Lecture8_67.png"></p>
<p><img data-src="img/Lecture8_68.png"></p>
<p><span style="color:#ff0000">Notation: estimate at time t given history of observations and</span></p>
<p><span style="color:#ff0000">controls up to time t-1 </span></p>
<p><img data-src="img/Lecture8_69.png"></p>
<p><span style="color:#ff0000">So, under the Kalman Filter assumptions we get</span></p>
<p><span style="color:#ff0000">Two main questions:</span></p>
<p><img data-src="img/Lecture8_70.png"></p>
<p><span style="color:#ff0000">How to get prediction mean and covariance from</span></p>
<p><span style="color:#ff0000"> prior mean and covariance? </span></p>
<p><img data-src="img/Lecture8_71.png"></p>
<p><span style="color:#ff0000">2. How to get posterior mean and covariance from</span></p>
<p><span style="color:#ff0000"> prediction mean and covariance? </span></p>
<p><span style="color:#ff0000">These questions were answered in the 1960s. The resulting algorithm</span></p>
<p><span style="color:#ff0000">was used in the Apollo missions to the moon, and in almost every</span></p>
<p><span style="color:#ff0000">system in which there is a noisy sensor involved </span> <span style="color:#ff0000"> COOL!</span></p>
<p><img data-src="img/Lecture8_72.png"></p>
</section>

<section id="kalman-filter-with-1d-state" class="title-slide slide level1 center">
<h1>Kalman Filter with 1D state</h1>
<p>Let’s start with the update step recursion. Here’s an example:</p>
<p><img data-src="img/Lecture8_73.png"></p>
<p>Suppose your measurement model is</p>
<p>with</p>
<p><img data-src="img/Lecture8_74.png"></p>
<p><img data-src="img/Lecture8_75.png"></p>
<p><img data-src="img/Lecture8_76.png"></p>
<p>Suppose your first noisy measurement is</p>
<p>Suppose your belief after the prediction step is</p>
<p><img data-src="img/Lecture8_77.png"></p>
<p><img data-src="img/Lecture8_78.png"></p>
<p><img data-src="img/Lecture8_79.png"></p>
<p>Q: What is the mean and covariance of ?</p>
<p><img data-src="img/Lecture8_80.png"></p>
</section>

<section id="kalman-filter-with-1d-state-the-update-step" class="title-slide slide level1 center">
<h1>Kalman Filter with 1D state:the update step</h1>
<p>From Bayes’ Filter we get so</p>
<p><img data-src="img/Lecture8_81.png"></p>
<p><img data-src="img/Lecture8_82.png"></p>
<p><img data-src="img/Lecture8_83.png"></p>
<p><img data-src="img/Lecture8_84.png"></p>
<p><img data-src="img/Lecture8_85.png"></p>
<p><img data-src="img/Lecture8_86.png"></p>
<p><img data-src="img/Lecture8_87.png"></p>
<p>From Bayes’ Filter we get so</p>
<p><img data-src="img/Lecture8_88.png"></p>
<p><img data-src="img/Lecture8_89.png"></p>
<p><img data-src="img/Lecture8_90.png"></p>
<p><img data-src="img/Lecture8_91.png"></p>
<p><img data-src="img/Lecture8_92.png"></p>
<p><span style="color:#ff0000">Prediction residual/error between</span></p>
<p><span style="color:#ff0000">actual observation and expected </span></p>
<p><span style="color:#ff0000">observation.</span></p>
<p><span style="color:#ff0000">You expected the measured mean </span></p>
<p><span style="color:#ff0000">to be 0, according to your prediction</span></p>
<p><span style="color:#ff0000">prior, but you actually observed 5.</span></p>
<p><span style="color:#ff0000">The smaller this prediction error is the better </span></p>
<p><span style="color:#ff0000">your estimate will be, or the better it will agree </span></p>
<p><span style="color:#ff0000">with the measurements.</span></p>
<p><img data-src="img/Lecture8_93.png"></p>
<p><img data-src="img/Lecture8_94.png"></p>
<p>From Bayes’ Filter we get so</p>
<p><img data-src="img/Lecture8_95.png"></p>
<p><img data-src="img/Lecture8_96.png"></p>
<p><img data-src="img/Lecture8_97.png"></p>
<p><img data-src="img/Lecture8_98.png"></p>
<p><img data-src="img/Lecture8_99.png"></p>
<p><span style="color:#ff0000">Kalman Gain: specifies</span></p>
<p><span style="color:#ff0000">how much effect will the </span></p>
<p><span style="color:#ff0000">measurement have in the </span></p>
<p><span style="color:#ff0000">posterior, compared to the </span></p>
<p><span style="color:#ff0000">prediction prior. Which one do you </span></p>
<p><span style="color:#ff0000">trust more, your prior </span></p>
<p><span style="color:#ff0000">or your measurement ? </span></p>
<p><img data-src="img/Lecture8_100.png"></p>
<p><img data-src="img/Lecture8_101.png"></p>
<p><img data-src="img/Lecture8_102.png"></p>
<p><img data-src="img/Lecture8_103.png"></p>
<p>From Bayes’ Filter we get so</p>
<p><img data-src="img/Lecture8_104.png"></p>
<p><img data-src="img/Lecture8_105.png"></p>
<p><img data-src="img/Lecture8_106.png"></p>
<p><img data-src="img/Lecture8_107.png"></p>
<p><img data-src="img/Lecture8_108.png"></p>
<p><span style="color:#ff0000">The measurement is more confident</span></p>
<p><span style="color:#ff0000">(lower variance) than the prior, so</span></p>
<p><span style="color:#ff0000">the posterior mean is going to be </span></p>
<p><span style="color:#ff0000">closer to 5 than to 0. </span></p>
<p><img data-src="img/Lecture8_109.png"></p>
<p><img data-src="img/Lecture8_110.png"></p>
<p>From Bayes’ Filter we get so</p>
<p><img data-src="img/Lecture8_111.png"></p>
<p><img data-src="img/Lecture8_112.png"></p>
<p><img data-src="img/Lecture8_113.png"></p>
<p><img data-src="img/Lecture8_114.png"></p>
<p><img data-src="img/Lecture8_115.png"></p>
<p><img data-src="img/Lecture8_116.png"></p>
<p><span style="color:#ff0000">No matter what happens, the variance of the </span></p>
<p><span style="color:#ff0000">posterior is going to be reduced. I.e. new </span></p>
<p><span style="color:#ff0000">measurement increases confidence no matter </span></p>
<p><span style="color:#ff0000">how noisy it is. </span></p>
<p><img data-src="img/Lecture8_117.png"></p>
<p>From Bayes’ Filter we get so</p>
<p><img data-src="img/Lecture8_118.png"></p>
<p><img data-src="img/Lecture8_119.png"></p>
<p><img data-src="img/Lecture8_120.png"></p>
<p><img data-src="img/Lecture8_121.png"></p>
<p><img data-src="img/Lecture8_122.png"></p>
<p><img data-src="img/Lecture8_123.png"></p>
<p><img data-src="img/Lecture8_124.png"></p>
<p><span style="color:#ff0000">In fact you can write this as</span></p>
<p><span style="color:#ff0000">so and</span></p>
<p><span style="color:#ff0000">I.e. the posterior is more confident than both </span></p>
<p><span style="color:#ff0000">the prior and the measurement. </span></p>
<p><img data-src="img/Lecture8_125.png"></p>
<p><img data-src="img/Lecture8_126.png"></p>
<p><img data-src="img/Lecture8_127.png"></p>
<p>From Bayes’ Filter we get so</p>
<p><img data-src="img/Lecture8_128.png"></p>
<p><img data-src="img/Lecture8_129.png"></p>
<p><img data-src="img/Lecture8_130.png"></p>
<p><img data-src="img/Lecture8_131.png"></p>
<p><img data-src="img/Lecture8_132.png"></p>
<p><img data-src="img/Lecture8_133.png"></p>
<p>In this example:</p>
<p><img data-src="img/Lecture8_134.png"></p>
<p><img data-src="img/Lecture8_135.png"></p>
<p>Another example:</p>
<p><img data-src="img/Lecture8_136.png"></p>
<p><img data-src="img/Lecture8_137.png"></p>
<p><img data-src="img/Lecture8_138.png"></p>
<p><img data-src="img/Lecture8_139.png"></p>
<p><img data-src="img/Lecture8_140.png"></p>
<p><img data-src="img/Lecture8_141.png"></p>
<p>Take-home message: new observations, no matter how noisy, always <strong>reduce </strong></p>
<p><strong>uncertainty</strong> in the posterior. The mean of the posterior, on the other hand,</p>
<p>only changes when there is a nonzero prediction residual.</p>
</section>

<section id="kalman-filter-with-1d-state-the-propagationprediction-step" class="title-slide slide level1 center">
<h1>Kalman Filter with 1D state:the propagation/prediction step</h1>
<p><img data-src="img/Lecture8_142.png"></p>
<p>Suppose that the dynamics model is</p>
<p><img data-src="img/Lecture8_143.png"></p>
<p><img data-src="img/Lecture8_144.png"></p>
<p>and you applied the command . Then</p>
<p><img data-src="img/Lecture8_145.png"></p>
<p><img data-src="img/Lecture8_146.png"></p>
<p><span style="color:#ff0000">Recall: this notation means</span></p>
<p><span style="color:#ff0000">expected value with respect to</span></p>
<p><span style="color:#ff0000">conditional expectation, </span> <span style="color:#ff0000">i.e</span></p>
<p><img data-src="img/Lecture8_147.png"></p>
<p><img data-src="img/Lecture8_148.png"></p>
<p><img data-src="img/Lecture8_149.png"></p>
<p><span style="color:#ff0000">Control is a constant with </span></p>
<p><span style="color:#ff0000">respect to the distribution</span></p>
<p><img data-src="img/Lecture8_150.png"></p>
<p><span style="color:#ff0000">Dynamics noise is zero mean, </span></p>
<p><span style="color:#ff0000">and independent of observations</span></p>
<p><span style="color:#ff0000">and controls</span></p>
<p><img data-src="img/Lecture8_151.png"></p>
<p>Suppose that the dynamics model is</p>
<p><img data-src="img/Lecture8_152.png"></p>
<p><img data-src="img/Lecture8_153.png"></p>
<p>and you applied the command . Then</p>
<p><img data-src="img/Lecture8_154.png"></p>
<p><img data-src="img/Lecture8_155.png"></p>
<p><img data-src="img/Lecture8_156.png"></p>
<p><span style="color:#ff0000">Recall: this notation means</span></p>
<p><span style="color:#ff0000">covariance with respect to</span></p>
<p><span style="color:#ff0000">conditional expectation, </span> <span style="color:#ff0000">i.e</span></p>
<p><img data-src="img/Lecture8_157.png"></p>
<p><img data-src="img/Lecture8_158.png"></p>
<p><img data-src="img/Lecture8_159.png"></p>
<p>Suppose that the dynamics model is</p>
<p><img data-src="img/Lecture8_160.png"></p>
<p><img data-src="img/Lecture8_161.png"></p>
<p>and you applied the command . Then</p>
<p><img data-src="img/Lecture8_162.png"></p>
<p><img data-src="img/Lecture8_163.png"></p>
<p><img data-src="img/Lecture8_164.png"></p>
<p><img data-src="img/Lecture8_165.png"></p>
<p><span style="color:#ff0000">Recall: covariance neglects addition </span></p>
<p><span style="color:#ff0000">of constant terms, i.e. </span></p>
<p><span style="color:#ff0000">Cov</span> <span style="color:#ff0000">(</span> <span style="color:#ff0000">X+b</span> <span style="color:#ff0000">) = </span> <span style="color:#ff0000">Cov</span> <span style="color:#ff0000">(X)</span></p>
<p><img data-src="img/Lecture8_166.png"></p>
<p>Suppose that the dynamics model is</p>
<p><img data-src="img/Lecture8_167.png"></p>
<p><img data-src="img/Lecture8_168.png"></p>
<p>and you applied the command . Then</p>
<p><img data-src="img/Lecture8_169.png"></p>
<p><img data-src="img/Lecture8_170.png"></p>
<p><img data-src="img/Lecture8_171.png"></p>
<p><img data-src="img/Lecture8_172.png"></p>
<p><span style="color:#ff0000">Recall:</span></p>
<p><span style="color:#ff0000">Cov</span> <span style="color:#ff0000">(X+Y)=</span> <span style="color:#ff0000">Cov</span> <span style="color:#ff0000">(X)+</span> <span style="color:#ff0000">Cov</span> <span style="color:#ff0000">(Y)-2Cov(X,Y)</span></p>
<p><span style="color:#ff0000">Recall: we denote </span> <span style="color:#ff0000">Cov</span> <span style="color:#ff0000">(X,X)=</span> <span style="color:#ff0000">Cov</span> <span style="color:#ff0000">(X)</span></p>
<p><span style="color:#ff0000">as a shorthand</span></p>
<p><span style="color:#ff0000">If this algebra doesn’t make sense, see the </span> <span style="color:#ff0000">GraphSLAM</span> <span style="color:#ff0000"> lecture</span> <span style="color:#ff0000">.</span></p>
<p><img data-src="img/Lecture8_173.png"></p>
<p>Suppose that the dynamics model is</p>
<p><img data-src="img/Lecture8_174.png"></p>
<p><img data-src="img/Lecture8_175.png"></p>
<p>and you applied the command . Then</p>
<p><img data-src="img/Lecture8_176.png"></p>
<p><img data-src="img/Lecture8_177.png"></p>
<p><img data-src="img/Lecture8_178.png"></p>
<p><img data-src="img/Lecture8_179.png"></p>
<p><span style="color:#ff0000">We assumed noise variables are </span></p>
<p><span style="color:#ff0000">independent of state. So this covariance </span></p>
<p><span style="color:#ff0000">is zero. </span></p>
<p><span style="color:#ff0000">We assumed dynamics noise is independent</span></p>
<p><span style="color:#ff0000">of past measurement and controls </span></p>
<p><img data-src="img/Lecture8_180.png"></p>
<p>Suppose that the dynamics model is</p>
<p><img data-src="img/Lecture8_181.png"></p>
<p><img data-src="img/Lecture8_182.png"></p>
<p>and you applied the command . Then</p>
<p><img data-src="img/Lecture8_183.png"></p>
<p><img data-src="img/Lecture8_184.png"></p>
<p><img data-src="img/Lecture8_185.png"></p>
<p><img data-src="img/Lecture8_186.png"></p>
<p><img data-src="img/Lecture8_187.png"></p>
<p>Suppose that the dynamics model is</p>
<p><img data-src="img/Lecture8_188.png"></p>
<p><img data-src="img/Lecture8_189.png"></p>
<p>and you applied the command . Then</p>
<p><img data-src="img/Lecture8_190.png"></p>
<p><img data-src="img/Lecture8_191.png"></p>
<p><img data-src="img/Lecture8_192.png"></p>
<p><img data-src="img/Lecture8_193.png"></p>
<p><img data-src="img/Lecture8_194.png"></p>
<p>Suppose that the dynamics model is</p>
<p><img data-src="img/Lecture8_195.png"></p>
<p><img data-src="img/Lecture8_196.png"></p>
<p>and you applied the command . Then</p>
<p><img data-src="img/Lecture8_197.png"></p>
<p><img data-src="img/Lecture8_198.png"></p>
<p><img data-src="img/Lecture8_199.png"></p>
<p><img data-src="img/Lecture8_200.png"></p>
<p>Take home message: uncertainty __ increases __ after the prediction step,</p>
<p>because we are speculating about the future.</p>
</section>

<section id="kalman-filter-with-2d-state" class="title-slide slide level1 center">
<h1>Kalman Filter with 2D state</h1>
<p>Suppose we have a robot that moves on a 1D line, but we also</p>
<p>want to estimate its velocity. Then the 2D state vector is</p>
<p>Suppose we do not have any control over this robot, i.e. we are just</p>
<p>trying to estimate its state through <strong>observations of the position </strong></p>
<p><strong>only</strong> . I.e.:</p>
<p>Also suppose that we predict zero acceleration in the near future, so</p>
<p>which in vector form is expressed as</p>
<p><img data-src="img/Lecture8_201.png"></p>
<p><img data-src="img/Lecture8_202.png"></p>
<p><img data-src="img/Lecture8_203.png"></p>
<p><img data-src="img/Lecture8_204.png"></p>
<p><img data-src="img/Lecture8_205.png"></p>
<p><img data-src="img/Lecture8_206.png"></p>
<p><img data-src="img/Lecture8_207.png"></p>
<p>Suppose we have a robot that moves on a 1D line, but we also</p>
<p>want to estimate its velocity. Then the 2D state vector is</p>
<p>Suppose we do not have any control over this robot, i.e. we are just</p>
<p>trying to estimate its state through <strong>observations of the position </strong></p>
<p><strong>only</strong> . I.e.:</p>
<p>Also suppose that we predict zero acceleration in the near future, so</p>
<p>which in vector form is expressed as</p>
<p><img data-src="img/Lecture8_208.png"></p>
<p><img data-src="img/Lecture8_209.png"></p>
<p><img data-src="img/Lecture8_210.png"></p>
<p><img data-src="img/Lecture8_211.png"></p>
<p><span style="color:#ff0000">For this example suppose </span></p>
<p><span style="color:#ff0000">dt=1, r = 1 and</span></p>
<p><img data-src="img/Lecture8_212.png"></p>
<p><img data-src="img/Lecture8_213.png"></p>
<p><img data-src="img/Lecture8_214.png"></p>
<p><img data-src="img/Lecture8_215.png"></p>
<p><img data-src="img/Lecture8_216.png"></p>
<p>Suppose that at time t the state is distributed as</p>
<p>with</p>
<p><img data-src="img/Lecture8_217.png"></p>
<p><img data-src="img/Lecture8_218.png"></p>
<p><img data-src="img/Lecture8_219.png"></p>
<p>In other words, we are confident that in the beginning</p>
<p>the position is with high probability (~0.997) within range</p>
<p>of the mean position, 0.</p>
<p><img data-src="img/Lecture8_220.png"></p>
<p>We are not very confident in the velocity, however. We just know</p>
<p>a priori that with high probability (~0.997) it is within range</p>
<p>of the mean velocity, 1.</p>
<p><img data-src="img/Lecture8_221.png"></p>
<p><img data-src="img/Lecture8_222.png"></p>
<p>Suppose that at time t the state is distributed as</p>
<p>with</p>
<p><img data-src="img/Lecture8_223.png"></p>
<p><img data-src="img/Lecture8_224.png"></p>
<p><img data-src="img/Lecture8_225.png"></p>
<p>In other words, we are confident that in the beginning</p>
<p>the position is with high probability (~0.997) within range</p>
<p>of the mean position, 0.</p>
<p><img data-src="img/Lecture8_226.png"></p>
<p>We are not very confident in the velocity, however. We just know</p>
<p>a priori that with high probability (~0.997) it is within range</p>
<p>of the mean velocity, 1.</p>
<p><img data-src="img/Lecture8_227.png"></p>
<p><span style="color:#ff0000">Notice that when the cross-correlation terms </span></p>
<p><span style="color:#ff0000">then the ellipse is axis-aligned. This means that the position</span></p>
<p><span style="color:#ff0000">and velocity are initially uncorrelated. </span></p>
<p><img data-src="img/Lecture8_228.png"></p>
</section>

<section id="kalman-filter-with-2d-state-the-propagationprediction-step" class="title-slide slide level1 center">
<h1>Kalman Filter with 2D state:the propagation/prediction step</h1>
<p><img data-src="img/Lecture8_229.png"></p>
<p>After the prediction step the state is distributed as</p>
<p>with</p>
<p><img data-src="img/Lecture8_230.png"></p>
<p><img data-src="img/Lecture8_231.png"></p>
<p><img data-src="img/Lecture8_232.png"></p>
<p><img data-src="img/Lecture8_233.png"></p>
<p>After the prediction step the state is distributed as</p>
<p>with</p>
<p><img data-src="img/Lecture8_234.png"></p>
<p><span style="color:#ff0000">Many things to notice here:</span></p>
<p><span style="color:#ff0000">The covariance has nonzero off-diagonal terms, so the position and velocity are now correlated. This is why the orange ellipse is rotated.</span></p>
<p><span style="color:#ff0000">Also, the orange ellipse is “larger” than the initial blue ellipse, which means that our uncertainty has increased by speculating for future outcomes. </span></p>
<p><span style="color:#ff0000">There is now large uncertainty in the predicted position, since there was large uncertainty in the velocity.</span></p>
<p><img data-src="img/Lecture8_235.png"></p>
<p><img data-src="img/Lecture8_236.png"></p>
</section>

<section id="kalman-filter-with-2d-state-the-update-step" class="title-slide slide level1 center">
<h1>Kalman Filter with 2D state:the update step</h1>
<p><img data-src="img/Lecture8_237.png"></p>
<p>Before the update step the state is distributed as</p>
<p>with and</p>
<p><img data-src="img/Lecture8_238.png"></p>
<p><img data-src="img/Lecture8_239.png"></p>
<p><img data-src="img/Lecture8_240.png"></p>
<p>At this point we predict that the next measurement of the position is going to be with</p>
<p>uncertainty which depends on previous uncertainty and measurement uncertainty.</p>
<p>Suppose that we actually measure which means that our mean estimate of the velocity was way off (it was 1).</p>
<p>Therefore, there is a prediction residual/error</p>
<p>How confident are we about this residual?</p>
<p><img data-src="img/Lecture8_241.png"></p>
<p><img data-src="img/Lecture8_242.png"></p>
<p><img data-src="img/Lecture8_243.png"></p>
<p><img data-src="img/Lecture8_244.png"></p>
<p><img data-src="img/Lecture8_245.png"></p>
<p><img data-src="img/Lecture8_246.png"></p>
<p>Before the update step the state is distributed as</p>
<p>with and</p>
<p><img data-src="img/Lecture8_247.png"></p>
<p><img data-src="img/Lecture8_248.png"></p>
<p><img data-src="img/Lecture8_249.png"></p>
<p>At this point we predict that the next measurement of the position is going to be with</p>
<p>uncertainty which depends on previous state’s uncertainty and measurement uncertainty.</p>
<p>Suppose that we actually measure which means that our mean estimate of the velocity was way off (it was 1).</p>
<p>Therefore, there is a prediction residual/error</p>
<p>How confident are we about this residual?</p>
<p><img data-src="img/Lecture8_250.png"></p>
<p><img data-src="img/Lecture8_251.png"></p>
<p><img data-src="img/Lecture8_252.png"></p>
<p><img data-src="img/Lecture8_253.png"></p>
<p><img data-src="img/Lecture8_254.png"></p>
<p><span style="color:#ff0000">This means that our </span></p>
<p><span style="color:#ff0000">measurement was within </span></p>
<p><span style="color:#ff0000">a range of from the</span></p>
<p><span style="color:#ff0000">true position with high </span></p>
<p><span style="color:#ff0000">probability (~0.997) </span></p>
<p><img data-src="img/Lecture8_255.png"></p>
<p><img data-src="img/Lecture8_256.png"></p>
<p>How do we update our belief based on the noisy measurement?</p>
<p>We’re not going to provide a proof here (see Probabilistic</p>
<p>Robotics, section 3.2), but the updated belief is</p>
<p>with</p>
<p><img data-src="img/Lecture8_257.png"></p>
<p><img data-src="img/Lecture8_258.png"></p>
<p><span style="color:#ff0000">Kalman Gain:</span></p>
<p><span style="color:#ff0000">determines how much the state and the covariance needs to be updated</span></p>
<p><img data-src="img/Lecture8_259.png"></p>
<p><img data-src="img/Lecture8_260.png"></p>
<p><img data-src="img/Lecture8_261.png"></p>
<p>How do we update our belief based on the noisy measurement?</p>
<p>We’re not going to provide a proof here (see Probabilistic</p>
<p>Robotics, section 3.2), but the updated belief is</p>
<p>with</p>
<p><img data-src="img/Lecture8_262.png"></p>
<p><img data-src="img/Lecture8_263.png"></p>
<p><span style="color:#ff0000">Kalman Gain:</span></p>
<p><span style="color:#ff0000">determines how much the state and the covariance needs to be updated</span></p>
<p><img data-src="img/Lecture8_264.png"></p>
<p><img data-src="img/Lecture8_265.png"></p>
<p><span style="color:#ff0000">After the measurement the covariance was reduced. We are now more confident than both the measurement and the prediction estimate. </span></p>
<p><img data-src="img/Lecture8_266.png"></p>
<p>How do we update our belief based on the noisy measurement?</p>
<p>We’re not going to provide a proof here (see Probabilistic</p>
<p>Robotics, section 3.2), but the updated belief is</p>
<p>with</p>
<p><img data-src="img/Lecture8_267.png"></p>
<p><img data-src="img/Lecture8_268.png"></p>
<p><img data-src="img/Lecture8_269.png"></p>
<p><img data-src="img/Lecture8_270.png"></p>
<p><span style="color:#ff0000">Also, notice how we MEASURED position, and through correlation, we were able to INFER velocity. This is not always possible.</span></p>
</section>

<section id="kalman-filter-in-n-dimensions" class="title-slide slide level1 center">
<h1>Kalman Filter in N dimensions</h1>
<p><img data-src="img/Lecture8_271.png"></p>
<p><img data-src="img/Lecture8_272.png"></p>
<p><img data-src="img/Lecture8_273.png"></p>
<p><img data-src="img/Lecture8_274.png"></p>
<p><img data-src="img/Lecture8_275.png"></p>
<p><img data-src="img/Lecture8_276.png"></p>
<p><img data-src="img/Lecture8_277.png"></p>
<p>Received measurement but expected to receive</p>
<p><img data-src="img/Lecture8_278.png"></p>
<p><img data-src="img/Lecture8_279.png"></p>
<p>Prediction residual is a Gaussian random variable</p>
<p>where the covariance of the residual is</p>
<p><img data-src="img/Lecture8_280.png"></p>
<p><img data-src="img/Lecture8_281.png"></p>
<p>Kalman Gain (optimal correction factor):</p>
<p><img data-src="img/Lecture8_282.png"></p>
<p><img data-src="img/Lecture8_283.png"></p>
<p><img data-src="img/Lecture8_284.png"></p>
<p><img data-src="img/Lecture8_285.png"></p>
<p><img data-src="img/Lecture8_286.png"></p>
<p><img data-src="img/Lecture8_287.png"></p>
<p><img data-src="img/Lecture8_288.png"></p>
<p><img data-src="img/Lecture8_289.png"></p>
<p><img data-src="img/Lecture8_290.png"></p>
<p><img data-src="img/Lecture8_291.png"></p>
<p>Received measurement but expected to receive</p>
<p><img data-src="img/Lecture8_292.png"></p>
<p><img data-src="img/Lecture8_293.png"></p>
<p>Prediction residual is a Gaussian random variable</p>
<p>where the covariance of the residual is</p>
<p><img data-src="img/Lecture8_294.png"></p>
<p><img data-src="img/Lecture8_295.png"></p>
<p><span style="color:#ff0000">Potentially expensive and</span></p>
<p><span style="color:#ff0000">error-prone operation: matrix inversion O(|z|^2.4)</span></p>
<p>Kalman Gain (optimal correction factor):</p>
<p><img data-src="img/Lecture8_296.png"></p>
<p><img data-src="img/Lecture8_297.png"></p>
<p><img data-src="img/Lecture8_298.png"></p>
<p><img data-src="img/Lecture8_299.png"></p>
<p><img data-src="img/Lecture8_300.png"></p>
<p><img data-src="img/Lecture8_301.png"></p>
<p><img data-src="img/Lecture8_302.png"></p>
<p><img data-src="img/Lecture8_303.png"></p>
<p><img data-src="img/Lecture8_304.png"></p>
<p><img data-src="img/Lecture8_305.png"></p>
<p>Received measurement but expected to receive</p>
<p><img data-src="img/Lecture8_306.png"></p>
<p><img data-src="img/Lecture8_307.png"></p>
<p>Prediction residual is a Gaussian random variable</p>
<p>where the covariance of the residual is</p>
<p><img data-src="img/Lecture8_308.png"></p>
<p><img data-src="img/Lecture8_309.png"></p>
<p><span style="color:#ff0000">Numerical errors may make the covariance non-symmetric at some point. In practice, we either force symmetry, or we decompose the covariance during the update. </span></p>
<p><span style="color:#ff0000">See “Factorization methods for discrete sequential estimation” by Gerald </span> <span style="color:#ff0000">Bierman</span></p>
<p><span style="color:#ff0000">for more info.</span></p>
<p>Kalman Gain (optimal correction factor):</p>
<p><img data-src="img/Lecture8_310.png"></p>
<p><img data-src="img/Lecture8_311.png"></p>
<p><img data-src="img/Lecture8_312.png"></p>
</section>

<section id="kalman-filter-with-4d-state" class="title-slide slide level1 center">
<h1>Kalman Filter with 4D state</h1>
<p>Suppose a cannonball is shot from a cannon, and assume we can somehow measure its position in flight.</p>
<p>Assuming zero drag and resistance from the air, the only force acting on the ball after it is ejected is its weight (suppose mass=1kg).</p>
<p>Then the continuous dynamics of the system are given by where is noise in the acceleration.</p>
<p>The discrete-time version of this dynamics model is</p>
<p>which can be expressed in matrix form as where</p>
<p>Since we can measure its position the measurement model is where and</p>
<p><img data-src="img/Lecture8_313.png"></p>
<p><img data-src="img/Lecture8_314.png"></p>
<p><img data-src="img/Lecture8_315.png"></p>
<p><img data-src="img/Lecture8_316.png"></p>
<p><img data-src="img/Lecture8_317.png"></p>
<p><img data-src="img/Lecture8_318.png"></p>
<p><img data-src="img/Lecture8_319.png"></p>
<p><img data-src="img/Lecture8_320.png"></p>
<p><img data-src="img/Lecture8_321.png"></p>
<p><img data-src="img/Lecture8_322.png"></p>
<p><img data-src="img/Lecture8_323.png"></p>
<p><img data-src="img/Lecture8_324.png"></p>
<p><img data-src="img/Lecture8_325.png"></p>
<p><img data-src="img/Lecture8_326.png"></p>
<p>Suppose a cannonball is shot from a cannon, and assume we can somehow measure its position in flight.</p>
<p>Assuming zero drag and resistance from the air, the only force acting on the ball after it is ejected is its weight (suppose mass=1kg).</p>
<p>Then the continuous dynamics of the system are given by where is noise in the acceleration.</p>
<p>The discrete-time version of this dynamics model is</p>
<p>which can be expressed in matrix form as where</p>
<p>Since we can measure its position the measurement model is where and</p>
<p><img data-src="img/Lecture8_327.png"></p>
<p><img data-src="img/Lecture8_328.png"></p>
<p><img data-src="img/Lecture8_329.png"></p>
<p><img data-src="img/Lecture8_330.png"></p>
<p><img data-src="img/Lecture8_331.png"></p>
<p><img data-src="img/Lecture8_332.png"></p>
<p><img data-src="img/Lecture8_333.png"></p>
<p><img data-src="img/Lecture8_334.png"></p>
<p><img data-src="img/Lecture8_335.png"></p>
<p><img data-src="img/Lecture8_336.png"></p>
<p><img data-src="img/Lecture8_337.png"></p>
<p><img data-src="img/Lecture8_338.png"></p>
<p><img data-src="img/Lecture8_339.png"></p>
<p><img data-src="img/Lecture8_340.png"></p>
<p><span style="color:#ff0000">Notice here that we don’t actually control the system, </span></p>
<p><span style="color:#ff0000">but we include to account for additive constants in the dynamics. </span></p>
<p><img data-src="img/Lecture8_341.png"></p>
<p><img data-src="img/Lecture8_342.png"></p>
<p><em>This is the</em></p>
<p><em>uncertainty around</em></p>
<p><em>the mean estimate.</em></p>
<p><em>With probability ~0.997</em></p>
<p><em>the true should be </em></p>
<p><em>within these bounds,</em></p>
<p><em>as long as the system</em></p>
<p><em>has been initialized </em></p>
<p><em>close enough to the </em></p>
<p><em>true initial state, and</em></p>
<p><em>as long as the KF </em></p>
<p><em>assumptions hold. </em></p>
<p><img data-src="img/Lecture8_343.png"></p>
<p><img data-src="img/Lecture8_344.png"></p>
<p><img data-src="img/Lecture8_345.png"></p>
<p><em>We initialized the KF estimated </em></p>
<p><em>mean position to be [0, 2] </em></p>
<p><em>when the true value was [0, 0]</em></p>
<p><em>and we assigned high </em></p>
<p><em>uncertainty in the initial</em></p>
<p><em>position estimate, which</em></p>
<p><em>was reduced after the first</em></p>
<p><em>few measurements. </em></p>
<p><img data-src="img/Lecture8_346.png"></p>
<p><em>We initialized the KF estimated </em></p>
<p><em>mean y-velocity to be 5 </em></p>
<p><em>when the true value was 10</em></p>
<p><em>and we assigned high </em></p>
<p><em>uncertainty in the initial</em></p>
<p><em>y-velocity estimate.</em></p>
<p><em>Even though we do not measure</em></p>
<p><em>the velocity directly, through </em></p>
<p><em>correlation with position, the KF</em></p>
<p><em>is able to INFER it and the </em></p>
<p><em>initially large uncertainty shrinks</em></p>
<p><em>as more measurements are </em></p>
<p><em>received.</em></p>
<p><img data-src="img/Lecture8_347.png"></p>
<p><em>We initialized the KF estimated </em></p>
<p><em>mean y-velocity to be 5 </em></p>
<p><em>when the true value was 10</em></p>
<p><em>and we assigned high </em></p>
<p><em>uncertainty in the initial</em></p>
<p><em>y-velocity estimate.</em></p>
<p><em>Even though we do not measure</em></p>
<p><em>the velocity directly, through </em></p>
<p><em>correlation with position, the KF</em></p>
<p><em>is able to INFER it and the </em></p>
<p><em>initially large uncertainty shrinks</em></p>
<p><em>as more measurements become</em></p>
<p><em>are received.</em></p>
<p><span style="color:#ff0000">Parameters and code to reproduce this can be found at </span> <span style="color:#ff0000"><a href="https://github.com/florianshkurti/comp417/tree/master/filtering_examples">https://github.com/florianshkurti/comp417/tree/master/filtering_examples</a></span></p>
</section>

<section id="appendix-1" class="title-slide slide level1 center">
<h1>Appendix 1</h1>
<p><img data-src="img/Lecture8_348.png"></p>
<p><img data-src="img/Lecture8_349.png"></p>
<p>Claim: where</p>
<p>Proof:</p>
<p><img data-src="img/Lecture8_350.png"></p>
<p><img data-src="img/Lecture8_351.png"></p>
<p><img data-src="img/Lecture8_352.png"></p>
<p><img data-src="img/Lecture8_353.png"></p>
<p><img data-src="img/Lecture8_354.png"></p>
<p><img data-src="img/Lecture8_355.png"></p>
<p><img data-src="img/Lecture8_356.png"></p>
<p><img data-src="img/Lecture8_357.png"></p>


</section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<p><img src="img/logo.png" class="slide-logo"></p>
<div class="footer footer-default">
<p>Organization</p>
</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1600,

        height: 900,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
            const codeEl = trigger.previousElementSibling.cloneNode(true);
            for (const childEl of codeEl.children) {
              if (isCodeAnnotation(childEl)) {
                childEl.remove();
              }
            }
            return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp("https:\/\/csc477\.github\.io\/website_fall24");
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    

</body></html>