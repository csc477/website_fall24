<!DOCTYPE html>
<html lang="en"><head>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-html/tabby.min.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-e1a5c8363afafaef2c763b6775fbf3ca.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.7.31">

  <meta name="author" content="Florian Shkurti">
  <title>CSC477 - Fall 2024 – CSC477 Introduction to Mobile Robotics</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/theme/quarto-f563837468303362081e247dddd440d0.css">
  <link rel="stylesheet" href="../style.css">
  <link href="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
<meta property="og:title" content="CSC477 Introduction to Mobile Robotics – CSC477 - Fall 2024">
<meta property="og:description" content="Weeks #11 &amp; #12: Multiview Geometry, Visual Odometry, Visual SLAM">
<meta property="og:site_name" content="CSC477 - Fall 2024">
<meta name="twitter:title" content="CSC477 Introduction to Mobile Robotics – CSC477 - Fall 2024">
<meta name="twitter:description" content="Weeks #11 &amp; #12: Multiview Geometry, Visual Odometry, Visual SLAM">
<meta name="twitter:card" content="summary">
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">CSC477 Introduction to Mobile Robotics</h1>
  <p class="subtitle">Weeks #11 &amp; #12: Multiview Geometry, Visual Odometry, Visual SLAM</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Florian Shkurti 
</div>
</div>
</div>

</section>
<section id="motivation" class="slide level2">
<h2>Motivation</h2>
<ul>
<li>We have already seen quite successful SLAM methods based on laser sensors. Why bother with vision?
<ul>
<li>Camera technology cheap and ubiquitous</li>
<li>Camera is a passive sensor, lower energy</li>
<li>Some environments/platforms can’t support laser</li>
<li>Vision is quite a “rich” source of information</li>
</ul></li>
</ul>
</section>
<section id="how-hard-is-computer-vision" class="slide level2">
<h2>How hard is computer vision?</h2>
<div class="columns">
<div class="column" style="width:30%;">
<p><img data-src="img/marvin-minsky.png"></p>
<p>Marvin Minsky, MIT<br>
Turing award, 1969</p>
</div><div class="column" style="width:70%;">
<p>“In 1966, Minsky hired a first-year undergraduate (JS) student and assigned him a problem to solve over the summer: connect a television camera to a computer and get the machine to describe what it sees.”</p>
<p><span class="small-font">Crevier 1993, pg. 88</span></p>
</div></div>
</section>
<section id="section" class="slide level2">
<h2></h2>

<img data-src="img/Lecture11_3.png" class="r-stretch"></section>
<section id="section-1" class="slide level2">
<h2></h2>
<p>Depth perception can be ambiguous from just a single image</p>

<img data-src="img/Lecture11_4.png" class="r-stretch"></section>
<section id="what-do-humans-see" class="slide level2">
<h2>What do humans see?</h2>

<img data-src="img/Lecture11_5.png" class="r-stretch"></section>
<section id="section-2" class="slide level2">
<h2></h2>

<img data-src="img/Lecture11_6.png" class="r-stretch"><p>Count the black dots! :0)</p>
</section>
<section id="peripheral-drift-illusion" class="slide level2">
<h2>Peripheral drift illusion</h2>

<img data-src="img/Lecture11_7.png" class="r-stretch"></section>
<section id="where-do-humans-fixate" class="slide level2">
<h2>Where do humans fixate?</h2>

<img data-src="img/Lecture11_8.png" class="r-stretch"><p><span class="absolute" style="bottom: 50%; right: 0px; ">Visual saccades</span></p>
</section>
<section id="camera-obscura-dark-room" class="slide level2">
<h2>Camera obscura: dark room</h2>
<ul>
<li>Known during classical period in China and Greece (e.g., Mo-Ti, China, 470BC to 390BC)</li>
</ul>

<img data-src="img/Lecture11_9.png" class="r-stretch"></section>
<section id="section-3" class="slide level2">
<h2></h2>
<div class="columns">
<div class="column">
<p><img data-src="img/Lecture11_10.png"></p>
<p>Lens Based Camera Obscura, 1568</p>
</div><div class="column">
<p>Oldest surviving photograph<br>
- Took 8 hours on pewter plate</p>
<p><img data-src="img/Lecture11_11.png"></p>
<p>Joseph Niece, 1826</p>
</div></div>
</section>
<section id="lenses" class="slide level2">
<h2>Lenses</h2>

<img data-src="img/Lecture11_12.png" class="r-stretch"><p>A lens focuses parallel rays onto a single focal point</p>
<ul>
<li>focal point at a distance f beyond the plane of the lens
<ul>
<li>f is a function of the shape and index of refraction of the lens</li>
</ul></li>
<li>Aperture of diameter D restricts the range of rays
<ul>
<li>aperture may be on either side of the lens</li>
</ul></li>
<li>Lenses are typically spherical (easier to produce)</li>
</ul>
</section>
<section id="depth-of-field" class="slide level2">
<h2>Depth of field</h2>

<img data-src="img/Lecture11_13.png" class="r-stretch"><p>Changing the aperture size affects depth of field</p>
<ul>
<li>A smaller aperture increases the range in which the object is approximately in focus</li>
</ul>
<p><span class="small-font">Flower images from Wikipedia <a href="http://en.wikipedia.org/wiki/Depth_of_field" class="uri">http://en.wikipedia.org/wiki/Depth_of_field</a></span></p>
</section>
<section id="section-4" class="slide level2">
<h2></h2>
<div class="columns">
<div class="column">
<p><span class="blue small-font absolute" style="top: 0px; left: 350px; ">To avoid thinking about<br>
image inversion</span></p>
<p><img data-src="img/pinhole-camera.png"></p>
<p>Point aperture <span class="math inline">\(\rightarrow\)</span> nearly every pixel in the</p>
<p>image is in focus</p>
</div><div class="column">

</div></div>
</section>
<section id="section-5" class="slide level2">
<h2></h2>
<div class="columns">
<div class="column">
<p><span class="blue small-font absolute" style="top: 0px; left: 350px; ">To avoid thinking about<br>
image inversion</span></p>
<p><img data-src="img/pinhole-camera.png"></p>
<p>Point aperture <span class="math inline">\(\rightarrow\)</span> nearly every pixel in the</p>
<p>image is in focus <span class="math inline">\(\rightarrow\)</span> almost infinite depth of field</p>
</div><div class="column">
<p><img data-src="img/Lecture11_16.png"></p>
<p>Aperture of nonzero diameter <span class="math inline">\(\rightarrow\)</span> only pixels corresponding to objects on the focal plane are in focus <span class="math inline">\(\rightarrow\)</span> narrow depth of field</p>
</div></div>
<div class="absolute small-font" style="top: 10px; right: 0px; ">
<p>Some times called the<br>
thin-lens model</p>
</div>
</section>
<section id="shrinking-the-aperture" class="slide level2">
<h2>Shrinking the aperture</h2>

<img data-src="img/Lecture11_17.png" class="r-stretch"><p>Why not make the aperture as small as possible?</p>
<ul>
<li>Less light gets through</li>
<li><em>Diffraction</em> effects…</li>
</ul>
</section>
<section id="projective-geometry" class="slide level2">
<h2>Projective Geometry</h2>
<p>Length (and so area) is lost.</p>

<img data-src="img/Lecture11_18.png" class="r-stretch"></section>
<section id="section-6" class="slide level2">
<h2></h2>
<p>Length and area are not preserved</p>

<img data-src="img/Lecture11_19.png" class="r-stretch"></section>
<section id="projective-geometry-1" class="slide level2">
<h2>Projective Geometry</h2>
<p>Angles are lost.</p>

<img data-src="img/Lecture11_20.png" class="r-stretch"></section>
<section id="projective-geometry-2" class="slide level2">
<h2>Projective Geometry</h2>
<p>What is preserved?</p>
<ul>
<li>Straight lines are still straight.</li>
</ul>

<img data-src="img/Lecture11_21.png" class="r-stretch"></section>
<section id="chromatic-aberration" class="slide level2">
<h2>Chromatic aberration</h2>
<p>Failure of a lens to focus all colors to the same convergence point.</p>
<p>Due to difference wavelengths having different refractive indeces</p>

<img data-src="img/Lecture11_22.png" class="r-stretch"></section>
<section id="field-of-view-zoom-focal-length" class="slide level2">
<h2>Field of View (Zoom, focal length)</h2>

<img data-src="img/field-of-view.png" class="r-stretch"></section>
<section id="camera-parameters" class="slide level2">
<h2>Camera parameters</h2>
<p>Focus - Shifts the depth that is in focus.</p>
<p>Focal length - Adjusts the zoom, i.e., wide angle or telephoto lens.</p>
<p>Aperture - Adjusts the depth of field and amount of light let into the sensor.</p>
<p>Exposure time - How long an image is exposed. The longer an image is exposed the more light, but could result in motion blur.</p>
<p>ISO - Adjusts the sensitivity of the “film”. Basically a gain function for digital cameras. Increasing ISO also increases noise.</p>
</section>
<section id="how-do-we-project-3d-points-to-pixels-what-is-the-measurement-model" class="slide level2 center">
<h2>How do we project 3D points to pixels? What is the measurement model?</h2>
</section>
<section id="from-3d-points-to-pixels-pinhole-camera" class="slide level2">
<h2>From 3D points to pixels: pinhole camera</h2>
<div class="columns">
<div class="column" style="width:45%;">
<p><img data-src="img/3dpoints-pixels-camera.png"></p>
</div><div class="column" style="width:55%;">
<p>(1) Perspective projection <span class="math inline">\(\begin{bmatrix} x \\ y \end{bmatrix} = \pi(X, Y, Z)\)</span></p>
<p>(2) Conversion from metric to pixel coordinates</p>
<p><span class="math display">\[\begin{align}
&amp; u = m_x x + c_x \\
&amp; v = m_y y + c_y\end{align}\]</span></p>
<p><span class="math inline">\(m_x, m_y\)</span> represent number of pixels per mm for the two axes</p>
</div></div>
</section>
<section id="perspective-projection-x-y-pix-y-z" class="slide level2">
<h2><span class="medium-font">Perspective projection [x, y] = <span class="math inline">\(\pi\)</span>(X, Y, Z)</span></h2>
<div class="columns">
<div class="column">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/Lecture11_29.png"></p>
<figcaption><a href="http://www.cim.mcgill.ca/%7Elanger/558.html" class="uri">http://www.cim.mcgill.ca/%7Elanger/558.html</a></figcaption>
</figure>
</div>
</div><div class="column">
<p>By similar triangles: x/f = X/Z</p>
<p>So, x = f * X/Z and similarly y = f * Y/Z</p>
<p>Problem: we just lost depth (Z) information by doing this projection, i.e. depth is now uncertain.</p>
</div></div>
</section>
<section id="from-3d-points-to-pixels-pinhole-camera-1" class="slide level2">
<h2>From 3D points to pixels: pinhole camera</h2>
<div class="columns">
<div class="column" style="width:45%;">
<p><img data-src="img/3dpoints-pixels-camera.png"></p>
</div><div class="column medium-font" style="width:55%;">
<p>(1) Perspective projection <span class="math inline">\(\begin{bmatrix} x \\ y \end{bmatrix} = \begin{bmatrix} fX/Z \\ fY/Z \end{bmatrix} = \pi(X, Y, Z)\)</span></p>
<p>(2) Conversion from metric to pixel coordinates</p>
<p><span class="math display">\[\begin{align}
&amp; u = m_x x + c_x \\
&amp; v = m_y y + c_y\end{align}\]</span></p>
<p><span class="math inline">\(h_{\text{pinhole}}(X,Y,Z) = \begin{bmatrix} \frac{fm_x X}{Z} + c_x \\ \frac{fm_y Y}{Z} + c_y \end{bmatrix} + \text{noise in pixels}\)</span></p>
</div></div>
</section>
<section id="from-3d-points-to-pixels-pinhole-camera-2" class="slide level2">
<h2>From 3D points to pixels: pinhole camera</h2>
<div class="columns">
<div class="column" style="width:45%;">
<p><img data-src="img/3dpoints-pixels-camera.png"></p>
</div><div class="column medium-font" style="width:55%;">
<p>(1) Perspective projection <span class="math inline">\(\begin{bmatrix} x \\ y \end{bmatrix} = \begin{bmatrix} fX/Z \\ fY/Z \end{bmatrix} = \pi(X, Y, Z)\)</span></p>
<p>(2) Conversion from metric to pixel coordinates</p>
<p><span class="math display">\[\begin{align}
&amp; u = m_x x + c_x \\
&amp; v = m_y y + c_y\end{align}\]</span></p>
<p><br></p>
<p>Usually presented as <span class="math inline">\(s \begin{bmatrix} u \\ v \\ 1 \end{bmatrix} = \begin{bmatrix} fm_x &amp; 0 &amp; c_x \\ 0 &amp; fm_y &amp; c_y \\ 0 &amp; 0 &amp; 1 \end{bmatrix} \begin{bmatrix} X \\ Y \\ Z \end{bmatrix}\)</span></p>
</div></div>
<div class="red semi-tiny-font absolute" style="bottom: 200px; right: 300px; ">
<p>Unknown depth/scale<br>
<span class="math inline">\(\qquad\qquad\)</span> <img data-src="img/small-arrow.png" height="50"></p>
</div>
<p><span class="red semi-tiny-font absolute" style="bottom: 270px; right: 100px; ">Camera calibration matrix</span></p>
</section>
<section id="from-3d-points-to-pixels-thin-lens-camera" class="slide level2">
<h2>From 3D points to pixels: thin lens camera</h2>
<div class="columns">
<div class="column">
<p><img data-src="img/Lecture11_38.png"></p>
</div><div class="column medium-font">
<p>(1) Perspective projection <span class="math inline">\(\begin{bmatrix} x \\ y \end{bmatrix} = \pi(X, Y, Z)\)</span></p>
<p>(2) Lens distortion <span class="math display">\[[x^*, y^*] = D(x,y)\]</span></p>
<p>(3) Conversion from metric to pixel coordinates</p>
<p><span class="math display">\[\begin{align}
&amp; u = m_x x^* + c_x \\
&amp; v = m_y y^* + c_y\end{align}\]</span></p>
</div></div>
</section>
<section id="lens-distortion-x-y-dxy" class="slide level2">
<h2>(2) Lens distortion [<span class="math inline">\(x^*, y^*\)</span>] = D(x,y)</h2>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img data-src="img/Lecture11_42.jpg"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img data-src="img/Lecture11_43.jpg"></p>
</div>
</div>
</div>
</section>
<section id="estimating-parameters-of-lens-distortion-x-y-dxy" class="slide level2">
<h2>(2) Estimating parameters of lens distortion: <span class="math inline">\([x^*, y^*]\)</span> = D(x,y)</h2>
<div class="quarto-layout-panel" data-layout="[48, 4 ,48]">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 48.0%;justify-content: center;">
<p><img data-src="img/Lecture11_44.jpg"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 4.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-left">
<figure>
<p><img data-src="img/orange-arrow.png" class="quarto-figure quarto-figure-left"></p>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 48.0%;justify-content: center;">
<p><img data-src="img/Lecture11_45.jpg"></p>
</div>
</div>
</div>
<p><span class="math display">\[\begin{align*}
x^* &amp;= x \frac{1 + k_1 r^2 + k_2 r^4 + k_3 r^6}{1 + k_4 r^2 + k_5 r^4 + k_6 r^6} \quad \text{where} \quad r = x^2 + y^2 \\
y^* &amp;= y \frac{1 + k_1 r^2 + k_2 r^4 + k_3 r^6}{1 + k_4 r^2 + k_5 r^4 + k_6 r^6} \quad \text{where} \quad r = x^2 + y^2
\end{align*}\]</span></p>
</section>
<section id="correcting-radial-distortion" class="slide level2">
<h2>Correcting radial distortion</h2>

<img data-src="img/Lecture11_48.png" class="r-stretch"></section>
<section id="from-3d-points-to-pixels-thin-lens-camera-1" class="slide level2">
<h2>From 3D points to pixels: thin lens camera</h2>
<div class="columns">
<div class="column red medium-font">
<p><img data-src="img/Lecture11_38.png"></p>
<p><br></p>
<p>If we have access to camera calibration parameters we can undo the lens distortion, and treat the measurement model as in the pinhole camera <span class="math inline">\(\rightarrow\)</span> single-camera image rectification</p>
</div><div class="column medium-font">
<p>(1) Perspective projection <span class="math inline">\(\begin{bmatrix} x \\ y \end{bmatrix} = \pi(X, Y, Z)\)</span></p>
<p>(2) Lens distortion <span class="math display">\[[x^*, y^*] = D(x,y)\]</span></p>
<p>(3) Conversion from metric to pixel coordinates</p>
<p><span class="math display">\[\begin{align}
&amp; u = m_x x^* + c_x \\
&amp; v = m_y y^* + c_y\end{align}\]</span></p>
</div></div>
</section>
<section id="what-visual-or-physiological-cues-help-us-to-perceive-3d-shape-and-depth" class="slide level2 center">
<h2>What visual or physiological cues help us to perceive 3D shape and depth?</h2>
</section>
<section id="focusdefocus" class="slide level2">
<h2>Focus/defocus</h2>
<div class="columns">
<div class="column" style="width:70%;">
<p><img data-src="img/Lecture11_54.png" height="550"></p>
</div><div class="column" style="width:30%;">
<p><br></p>
<p>Images from same point of view, different camera parameters</p>
<p><br><br><br><br></p>
<p>3d shape / depth estimates</p>
<p><br><br><br></p>
<p><span class="semi-tiny-font">[figs from H. Jin and P. Favaro, 2002]</span></p>
</div></div>
</section>
<section id="section-7" class="slide level2">
<h2></h2>

<img data-src="img/Lecture11_55.jpg" class="r-stretch"></section>
<section id="perspective-effects" class="slide level2">
<h2>Perspective effects</h2>

<img data-src="img/Lecture11_56.png" class="r-stretch"><p><span class="small-font">Image credit: S. Seitz</span></p>
</section>
<section id="stereo" class="slide level2">
<h2>Stereo</h2>

<img data-src="img/Lecture11_57.png" class="r-stretch"><p><span class="small-font">Slides: James Hays and Kristen Grauman</span></p>
</section>
<section id="why-multiple-views" class="slide level2">
<h2>Why multiple views?</h2>
<p>Structure and depth can be ambiguous from single views…</p>

<img data-src="img/Lecture11_58.png" class="r-stretch"><p><span class="small-font">Images from Lana Lazebnik</span></p>
</section>
<section id="section-8" class="slide level2">
<h2></h2>

<img data-src="img/Lecture11_59.png" class="r-stretch"><p>If stereo were critical for depth perception, navigation, recognition, etc., then rabbits would never have evolved.</p>
</section>
<section id="shape-from-shading" class="slide level2">
<h2>Shape from shading</h2>

<img data-src="img/Lecture11_60.png" class="quarto-figure quarto-figure-center r-stretch"><p><span class="small-font">“Numerical schemes for advanced reflectance models for Shape from Shading”, Vogel, Cristiani</span></p>
</section>
<section id="texture" class="slide level2">
<h2>Texture</h2>

<img data-src="img/texture.png" class="quarto-figure quarto-figure-center r-stretch"><p><span class="small-font">[From A.M. Loh. The recovery of 3-D structure using visual texture patterns. PhD thesis]</span></p>
</section>
<section id="occlusion" class="slide level2">
<h2>Occlusion</h2>

<img data-src="img/Lecture11_62.png" class="quarto-figure quarto-figure-center r-stretch"><p><span class="small-font">Rene Magritt’e famous painting Le Blanc-Seing (literal translation: “The Blank Signature”) roughly translates as “free hand” or “free rein”.</span></p>
</section>
<section id="human-stereopsis" class="slide level2">
<h2>Human stereopsis</h2>

<img data-src="img/human-stereopsis.png" class="quarto-figure quarto-figure-center r-stretch"><p>Human eyes <strong>fixate</strong> on point in space - rotate so that corresponding images form in centers of fovea.</p>
</section>
<section id="structure-from-motion" class="slide level2">
<h2>Structure from Motion</h2>

<img data-src="img/Lecture11_64.jpg" class="r-stretch"><p><span class="small-font">“SFMedu: A Structure from Motion System for Education”, Jianxiong Xiao</span></p>
</section>
<section id="section-9" class="slide level2 center">
<h2></h2>
<p>Many depth from X methods. We are going to focus on<br>
structure from motion and stereo <span class="math inline">\(\rightarrow\)</span> part of multiple-view geometry</p>
</section>
<section id="section-10" class="slide level2">
<h2></h2>
<ul>
<li><strong>Visual SLAM</strong>
<ul>
<li>Localization and mapping with measurements usually coming from tracking image features:
<ul>
<li>keypoints/corners</li>
<li>edges</li>
<li>image intensity patches</li>
</ul></li>
<li>Can use one or more cameras</li>
</ul></li>
<li><strong>Visual Odometry</strong>
<ul>
<li>Real-time localization with measurements usually coming from tracking image features:
<ul>
<li>keypoints/corners</li>
<li>edges</li>
<li>image intensity patches</li>
</ul></li>
<li>Can use one or more cameras</li>
</ul></li>
</ul>
</section>
<section id="multi-view-geometry-problems" class="slide level2">
<h2>Multi-view geometry problems</h2>
<div class="columns">
<div class="column semi-tiny-font" style="width:15%;">
<div class="absolute red" style="top: 65px; left: 20px; ">
<p>A.k.a. mapping <img data-src="img/right-arrow-red.png" width="50"></p>
</div>
<p><br><br><br><br><br></p>
<p><span style="color:#ff0000">3D point coordinates</span> <span style="color:#ff0000">are unknown and to </span> <span style="color:#ff0000">be estimated</span></p>
<p><br><br><br><br><br><br><br><br></p>
<p><span style="color:#ff0000">Camera frame </span> <span style="color:#ff0000">transformations</span> <span style="color:#ff0000">are known</span></p>
</div><div class="column" style="width:85%;">
<ul>
<li><strong>Structure</strong>: Given projections of the same 3D point in two or more images, compute the 3D coordinates of that point</li>
</ul>
<p><img data-src="img/Lecture11_65.png"></p>
</div></div>
</section>
<section id="multi-view-geometry-problems-1" class="slide level2">
<h2>Multi-view geometry problems</h2>
<div class="columns">
<div class="column semi-tiny-font" style="width:15%;">
<p><br><br><br><br><br></p>
<p><span style="color:#ff0000">3D point coordinates </span> <span style="color:#ff0000">are unknown, but we </span> <span style="color:#ff0000">won’t try to estimate them</span></p>
<p><br><br><br><br><br><br><br><br></p>
<p><span style="color:#ff0000">Camera frame </span> <span style="color:#ff0000">transformations</span> <span style="color:#ff0000">are unknown and to </span> <span style="color:#ff0000">be estimated</span></p>
</div><div class="column" style="width:85%;">
<ul>
<li><strong>Motion</strong>: Given a set of corresponding points in two or more images, compute the camera parameters</li>
</ul>
<p><img data-src="img/Lecture11_66.png"></p>
</div></div>
</section>
<section id="multi-view-geometry-problems-2" class="slide level2">
<h2>Multi-view geometry problems</h2>
<div class="columns">
<div class="column semi-tiny-font" style="width:15%;">
<p><br><br><br><br><br></p>
<p><span style="color:#ff0000">3D point coordinates </span> <span style="color:#ff0000">are unknown, but we </span> <span style="color:#ff0000">won’t try to estimate them</span></p>
<p><br><br><br><br><br><br><br><br></p>
<p><span style="color:#ff0000">Camera frame </span> <span style="color:#ff0000">transformations</span> <span style="color:#ff0000">are unknown, but we</span> <span style="color:#ff0000">won’t try to estimate them </span></p>
</div><div class="column" style="width:85%;">
<ul>
<li><strong>Optical flow</strong>: Given two images, find the location of a world point in a second close-by image with no camera info.</li>
</ul>
<p><img data-src="img/Lecture11_67.png"></p>
</div></div>
<div class="absolute semi-tiny-font" style="bottom: 200px; right: 100px; ">
<p><span style="color:#ff0000">We are estimating pixel displacement</span><br>
<span style="color:#ff0000">from one image to the next</span></p>
</div>
</section>
<section id="multi-view-geometry-problems-3" class="slide level2">
<h2>Multi-view geometry problems</h2>
<ul>
<li><strong>Stereo correspondence</strong>: Given a point in one of the images, where could its corresponding points be in the other images?</li>
</ul>

<img data-src="img/Lecture11_68.png" class="r-stretch"></section>
<section id="section-11" class="slide level2 center">
<h2></h2>
<p>Basic underlying component in many of these problems:</p>
<p>keypoint detection and</p>
<p>matching across images</p>
</section>
<section id="local-features-main-components" class="slide level2">
<h2>Local features: main components</h2>
<div class="columns">
<div class="column">
<p>1) Detection:</p>
<p>Find a set of distinctive key points.</p>
<p><br></p>
<p>2) Description:</p>
<p>Extract feature descriptor around each interest point as vector.</p>
<p><img data-src="img/local-component-descrip.png"></p>
<p>3) Matching:</p>
<p>Compute distance between feature vectors to find correspondence. <span class="math display">\[d(x_1, x_2) &lt; T\]</span></p>
</div><div class="column">
<p><img data-src="img/Lecture11_69.png" height="550"></p>
</div></div>
</section>
<section id="local-features-main-components-1" class="slide level2">
<h2>Local features: main components</h2>
<div class="columns">
<div class="column tiny-font red" style="width:10%;">
<div class="absolute" style="top: 250px; left: -80px; ">
<p>Ideally, we want the descriptor<br>
to be invariant (i.e. little to<br>
no change) when there are<br>
</p>
<p>- viewpoint changes<br>
(small rotation or translation<br>
of the camera)</p>
<p>- scale-changes</p>
<p>- illumination changes</p>
</div>
</div><div class="column" style="width:45%;">
<p>1) Detection:</p>
<p>Find a set of distinctive key points.</p>
<p><br></p>
<p>2) Description:</p>
<p>Extract feature descriptor around each interest point as vector.</p>
<p><img data-src="img/local-component-descrip.png"></p>
<p>3) Matching:</p>
<p>Compute distance between feature vectors to find correspondence. <span class="math display">\[d(x_1, x_2) &lt; T\]</span></p>
</div><div class="column" style="width:45%;">
<p><img data-src="img/Lecture11_69.png" height="550"></p>
</div></div>
</section>
<section id="local-measures-of-uniqueness" class="slide level2">
<h2>Local measures of uniqueness</h2>
<p>Suppose we only consider a small window of pixels</p>
<ul>
<li>What defines whether a feature is a good or bad candidate?</li>
</ul>

<img data-src="img/Lecture11_71.png" class="r-stretch"><p><span class="small-font">Slide adapted from Darya Frolova, Denis Simakov, Weizmann Institute.</span></p>
</section>
<section id="feature-detection" class="slide level2">
<h2>Feature detection</h2>
<p>Local measure of feature uniqueness</p>
<ul>
<li>How does the window change when you shift by a small amount?</li>
</ul>

<img data-src="img/Lecture11_72.png" class="r-stretch"><div class="quarto-layout-panel" data-layout-ncol="3">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<p><span class="blue">“flat”</span> region:<br>
no change in all directions</p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<p><span class="blue">“edge”:</span><br>
no change along the edge direction</p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<p><span class="blue">“corner”:</span><br>
significant change in all directions</p>
</div>
</div>
</div>
<p><span class="small-font">Slide adapted from Darya Frolova, Denis Simakov, Weizmann Institute.</span></p>
</section>
<section id="corner-detectors" class="slide level2">
<h2>Corner detectors</h2>
<ul>
<li><p>Harris</p></li>
<li><p>FAST</p></li>
<li><p>Laplacian of Gaussian detector</p></li>
<li><p>SUSAN</p></li>
<li><p>Forstner</p></li>
</ul>
</section>
<section id="superimposed-harris-keypoints" class="slide level2">
<h2>Superimposed Harris keypoints</h2>

<img data-src="img/Lecture11_73.png" class="r-stretch"><p><span class="absolute medium-font" style="top: 200px; right: 0px; ">500 strongest<br>
keypoints</span></p>
</section>
<section id="scale-space-representation" class="slide level2">
<h2>Scale-space representation</h2>

<img data-src="img/Lecture11_74.png" class="quarto-figure quarto-figure-center r-stretch"><div class="small-font red absolute" style="left: 0px; bottom: 50px; ">
<p>Feature detection:</p>
<p>search for “corners”/keypoints<br>
across many scales, and return<br>
a list of (x, y, scale) keypoints</p>
</div>
</section>
<section id="characteristics-of-good-features" class="slide level2">
<h2>Characteristics of good features</h2>

<img data-src="img/Lecture11_75.png" class="quarto-figure quarto-figure-center r-stretch"><div class="medium-font">
<ul>
<li>Repeatability
<ul>
<li>The same feature can be found in several images despite geometric and photometric transformations</li>
</ul></li>
<li>Saliency
<ul>
<li>Each feature is distinctive</li>
</ul></li>
<li>Compactness and efficiency
<ul>
<li>Many fewer features than image pixels</li>
</ul></li>
<li>Locality
<ul>
<li>A feature occupies a relatively small area of the image; robust to clutter and occlusion</li>
</ul></li>
</ul>
</div>
<p><span class="tiny-font absolute" style="left: 0px; bottom: 0px; ">Kristen Grauman</span></p>
</section>
<section id="sift-descriptor-formation" class="slide level2">
<h2>SIFT descriptor formation</h2>
<ul>
<li>Compute on local 16 x 16 window around detection.</li>
<li>Rotate and scale window according to discovered orientation <span class="math inline">\(\theta\)</span> and scale <span class="math inline">\(\sigma\)</span> (gain invariance).</li>
<li>Compute gradients weighted by a Gaussian of variance half the window (for smooth falloff).</li>
</ul>

<img data-src="img/Lecture11_76.png" class="r-stretch"><p><span class="math inline">\(\qquad\qquad\qquad\qquad\qquad\)</span><span class="small-font">Actually 16x16, only showing 8x8</span></p>
</section>
<section id="sift-vector-formation" class="slide level2">
<h2>SIFT vector formation</h2>
<ul>
<li>4x4 array of gradient orientation histograms weighted by gradient magnitude.</li>
<li>Bin into 8 orientations x 4x4 array = 128 dimensions.</li>
</ul>

<img data-src="img/Lecture11_77.png" class="r-stretch"></section>
<section id="reduce-effect-of-illumination" class="slide level2">
<h2>Reduce effect of illumination</h2>
<ul>
<li>128-dim vector normalized to 1</li>
<li>Threshold gradient magnitudes to avoid excessive influence of high gradients
<ul>
<li>After normalization, clamp gradients &gt; 0.2</li>
<li>Renormalize</li>
</ul></li>
</ul>

<img data-src="img/Lecture11_78.png" class="r-stretch"></section>
<section id="local-descriptors-surf" class="slide level2">
<h2>Local Descriptors: SURF</h2>
<div class="columns">
<div class="column" style="width:45%;">
<p><img data-src="img/Lecture11_79.png"></p>
<p><br><br><br></p>
<p>[Bay, ECCV’06], [Cornelis, CVGPU’08]</p>
</div><div class="column" style="width:55%;">
<p><strong>Fast approximation of SIFT idea</strong></p>
<p>Efficient computation by 2D box filters &amp; integral images</p>
<p>→ 6 times faster than SIFT</p>
<p>Equivalent quality for object identification</p>
<p><br><br></p>
<p><strong>GPU implementation available</strong></p>
<p>Feature extraction @ 200Hz<br>
(detector + descriptor, 640x480 img)</p>
<p><a href="http://www.vision.ee.ethz.ch/~surf" class="uri">http://www.vision.ee.ethz.ch/~surf</a></p>
</div></div>
<div class="right-align">
<p><span class="small-font">K. Grauman, B. Leibe</span></p>
</div>
</section>
<section id="many-other-local-descriptors" class="slide level2">
<h2>Many other local descriptors</h2>
<ul>
<li><p>ORB</p></li>
<li><p>BRIEF</p></li>
<li><p>FREAK</p></li>
<li><p>RootSIFT-PCA</p></li>
</ul>
</section>
<section id="feature-matching" class="slide level2">
<h2>Feature matching</h2>

<img data-src="img/Lecture11_80.png" class="r-stretch"></section>
<section id="overview-of-keypoint-matching" class="slide level2">
<h2>Overview of Keypoint Matching</h2>
<div class="columns">
<div class="column" style="width:70%;">
<p><img data-src="img/Lecture11_81.png"></p>
</div><div class="column" style="width:30%;">
<ol type="1">
<li>Find a set of distinctive key-points</li>
<li>Define a region around each keypoint</li>
<li>Extract and normalize the region content</li>
<li>Compute a local descriptor from the normalized region</li>
<li>Match local descriptors</li>
</ol>
</div></div>
</section>
<section id="problem-1-landmark-triangulation" class="slide level2 center">
<h2>Problem #1: landmark triangulation</h2>
</section>
<section id="multi-view-geometry-problems-4" class="slide level2">
<h2>Multi-view geometry problems</h2>
<div class="columns">
<div class="column small-font" style="width:15%;">
<div class="absolute red" style="top: 55px; left: -15px; ">
<p>A.k.a. mapping <img data-src="img/right-arrow-red.png" width="50"></p>
</div>
<p><br><br><br></p>
<p><span style="color:#ff0000">3D point coordinates</span> <span style="color:#ff0000">are unknown and to </span> <span style="color:#ff0000">be estimated</span></p>
<p><br><br><br><br><br><br><br><br></p>
<p><span style="color:#ff0000">Camera frame </span> <span style="color:#ff0000">transformations</span> <span style="color:#ff0000">are known</span></p>
</div><div class="column" style="width:85%;">
<ul>
<li><strong>Structure</strong>: Given projections of the same 3D point in two or more images, compute the 3D coordinates of that point</li>
</ul>
<p><img data-src="img/Lecture11_65.png"></p>
</div></div>
</section>
<section id="stereo-1" class="slide level2">
<h2>Stereo</h2>
<p><strong>Epipolar geometry</strong></p>
<ul>
<li>Case with two cameras with parallel optical axes <span class="red"><span class="math inline">\(\qquad \leftarrow\)</span> First this</span></li>
<li>General case</li>
</ul>

<img data-src="img/Lecture11_83.png" class="r-stretch"></section>
<section id="stereo-parallel-calibrated-cameras" class="slide level2">
<h2>Stereo: Parallel Calibrated Cameras</h2>
<ul>
<li>We assume that the two calibrated cameras (we know intrinsics and extrinsics) are parallel, i.e.&nbsp;the right camera is just some distance to the right of left camera. We assume we know this distance. We call it the <strong>baseline</strong>.</li>
</ul>

<img data-src="img/Lecture11_84.png" class="r-stretch"><div class="absolute small-font" style="bottom: 40px; right: 300px; ">
<p>The right camera<br>
is shifted to the<br>
right in X direction</p>
</div>
</section>
<section id="stereo-parallel-calibrated-cameras-1" class="slide level2">
<h2>Stereo: Parallel Calibrated Cameras</h2>
<ul>
<li>Points <span class="math inline">\(\mathbf{O_l, O_r}\)</span> and <strong>P</strong> (and <span class="math inline">\(\mathbf{p_l}\)</span> and <span class="math inline">\(\mathbf{p_r}\)</span>) lie on a plane. Since two image planes lie on the same plane (distance f from each camera), the lines <span class="math inline">\(\mathbf{O_l O_r}\)</span>, and <span class="math inline">\(\mathbf{p_l p_r}\)</span>, are parallel.</li>
</ul>

<img data-src="img/Lecture11_85.png" class="r-stretch"></section>
<section id="stereo-parallel-calibrated-cameras-2" class="slide level2">
<h2>Stereo: Parallel Calibrated Cameras</h2>
<ul>
<li>Since lines <span class="math inline">\(\mathbf{O_l O_r}\)</span> and <span class="math inline">\(\mathbf{p_l p_r}\)</span>, are parallel, and <span class="math inline">\(\mathbf{O_l}\)</span> and <span class="math inline">\(\mathbf{O_r}\)</span> have the same y, then also <span class="math inline">\(\mathbf{p_l}\)</span> and <span class="math inline">\(\mathbf{p_r}\)</span> have the same <span class="math inline">\(y: y_r = y_i\)</span>!</li>
</ul>

<img data-src="img/Lecture11_86.png" class="r-stretch"></section>
<section id="stereo-parallel-calibrated-cameras-3" class="slide level2">
<h2>Stereo: Parallel Calibrated Cameras</h2>
<ul>
<li>Another observation: No point from <span class="math inline">\(\mathbf{O_l p_l}\)</span>, can project to the right of <span class="math inline">\(x_l\)</span>, in the right image. <strong>Why?</strong></li>
</ul>

<img data-src="img/Lecture11_87.png" class="r-stretch"></section>
<section id="stereo-parallel-calibrated-cameras-4" class="slide level2">
<h2>Stereo: Parallel Calibrated Cameras</h2>
<ul>
<li>Because that would mean our image can see behind the camera…</li>
</ul>

<img data-src="img/Lecture11_88.png" class="r-stretch"></section>
<section id="stereo-parallel-calibrated-cameras-5" class="slide level2">
<h2>Stereo: Parallel Calibrated Cameras</h2>
<ul>
<li>We can then use similar triangles to compute the depth of the point P</li>
</ul>
<p><img data-src="img/Lecture11_89.png" class="absolute" style="bottom: 20px; height: 550px; "></p>
<div class="red semi-tiny-font absolute right-align" style="bottom: 80px; right: -10px; ">
<p><img data-src="img/black-arrow.png" width="100"> In metric, not in pixel<br>
coordinates. To convert<br>
to pixel coordinates need<br>
to use elements of the camera<br>
calibration matrix.</p>
</div>
</section>
<section id="section-12" class="slide level2 center">
<h2></h2>
<p>Conclusion: if you have a well-calibrated and rectified (parallel) stereo camera you do not need to do least squares triangulation.</p>
<p>You can estimate depth via the disparity map.</p>
</section>
<section id="stereo-parallel-calibrated-cameras-6" class="slide level2">
<h2>Stereo: Parallel Calibrated Cameras</h2>
<ul>
<li>For each point <span class="math inline">\(p_l = (x_l, y_l)\)</span>, how do I get <span class="math inline">\(p_r = (x_r, y_r)\)</span>? By matching. Patch around <span class="math inline">\((x_r, y_r)\)</span> should look similar to the patch around <span class="math inline">\((x_l, y_l)\)</span>.</li>
</ul>

<img data-src="img/Lecture11_90.png" class="r-stretch"><p>Do this for all the points in the left image!</p>
</section>
<section id="stereo-parallel-calibrated-cameras-7" class="slide level2">
<h2>Stereo: Parallel Calibrated Cameras</h2>
<ul>
<li>We get a disparity map as a result</li>
</ul>

<img data-src="img/Lecture11_91.png" class="r-stretch"><p>Result: <strong>Disparity map</strong> (red values large disp., blue small disp.)</p>
</section>
<section id="stereo-parallel-calibrated-cameras-8" class="slide level2">
<h2>Stereo: Parallel Calibrated Cameras</h2>
<ul>
<li>Smaller patches: more detail, but noisy. Bigger: less detail, but smooth</li>
</ul>

<img data-src="img/Lecture11_92.png" class="r-stretch"><div class="absolute" style="bottom: 100px; right: 250px; ">
<p><br></p>
<p>patch size = 5</p>
<p><br><br><br></p>
<p>patch size = 35</p>
<p><br><br></p>
<p>patch size = 85</p>
</div>
</section>
<section id="you-can-do-it-much-better" class="slide level2">
<h2>You Can Do It Much Better…</h2>
<p><span class="small-font">[K. Yamaguchi, D. McAllester and R. Urtasun, ECCV 2014]</span></p>

<img data-src="img/Lecture11_93.png" class="r-stretch"></section>
<section id="multi-view-geometry-problems-5" class="slide level2">
<h2>Multi-view geometry problems</h2>
<div class="columns">
<div class="column semi-tiny-font red" style="width:15%;">
<p><br><br><br><br></p>
<p>3 x 7 = 21 variables to be estimated</p>
<p><br><br><br><br><br><br><br><br></p>
<p>7 pixel observations in each camera, so 21 pixel observations across all cameras</p>
<p><span class="math inline">\(\rightarrow\)</span> 42 constraints in total</p>
</div><div class="column" style="width:85%;">
<ul>
<li><strong>Structure</strong>: Given projections of the same 3D point in two or more images, compute the 3D coordinates of that point</li>
</ul>
<p><img data-src="img/Lecture11_65.png"></p>
</div></div>
</section>
<section id="triangulation-as-a-least-squares-problem" class="slide level2">
<h2>Triangulation as a least squares problem</h2>
<p><span class="math display">\[^wX^*, ^wY^*, ^wZ^* = \underset{^wp=[^wX, ^wY, ^wZ]}{\operatorname{argmin}} \sum_{k=1}^{K} \left\|\bar{z}^{(k)} - \mathbb{E}\left[h_\text{pinhole}\left(_{w}^kR^w p + ^kt_{kw}\right)\right]\right\|^2\]</span></p>
<div class="red small-font absolute" style="bottom: 250px; right: 430px; ">
<p><span class="math inline">\(\qquad\qquad\qquad\quad\)</span> <img data-src="img/red-uparrow1.png" height="120"><br>
Actual pixel observation of a<br>
keypoint by camera frame <span class="math inline">\(\color{black}k\)</span></p>
</div>
<div class="red small-font absolute" style="bottom: 220px; right: 150px; ">
<p><img data-src="img/red-arrow2.png" height="150"><br>
Expected pixel observation of<br>
3D point <span class="math inline">\(\color{black}^wp\)</span> by camera frame <span class="math inline">\(\color{black}k\)</span></p>
</div>
<div class="absolute" style="bottom: 0px; right: 0px; ">
<p><span class="math inline">\(h_{\text{pinhole}}(X, Y, Z) = \begin{bmatrix} \frac{f m_x X}{Z} + c_x \\ \frac{f m_y Y}{Z} + c_y \end{bmatrix} + n, \quad n \sim \mathcal{N}(0, R) \quad \text{noise (in pixels)}\)</span></p>
</div>
</section>
<section id="triangulation-as-a-least-squares-problem-1" class="slide level2">
<h2>Triangulation as a least squares problem</h2>
<p><span class="math display">\[^wX^*, ^wY^*, ^wZ^* = \underset{^wp=[^wX, ^wY, ^wZ]}{\operatorname{argmin}} \sum_{k=1}^{K} \left\|\bar{z}^{(k)} - \mathbb{E}\left[h_\text{pinhole}\left(_{w}^kR^w \color{red}\boxed{\color{black}p} \color{black}+ ^kt_{kw}\right)\right]\right\|^2\]</span></p>
<div class="red small-font absolute" style="left: 70px; bottom: 250px; ">
<p>Enumerate all cameras that <img data-src="img/left-arrow2.png" height="120"><br>
observed the keypoint.</p>
</div>
<div class="red small-font absolute right-align" style="bottom: 330px; right: -100px; ">
<p><img data-src="img/right-arrow3.png" width="150"> The only term to be<br>
optimized. The rest are known.</p>
</div>
<div class="absolute" style="bottom: 0px; right: 0px; ">
<p><span class="math inline">\(h_{\text{pinhole}}(X, Y, Z) = \begin{bmatrix} \frac{f m_x X}{Z} + c_x \\ \frac{f m_y Y}{Z} + c_y \end{bmatrix} + n, \quad n \sim \mathcal{N}(0, R) \quad \text{noise (in pixels)}\)</span></p>
</div>
</section>
<section id="triangulation-as-a-least-squares-problem-2" class="slide level2">
<h2>Triangulation as a least squares problem</h2>
<p><span class="math display">\[^wX^*, ^wY^*, ^wZ^* = \underset{^wp=[^wX, ^wY, ^wZ]}{\operatorname{argmin}} \sum_{k=1}^{K} \left\|\bar{z}^{(k)} - \mathbb{E}\left[h_\text{pinhole}\left(\color{red}\boxed{\color{black}_{w}^kR^w p + ^kt_{kw}}\color{black}\right)\right]\right\|^2\]</span></p>
<div class="red small-font absolute" style="bottom: 300px; right: 100px; ">
<p><span class="math inline">\(\qquad\qquad \Big\uparrow\)</span><br>
3D point expressed in the frame<br>
of camera k</p>
<p><span class="math inline">\(\color{black}^{k}p = {}^{k}_{w}R^{w}p + {}^{k}t_{kw}\)</span></p>
</div>
<div class="absolute" style="bottom: 0px; right: 0px; ">
<p><span class="math inline">\(h_{\text{pinhole}}(X, Y, Z) = \begin{bmatrix} \frac{f m_x X}{Z} + c_x \\ \frac{f m_y Y}{Z} + c_y \end{bmatrix} + n, \quad n \sim \mathcal{N}(0, R) \quad \text{noise (in pixels)}\)</span></p>
</div>
</section>
<section id="triangulation-as-a-least-squares-problem-3" class="slide level2">
<h2>Triangulation as a least squares problem</h2>
<p><span class="math display">\[^wX^*, ^wY^*, ^wZ^* = \underset{^wp=[^wX, ^wY, ^wZ]}{\operatorname{argmin}} \sum_{k=1}^{K} \color{red}\boxed{\color{black} \left\|\bar{z}^{(k)} - \mathbb{E}\left[h_\text{pinhole}\left(_{w}^kR^w p + ^kt_w\right)\right]\right\|^2 }\]</span></p>
<div class="red small-font absolute" style="bottom: 300px; right: 100px; ">
<p><span class="math inline">\(\qquad\qquad \Big\uparrow\)</span><br>
Reprojection error of point<br>
<span class="math inline">\(\color{black}^{k}p = {}^{k}_{w}R^{w}p + {}^{k}t_{kw}\)</span></p>
<p>into camera k’s frame</p>
</div>
<div class="absolute" style="bottom: 0px; right: 0px; ">
<p><span class="math inline">\(h_{\text{pinhole}}(X, Y, Z) = \begin{bmatrix} \frac{f m_x X}{Z} + c_x \\ \frac{f m_y Y}{Z} + c_y \end{bmatrix} + n, \quad n \sim \mathcal{N}(0, R) \quad \text{noise (in pixels)}\)</span></p>
</div>
</section>
<section id="triangulation-as-a-least-squares-problem-4" class="slide level2">
<h2>Triangulation as a least squares problem</h2>
<p><span class="math inline">\(\begin{align}
{}^w X^*, {}^w Y^*, {}^w Z^* &amp;= \underset{{}^w p = [{}^w X, {}^w Y, {}^w Z]}{\text{argmin}} \sum_{k=1}^{K} \|\tilde{z}^{(k)} - \mathbb{E}[h_{\text{pinhole}}({}^k_w R {}^w p + {}^k t_{kw})] \|^2 \\
&amp;= \underset{{}^w p = [{}^w X, {}^w Y, {}^w Z]}{\text{argmin}} \sum_{k=1, {}^w p \to {}^k p}^{K} \|\tilde{z}^{(k)} - \mathbb{E}[h_{\text{pinhole}}({}^k p)] \|^2 \\
\end{align}\)</span></p>
<div class="absolute" style="bottom: 0px; right: 0px; ">
<p><span class="math inline">\(h_{\text{pinhole}}(X, Y, Z) = \begin{bmatrix} \frac{f m_x X}{Z} + c_x \\ \frac{f m_y Y}{Z} + c_y \end{bmatrix} + n, \quad n \sim \mathcal{N}(0, R) \quad \text{noise (in pixels)}\)</span></p>
</div>
</section>
<section id="triangulation-as-a-least-squares-problem-5" class="slide level2">
<h2>Triangulation as a least squares problem</h2>
<p><span class="math inline">\(\begin{align}
{}^w X^*, {}^w Y^*, {}^w Z^* &amp;= \underset{{}^w p = [{}^w X, {}^w Y, {}^w Z]}{\text{argmin}} \sum_{k=1}^{K} \|\tilde{z}^{(k)} - \mathbb{E}[h_{\text{pinhole}}({}^k_w R {}^w p + {}^k t_{kw})] \|^2 \\
&amp;= \underset{{}^w p = [{}^w X, {}^w Y, {}^w Z]}{\text{argmin}} \sum_{k=1, {}^w p \to {}^k p}^{K} \|\tilde{z}^{(k)} - \mathbb{E}[h_{\text{pinhole}}({}^k p)] \|^2 \\
&amp;= \underset{{}^w p = [{}^w X, {}^w Y, {}^w Z]}{\text{argmin}} \sum_{k=1, {}^w p \to {}^k p}^{K} \left\| \begin{bmatrix} \tilde{u}^{(k)} \\ \tilde{v}^{(k)} \end{bmatrix} - \begin{bmatrix} \frac{f m_x {}^k X}{{}^k Z} + c_x \\ \frac{f m_y {}^k Y}{{}^k Z} + c_y \end{bmatrix} \right\|^2
\end{align}\)</span></p>
<div class="absolute" style="bottom: 0px; right: 0px; ">
<p><span class="math inline">\(h_{\text{pinhole}}(X, Y, Z) = \begin{bmatrix} \frac{f m_x X}{Z} + c_x \\ \frac{f m_y Y}{Z} + c_y \end{bmatrix} + n, \quad n \sim \mathcal{N}(0, R) \quad \text{noise (in pixels)}\)</span></p>
</div>
<div class="fragment">
<div class="red semi-tiny-font absolute" style="left: -50px; bottom: 100px; ">
<p>Note: unconstrained optimization<br>
does not guarantee that the solution<br>
will be in the camera’s field of view.<br>
For example, it could happen that<br>
it returns <span class="math inline">\(\color{black}^kZ &lt; 0\)</span> which is an invalid<br>
solution (i.e.&nbsp;behind the camera)</p>
</div>
</div>
</section>
<section id="potential-pitfalls-with-triangulationnear-parallel-rays" class="slide level2">
<h2>Potential pitfalls with triangulation:near parallel rays</h2>
<div class="quarto-layout-panel" data-layout="[15, 85]">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 15.0%;justify-content: flex-start;">
<p><span class="small-font"><span style="color:#ff0000">“point at infinity”</span></span></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 85.0%;justify-content: center;">
<p><img data-src="img/Lecture11_115.png"></p>
</div>
</div>
</div>
<p>Intersection point is too far away, dominated by noise and insufficient image resolution.</p>
<p>Triangulating these points is typically impossible without sufficient baseline between camera frames.</p>
</section>
<section id="problem-2-camera-localizationvisual-odometry" class="slide level2 center">
<h2>Problem #2: camera localization/visual odometry</h2>
</section>
<section id="multi-view-geometry-problems-6" class="slide level2">
<h2>Multi-view geometry problems</h2>
<div class="columns">
<div class="column small-font" style="width:15%;">
<p><br><br><br></p>
<p><span style="color:#ff0000">3D point coordinates </span> <span style="color:#ff0000">are unknown, but we </span> <span style="color:#ff0000">won’t try to estimate them</span></p>
<p><br><br><br><br><br><br><br><br></p>
<p><span style="color:#ff0000">Camera frame </span> <span style="color:#ff0000">transformations</span> <span style="color:#ff0000">are unknown and to </span> <span style="color:#ff0000">be estimated</span></p>
</div><div class="column" style="width:85%;">
<ul>
<li><strong>Motion</strong>: Given a set of corresponding points in two or more images, compute the camera parameters</li>
</ul>
<p><img data-src="img/Lecture11_66.png"></p>
</div></div>
</section>
<section id="camera-localization-as-a-least-squares-problem" class="slide level2">
<h2>Camera localization as a least squares problem?</h2>
<p><span class="math display">\[^wX^*, ^wY^*, ^wZ^* = \underset{^wp=[^wX, ^wY, ^wZ]}{\operatorname{argmin}} \sum_{k=1}^{K} \left\|\bar{z}^{(k)} - \mathbb{E}\left[h_\text{pinhole}\left(\color{red}\boxed{\color{black}_{w}^kR}\color{black}^w p + \color{red}\boxed{\color{black}^kt_w}\color{black}\right)\right]\right\|^2\]</span></p>
<p><img data-src="img/3arrows.png" class="absolute" style="top: 220px; right: 0px; width: 250px; "></p>
<div class="red semi-tiny-font absolute" style="top: 400px; right: 120px; ">
<p>But, 3D position is unknown!</p>
<p>So, we cannot solve the problem<br>
using the reprojection error<br>
unless we know the 3D position<br>
corresponding to the keypoint.</p>
</div>
<div class="red semi-tiny-font absolute" style="top: 300px; right: -120px; ">
<p>The only terms to be<br>
optimized.</p>
</div>
<div class="absolute" style="bottom: 0px; right: 0px; ">
<p><span class="math inline">\(h_{\text{pinhole}}(X, Y, Z) = \begin{bmatrix} \frac{f m_x X}{Z} + c_x \\ \frac{f m_y Y}{Z} + c_y \end{bmatrix} + n, \quad n \sim \mathcal{N}(0, R) \quad \text{noise (in pixels)}\)</span></p>
</div>
</section>
<section id="working-principle" class="slide level2">
<h2>Working Principle</h2>

<img data-src="img/Lecture11_119.png" class="quarto-figure quarto-figure-center r-stretch"><div class="absolute semi-tiny-font" style="left: 0px; bottom: 40%; ">
<p><span style="color:#ff0000">Let’s restrict the discussion</span><br>
<span style="color:#ff0000">to two cameras only</span></p>
</div>
</section>
<section id="key-idea-epipolar-constraint" class="slide level2">
<h2>Key idea: Epipolar constraint</h2>

<img data-src="img/Lecture11_120.png" class="r-stretch"><div class="quarto-layout-panel" data-layout="[47, -6, 47]">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 47.0%;justify-content: flex-start;">
<p>Potential matches for x’ have to lie on the corresponding line <em>l</em>.</p>
</div>
<div class="quarto-figure-spacer quarto-layout-cell" style="flex-basis: 6.0%;justify-content: flex-start;">
<p>&nbsp;</p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 47.0%;justify-content: flex-start;">
<p>Potential matches for x have to lie on the corresponding line P.</p>
</div>
</div>
</div>
</section>
<section id="epipolar-geometry-notation" class="slide level2">
<h2>Epipolar geometry: notation</h2>

<img data-src="img/Lecture11_121.png" class="r-stretch"><ul>
<li><p><span style="color: #e91e63;"><strong>Baseline</strong> – line connecting the two camera centers</span></p></li>
<li><p><span style="color: #4caf50;"><strong>Epipoles</strong><br>
= intersections of baseline with image planes<br>
= projections of the other camera center </span></p></li>
<li><p><span style="color: #9c27b0;"><strong>Epipolar Plane</strong> – plane containing baseline (1D family) </span></p></li>
<li><p><span style="color: #910019;"><strong>Epipolar Lines</strong> - intersections of epipolar plane with image planes (always come in corresponding pairs) </span></p></li>
</ul>
</section>
<section id="epipolar-constraint-calibrated-case" class="slide level2">
<h2>Epipolar constraint: Calibrated case</h2>

<img data-src="img/Lecture11_122.png" class="r-stretch"><p><span class="math inline">\(\hat{x} \cdot [t \times (R\hat{x}')] = 0\)</span> (because <span class="math inline">\(\hat{x}\)</span>, <span class="math inline">\(R\hat{x}'\)</span>, and <span class="math inline">\(t\)</span> are co-planar)</p>
</section>
<section id="essential-matrix" class="slide level2">
<h2>Essential matrix</h2>

<img data-src="img/Lecture11_123.png" class="quarto-figure quarto-figure-center r-stretch"><div class="columns medium-font">
<div class="column" style="width:5%;">

</div><div class="column" style="width:45%;">
<p>E is a 3x3 matrix which relates corresponding pairs of normalized homogeneous image points across pairs of images - for K calibrated cameras.</p>
<p><em>Estimates relative position/orientation.</em></p>
</div><div class="column center-align" style="width:45%;">
<p><strong>Essential Matrix</strong><br>
(Longuet-Higgins, 1981)</p>
<p><span class="grey small-font">Note: [t], is matrix representation of cross product</span></p>
</div><div class="column" style="width:5%;">

</div></div>
<div class="red semi-tiny-font absolute" style="left: -50px; bottom: 40%; ">
<p>“5-point algorithm” by<br>
David Nister computes<br>
essential matrix and then<br>
decomposes it into rotation<br>
and translation.</p>
</div>
<div class="red semi-tiny-font absolute" style="bottom: 40%; right: -50px; ">
<p>After estimating the<br>
essential matrix, we<br>
extract t, R.</p>
<p>However, the<br>
translation t, is only<br>
estimated up to a<br>
multiplicative scale.</p>
<p><span class="math inline">\(\rightarrow\)</span> Translation is not<br>
fully observable with a<br>
single camera.</p>
<p><span class="math inline">\(\rightarrow\)</span> To make it observable<br>
we need stereo</p>
</div>
</section>
<section id="visual-odometry-with-a-single-camera-translation-is-recovered-only-up-to-a-scale" class="slide level2">
<h2><span class="medium-font">Visual odometry with a single camera: translation is recovered only up to a scale</span></h2>
<ul>
<li>Scale = multiplier between real-world metric distance units and estimated map distance units</li>
</ul>
<div class="red small-font quarto-layout-panel" data-layout="[40, 60]">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 40.0%;justify-content: flex-start;">
<p><br><br><br> Camera placements (1) and (2) generate the same observation of P. In fact, infinitely many possible placements of the two camera frames along their projection rays could have generated the same measurement.</p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 60.0%;justify-content: center;">
<p><img data-src="img/visual-odometry.png"></p>
</div>
</div>
</div>
</section>
<section id="visual-odometry-with-a-single-camera-translation-is-recovered-only-up-to-a-scale-1" class="slide level2">
<h2><span class="medium-font">Visual odometry with a single camera: translation is recovered only up to a scale</span></h2>
<ul>
<li>Scale = multiplier between real-world metric distance units and estimated map distance units</li>
</ul>
<div class="columns">
<div class="column red small-font" style="width:40%;">
<p><br></p>
<p>Q: Is there a way to obtain true metric distances only with a single camera?</p>
<p>A: The only way is to have an object of known metric dimensions in the observed scene. For example if you know distances AB, BC, CA then you can recover true translation. This is commonly referred to as the Perspective-3-Point (P3P), or in General, the Perspective-n-Point (PnP) problem.</p>
</div><div class="column" style="width:10%;">

</div><div class="column" style="width:50%;">
<p><img data-src="img/Lecture11_126.png"></p>
</div></div>
</section>
<section id="visual-odometry-with-a-single-camera-translation-is-recovered-only-up-to-a-scale-2" class="slide level2">
<h2><span class="medium-font">Visual odometry with a single camera: translation is recovered only up to a scale</span></h2>
<ul>
<li>Scale = multiplier between real-world metric distance units and estimated map distance units</li>
</ul>
<div class="columns">
<div class="column red small-font" style="width:40%;">
<p><br></p>
<p>Q: Does scale remain constant throughout the trajectory of a single camera?</p>
<p>A: No, there is scale drift, which is most apparent during in-place rotations (i.e.&nbsp;pure rotation, no translation), because depth estimation for 3D points is unconstrained, so it is easily misestimated.</p>
</div><div class="column" style="width:10%;">

</div><div class="column" style="width:50%;">
<p><img data-src="img/camera-visual-odometry.png"></p>
</div></div>
</section>
<section id="d-to-2d-algorithm" class="slide level2">
<h2>2D-to-2D Algorithm</h2>

<img data-src="img/Lecture11_127.png" class="r-stretch"><p>How do we compute the relative scale between <span class="math inline">\(I_{k-2}, I_{k-1}\)</span>, and <span class="math inline">\(I_k\)</span> ?</p>
</section>
<section id="vo-drift" class="slide level2">
<h2>VO Drift</h2>
<div class="columns">
<div class="column">
<ul>
<li><p>The errors introduced by each new frame-to-frame motion accumulate over time</p></li>
<li><p>This generates a drift of the estimated trajectory from the real path</p></li>
</ul>
</div><div class="column medium-font">
<p><img data-src="img/Lecture11_128.png"></p>
<p>The uncertainty of the camera pose at Cr is a combination of the uncertainty at Ck-1 (black solid ellipse) and the uncertainty of the transformation Tk.k-1 (gray dashed ellipse)</p>
</div></div>
<p><span class="tiny-font absolute" style="bottom: 0px; ">Copyright of Davide Scaramuzza - davide.scaramuzza@ieee.org - https://sites.google.com/site/scarabotix/</span></p>
</section>
<section id="influence-of-outliers-on-motion-estimation" class="slide level2">
<h2>Influence of Outliers on Motion Estimation</h2>
<div class="columns">
<div class="column">
<p><img data-src="img/Lecture11_129.png"></p>
</div><div class="column">
<ul>
<li>Error at the loop closure: 6.5 m</li>
<li>Error in orientation: 5 deg</li>
<li>Trajectory length: 400 m</li>
</ul>
</div></div>
</section>
<section id="are-points-at-infinity-useful-for-localization" class="slide level2">
<h2>Are points-at-infinity useful for localization?</h2>
<div class="columns">
<div class="column" style="width:15%;">

</div><div class="column" style="width:85%;">
<p><img data-src="img/Lecture11_130.png"></p>
<p>For estimating translation, most likely no. For estimating rotation, yes.</p>
<p>Look up “Inverse Depth Parameterization for Monocular SLAM” for more info.</p>
</div></div>
<div class="absolute semi-tiny-font" style="left: -50px; bottom: 250px; ">
<p><span style="color:#ff0000">Points-at-infinity</span><br>
<span style="color:#ff0000">can help estimate</span><br>
<span style="color:#ff0000">the camera’s rotation,</span><br>
<span style="color:#ff0000">similarly to how we use stars</span><br>
<span style="color:#ff0000">for navigation, without </span><br>
<span style="color:#ff0000">estimating how far they are.</span></p>
</div>
</section>
<section id="problem-3-visual-slam" class="slide level2 center">
<h2>Problem #3: Visual SLAM</h2>
</section>
<section id="structure-from-motion-1" class="slide level2">
<h2>Structure from Motion</h2>
<p>How can we estimate both 3D point positions and the relative camera transformations?</p>
<div class="columns">
<div class="column semi-tiny-font" style="width:15%;">
<p><br><br><br></p>
<p><span style="color:#ff0000">Sometimes also</span> <span style="color:#ff0000">called bundle </span> <span style="color:#ff0000">adjustment</span></p>
<p><br><br><br></p>
<p><span style="color:#ff0000">Q: Why is it different than</span> <span style="color:#ff0000">SLAM?</span></p>
</div><div class="column" style="width:85%;">
<p><img data-src="img/structure-from-motion.png"></p>
</div></div>
</section>
<section id="structure-from-motion-2" class="slide level2">
<h2>Structure from Motion</h2>
<p>How can we estimate both 3D point positions and the relative camera transformations?</p>
<div class="columns">
<div class="column semi-tiny-font red" style="width:15%;">
<p><br><br><br></p>
<p><span style="color:#ff0000">Sometimes also</span> <span style="color:#ff0000">called bundle </span> <span style="color:#ff0000">adjustment</span></p>
<p><br><br><br></p>
<p><span style="color:#ff0000">Q: Why is it different than</span> <span style="color:#ff0000">SLAM?</span></p>
<p><br></p>
<p>A: SLAM potentially includes</p>
<ul>
<li>loop closure</li>
<li>dynamics constraints</li>
<li>velocities, accelerations</li>
</ul>
</div><div class="column" style="width:85%;">
<p><img data-src="img/structure-from-motion.png"></p>
</div></div>
</section>
<section id="loop-closure-in-visual-slam" class="slide level2">
<h2>Loop Closure in Visual SLAM</h2>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img data-src="img/Lecture11_139.jpg"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img data-src="img/Lecture11_140.jpg"></p>
</div>
</div>
</div>
<p><span class="small-font">ORB-SLAM, Mur-Artal, Tardos, Montiel, Galvez-Lopez</span></p>
</section>
<section id="section-13" class="slide level2">
<h2></h2>
<p>Bundler (bundle adjustment/structure from motion)</p>

<img data-src="img/Lecture11_141.png" class="r-stretch"></section>
<section id="structure-from-motion-as-least-squares" class="slide level2">
<h2>Structure from Motion as Least Squares</h2>
<div class="medium-font">
<p><span class="math display">\[\color{green}\boxed{\color{black}_w^k R^*, ^kt_{kw}^{*}}, \color{green}\boxed{\color{black}^wX^*, ^wY^*, ^wZ^*} \color{black} = \underset{^wp=[^wX, ^wY, ^wZ], _w^k R, ^kt_{kw}\quad}{\operatorname{argmin}} \sum_{k=1}^{K} \left\|\bar{z}^{(k)} - \mathbb{E}\left[h_\text{pinhole}\left(_{w}^kR^w p + ^kt_{kw}\right)\right]\right\|^2\]</span></p>
</div>
<div class="red semi-tiny-font right-align absolute" style="bottom: 293px; right: 210px; ">
<p><img data-src="img/double-red-arrow.png" height="150"><br>
Indicates the frame of the k-th camera.</p>
</div>
<div class="absolute medium-font" style="bottom: 0px; right: 0px; ">
<p><span class="math inline">\(h_{\text{pinhole}}(X, Y, Z) = \begin{bmatrix} \frac{f m_x X}{Z} + c_x \\ \frac{f m_y Y}{Z} + c_y \end{bmatrix} + n, \quad n \sim \mathcal{N}(0, R) \quad \text{noise (in pixels)}\)</span></p>
</div>
</section>
<section id="structure-from-motion-as-least-squares-1" class="slide level2">
<h2>Structure from Motion as Least Squares</h2>
<div class="medium-font">
<p><span class="math display">\[\color{black}_w^k R^*, ^kt_{kw}^{*}, \color{black}^wX^*, ^wY^*, ^wZ^* \color{black} = \underset{^wp=[^wX, ^wY, ^wZ], _w^k R, ^kt_{kw}\quad}{\operatorname{argmin}} \sum_{k=1}^{K} \left\|\color{blue}\boxed{\color{black}\bar{z}^{(k)}} \color{black}- \color{red}\boxed{\color{black}\mathbb{E}\left[h_\text{pinhole}\left(_{w}^kR^w p + ^kt_{kw}\right)\right]}\color{black}\right\|^2\]</span></p>
</div>
<div class="absolute" style="bottom: 300px; right: 260px; ">
<p><img data-src="img/red-blue-arrows.png" height="150"></p>
</div>
<div class="red semi-tiny-font absolute" style="bottom: 330px; right: 200px; ">
<p>Expected pixel projection of<br>
3D point <span class="math inline">\(^wp\)</span> onto camera k</p>
</div>
<div class="blue semi-tiny-font absolute" style="bottom: 270px; right: 400px; ">
<p>Actual pixel measurement of<br>
3D point <span class="math inline">\(^wp\)</span> from camera k</p>
</div>
<div class="absolute medium-font" style="bottom: 0px; right: 0px; ">
<p><span class="math inline">\(h_{\text{pinhole}}(X, Y, Z) = \begin{bmatrix} \frac{f m_x X}{Z} + c_x \\ \frac{f m_y Y}{Z} + c_y \end{bmatrix} + n, \quad n \sim \mathcal{N}(0, R) \quad \text{noise (in pixels)}\)</span></p>
</div>
</section>
<section id="structure-from-motion-as-least-squares-2" class="slide level2">
<h2>Structure from Motion as Least Squares</h2>
<div class="medium-font">
<p><span class="math display">\[\color{black}_w^k R^*, ^kt_{kw}^{*}, \color{black}^wX^*, ^wY^*, ^wZ^* \color{black} = \underset{^wp=[^wX, ^wY, ^wZ], _w^k R, ^kt_{kw}\quad}{\operatorname{argmin}} \sum_{k=1}^{K} \left\|\bar{z}^{(k)} - \mathbb{E}\left[h_\text{pinhole}\left(_{w}^kR^w p + ^kt_{kw}\right)\right]\right\|^2\]</span></p>
</div>
<p><img data-src="img/boxes-arrows.png" class="absolute" style="top: 180px; left: 105px; width: 220px; "></p>
<div class="red-annotation">
<p><br><br></p>
<p>Q: Is the scale of these two estimates accurate/unambiguous<br>
when measurements are done from a monocular (single)<br>
camera in motion? I.e. is it observable?</p>
<p>Note: <strong>scale</strong> = relationship between real-world metric distances<br>
and estimated map distances. I.e. relationship between distance<br>
units.</p>
</div>
</section>
<section id="structure-from-motion-as-least-squares-3" class="slide level2">
<h2>Structure from Motion as Least Squares</h2>
<div class="medium-font">
<p><span class="math display">\[\color{black}_w^k R^*, ^kt_{kw}^{*}, \color{black}^wX^*, ^wY^*, ^wZ^* \color{black} = \underset{^wp=[^wX, ^wY, ^wZ], _w^k R, ^kt_{kw}\quad}{\operatorname{argmin}} \sum_{k=1}^{K} \left\|\bar{z}^{(k)} - \mathbb{E}\left[h_\text{pinhole}\left(_{w}^kR^w p + ^kt_{kw}\right)\right]\right\|^2\]</span></p>
</div>
<p><img data-src="img/boxes-arrows.png" class="absolute" style="top: 180px; left: 105px; width: 220px; "></p>
<div class="red-annotation">
<p><br><br></p>
<p>Q: Is the scale of these two estimates accurate/unambiguous<br>
when measurements are done from a monocular (single)<br>
camera in motion? I.e. is it observable?</p>
<p>A: No, regardless of how many common keypoints are<br>
matched in between camera frames. Without external<br>
reference distance, e.g.&nbsp;stereo baseline, or real size of<br>
observed object, the scale is ambiguous and unobservable,<br>
just as it was in Visual Odometry.</p>
</div>
</section>
<section id="structure-from-motion-as-least-squares-4" class="slide level2">
<h2>Structure from Motion as Least Squares</h2>
<div class="medium-font">
<p><span class="math display">\[\color{black}_w^k R^*, ^kt_{kw}^{*}, \color{black}^wX^*, ^wY^*, ^wZ^* \color{black} = \underset{^wp=[^wX, ^wY, ^wZ], _w^k R, ^kt_{kw}\quad}{\operatorname{argmin}} \sum_{k=1}^{K} \left\|\bar{z}^{(k)} - \mathbb{E}\left[h_\text{pinhole}\left(_{w}^kR^w p + ^kt_{kw}\right)\right]\right\|^2\]</span></p>
</div>
<p><img data-src="img/boxes-arrows.png" class="absolute" style="top: 180px; left: 105px; width: 220px; "></p>
<div class="red-annotation">
<p><br><br></p>
<p>Q: Is the scale of these two estimates constant during the entire experiment,<br>
if we use a monocular camera?</p>
<div class="fragment">
<p>A: No.&nbsp;During in-place rotations there is not enough baseline between<br>
camera frames to triangulate new points. So, error in structure and in motion<br>
accumulates <span class="math inline">\(\rightarrow\)</span> <strong>scale drift</strong></p>
<p>Similarly to visual odometry with a single camera.</p>
</div>
</div>
</section>
<section id="problem-4-optical-flow" class="slide level2 center">
<h2>Problem #4: optical flow</h2>
</section>
<section id="multi-view-geometry-problems-7" class="slide level2">
<h2>Multi-view geometry problems</h2>
<ul>
<li>Optical flow: Given two images, find the location of a world point in a second close-by image with no camera info.</li>
</ul>
<div class="columns">
<div class="column semi-tiny-font red" style="width:15%;">
<p><br><br></p>
<p><span style="color:#ff0000">3D point coordinates </span> <span style="color:#ff0000">are unknown, but we </span> <span style="color:#ff0000">won’t try to estimate them</span></p>
<p><br><br><br><br><br></p>
<p><span style="color:#ff0000">Camera frame </span> <span style="color:#ff0000">transformations</span> <span style="color:#ff0000">are unknown, but we</span> <span style="color:#ff0000">won’t try to estimate them </span></p>
</div><div class="column" style="width:45%;">
<p><img data-src="img/Lecture11_152.png"></p>
</div><div class="column semi-tiny-font" style="width:40%;">
<p><br><br><br></p>
<p><span style="color:#ff0000">We are estimating pixel displacement</span> <span style="color:#ff0000">from one image to the next</span></p>
<p><img data-src="img/Lecture11_153.png"></p>
<p><a href="http://tcr.amegroups.com/article/viewFile/3200" class="uri">http://tcr.amegroups.com/article/viewFile/3200</a></p>
</div></div>
</section>
<section id="video" class="slide level2">
<h2>Video</h2>
<ul>
<li>A video is a sequence of frames captured over time</li>
<li>Now our image data is a function of space (x, y) and time (t)</li>
</ul>

<img data-src="img/Lecture11_154.png" class="r-stretch"></section>
<section id="motion-estimation-optical-flow" class="slide level2">
<h2>Motion estimation: Optical flow</h2>
<p><em>Optic flow</em> is the <strong>apparent</strong> motion of objects or surfaces</p>

<img data-src="img/Lecture11_155.png" class="r-stretch"><p>Will start by estimating motion of each pixel separately</p>
<p>Then will consider motion of entire image</p>
</section>
<section id="motion-applications-segmentation-of-video" class="slide level2">
<h2><span class="medium-font">Motion Applications: Segmentation of video</span></h2>
<ul>
<li>Background subtraction
<ul>
<li>A static camera is observing a scene</li>
<li>Goal: separate the static <em>background</em> from the moving <em>foreground</em></li>
</ul></li>
</ul>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img data-src="img/Lecture11_156.png"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img data-src="img/Lecture11_157.png"></p>
</div>
</div>
</div>
</section>
<section id="motion-applications-segmentation-of-video-1" class="slide level2">
<h2><span class="medium-font">Motion Applications: Segmentation of video</span></h2>
<ul>
<li>Shot boundary detection in edited video
<ul>
<li>Edited video is usually composed of <em>shots</em> or sequences showing the same objects or scene</li>
<li>Goal: segment video into shots for summarization and browsing (each shot can be represented by a single keyframe in a user interface)</li>
<li>Difference from background subtraction: the camera is not necessarily stationary</li>
</ul></li>
</ul>

<img data-src="img/Lecture11_158.png" class="r-stretch"></section>
<section id="motion-applications-segmentation-of-video-2" class="slide level2">
<h2><span class="medium-font">Motion Applications: Segmentation of video</span></h2>
<ul>
<li>Background subtraction</li>
<li>Shot boundary detection</li>
<li>Motion segmentation
<ul>
<li>Segment the video into multiple coherently moving objects</li>
</ul></li>
</ul>

<img data-src="img/Lecture11_159.png" class="quarto-figure quarto-figure-right r-stretch"></section>
<section id="motion-and-perceptual-organization" class="slide level2">
<h2>Motion and perceptual organization</h2>
<ul>
<li>Sometimes, motion is the only cue</li>
</ul>

<img data-src="img/Lecture11_160.png" class="quarto-figure quarto-figure-center r-stretch"></section>
<section id="motion-and-perceptual-organization-1" class="slide level2">
<h2>Motion and perceptual organization</h2>
<ul>
<li>Sometimes, motion is the only cue</li>
</ul>

<img data-src="img/Lecture11_161.gif" class="quarto-figure quarto-figure-center r-stretch"></section>
<section id="motion-and-perceptual-organization-2" class="slide level2">
<h2>Motion and perceptual organization</h2>
<iframe data-external="1" src="https://www.youtube.com/embed/VTNmLt7QX8E" width="600" height="400" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
<p><span class="semi-tiny-font">Experimental study of apparent behavior. Fritz Heider &amp; Marianne Simmel. 1944</span></p>
</section>
<section id="problem-definition-optical-flow" class="slide level2">
<h2>Problem definition: optical flow</h2>
<p><img data-src="img/Lecture11_162.png" height="250"></p>
<div class="medium-font">
<p>How to estimate pixel motion from image <span class="math inline">\(I(x,y,t)\)</span> to <span class="math inline">\(I(x,y,t+ 1)\)</span> ?</p>
<ul>
<li>Solve pixel correspondence problem
<ul>
<li>Given a pixel in l(x,y,t), look for nearby pixels of the same color in <em>l(x,y,t+1)</em></li>
</ul></li>
</ul>
<p>Key assumptions</p>
<ul>
<li><span class="red">Small motion</span>: Points do not move very far</li>
<li><span class="blue">Color constancy</span>: A point in <em>l(x,y,t)</em> looks the same in <em>l(x,y,t+ 1)</em>
<ul>
<li>For grayscale images, this is brightness constancy</li>
</ul></li>
</ul>
</div>
<p><img data-src="img/red-box.png" class="absolute" style="left: 349px; bottom: 185px; width: 70px; "></p>
<p><img data-src="img/blue-box.png" class="absolute" style="bottom: 185px; right: 410px; width: 110px; "></p>
</section>
<section id="optical-flow-constraints-grayscale-images" class="slide level2">
<h2><span class="medium-font">Optical flow constraints (grayscale images)</span></h2>

<img data-src="img/Lecture11_163.png" class="r-stretch"><ul>
<li>Let’s look at these constraints more closely
<ul>
<li><p>Brightness constancy constraint (equation) <span class="math inline">\(I(x,y,t) = I(x+u,y+v,t+1)\)</span></p></li>
<li><p>Small motion: (u and v are less than 1 pixel, or smooth)</p></li>
</ul></li>
</ul>
<p>From Taylor expansion of I</p>
<p><span class="math inline">\(I(x+u,y+v,t+1)=I(x,y,t+1)+\frac{\partial I}{\partial x}u+\frac{\partial I}{\partial y}v+\text{ higher order terms}\)</span></p>
</section>
<section id="optical-flow-equation" class="slide level2">
<h2>Optical flow equation</h2>
<ul>
<li>Combining these two equations</li>
</ul>
<div class="medium-font">
<p><span class="math display">\[\begin{align}
0 &amp;= I(x + u, y + v, t + 1) - I(x, y, t) \\
&amp;\approx I(x, y, t + 1) + I_x u + I_y v - I(x, y, t) \quad \text{(Short hand: } I_x = \frac{\partial I}{\partial x} \text{ for } t \text{ or } t+1\text{)}
\end{align}\]</span></p>
</div>
</section>
<section id="optical-flow-equation-1" class="slide level2">
<h2>Optical flow equation</h2>
<ul>
<li>Combining these two equations</li>
</ul>
<div class="medium-font">
<p><span class="math display">\[\begin{align}
0 &amp;= I(x + u, y + v, t + 1) - I(x, y, t) \\
&amp;\approx I(x, y, t + 1) + I_x u + I_y v - I(x, y, t) \quad \text{(Short hand: } I_x = \frac{\partial I}{\partial x} \text{ for } t \text{ or } t+1\text{)} \\
&amp;\approx \left[I(x, y, t+1)-I(x, y, t)\right]+I_x u+I_y v \\
&amp; \approx I_t+I_x u+I_y v
\end{align}\]</span></p>
</div>
</section>
<section id="optical-flow-equation-2" class="slide level2">
<h2>Optical flow equation</h2>
<ul>
<li>Combining these two equations</li>
</ul>
<div class="medium-font">
<p><span class="math display">\[\begin{align}
0 &amp;= I(x + u, y + v, t + 1) - I(x, y, t) \\
&amp;\approx I(x, y, t + 1) + I_x u + I_y v - I(x, y, t) \quad \text{(Short hand: } I_x = \frac{\partial I}{\partial x} \text{ for } t \text{ or } t+1\text{)} \\
&amp;\approx \color{red}\boxed{\color{black}\left[I(x, y, t+1)-I(x, y, t)\right]}\color{black}+I_x u+I_y v \\
&amp; \approx \color{red}\boxed{\color{black}I_t}\color{black}+I_x u+I_y v
\end{align}\]</span></p>
</div>
<p>In the limit as u and v go to zero, this becomes exact</p>
<div class="bordered-box">
<p><em>Brightness constancy constraint equation</em> <span class="math display">\[I_{x}u+I_{y}v+I_{t}=0\]</span></p>
</div>
</section>
<section id="the-brightness-constancy-constraint" class="slide level2">
<h2>The brightness constancy constraint</h2>
<p>Can we use this equation to recover image motion (u,v) at each pixel?</p>
<p><span class="math inline">\(0 = I_t + \nabla I \cdot \langle u,v \rangle \quad \text{or} \quad I_x u + I_y v + I_t = 0\)</span></p>
<ul>
<li>How many equations and unknowns per pixel?
<ul>
<li>One equation (this is a scalar equation!), two unknowns (u,v)</li>
</ul></li>
</ul>
<p>The component of the motion perpendicular to the gradient (i.e., parallel to the edge) cannot be measured</p>
<div class="columns">
<div class="column">
<p>If <em>(u, v)</em> satisfies the equation, so does <em>(utu’, v+v’)</em> if</p>
<p><span class="math inline">\(\nabla I \cdot [u' v']^T = 0\)</span></p>
</div><div class="column">
<p><img data-src="img/Lecture11_168.png"></p>
</div></div>
</section>
<section id="aperture-problem" class="slide level2">
<h2>Aperture problem</h2>

<img data-src="img/Lecture11_169.png" class="r-stretch"></section>
<section id="aperture-problem-1" class="slide level2">
<h2>Aperture problem</h2>

<img data-src="img/Lecture11_170.png" class="r-stretch"></section>
<section id="solving-the-ambiguity" class="slide level2">
<h2>Solving the ambiguity…</h2>
<p><span class="small-font">B. Lucas and T. Kanade. An iterative image registration technique with an application to stereo vision. In <em>Proceedings of the International Joint Conference on Artificial Intelligence</em>, pp.&nbsp;674-679, 1981.</span></p>
<ul>
<li>How to get more equations for a pixel?</li>
<li><strong>Spatial coherence constraint</strong></li>
<li>Assume the pixel’s neighbors have the same (u,v)</li>
<li>If we use a 5x5 window, that gives us 25 equations per pixel <span class="math inline">\(0 = I_t(\mathbf{p}_i) + \nabla I(\mathbf{p}_i) \cdot [u \quad v]\)</span></li>
</ul>
<p><span class="math display">\[\begin{bmatrix}
I_{x}(\mathbf{p}_{1}) &amp; I_{y}(\mathbf{p}_{1}) \\
I_{x}(\mathbf{p}_{2}) &amp; I_{y}(\mathbf{p}_{2}) \\
\vdots &amp; \vdots \\
I_{x}(\mathbf{p}_{25}) &amp; I_{y}(\mathbf{p}_{25})
\end{bmatrix}
\begin{bmatrix}
u \\
v
\end{bmatrix}
= -
\begin{bmatrix}
I_{t}(\mathbf{p}_{1}) \\
I_{t}(\mathbf{p}_{2}) \\
\vdots \\
I_{t}(\mathbf{p}_{25})
\end{bmatrix}\]</span></p>
</section>
<section id="solving-the-ambiguity-1" class="slide level2">
<h2>Solving the ambiguity…</h2>
<ul>
<li>Least squares problem: <span class="math display">\[\begin{bmatrix}
I_x(\mathbf{p}_1) &amp; I_y(\mathbf{p}_1) \\
I_x(\mathbf{p}_2) &amp; I_y(\mathbf{p}_2) \\
\vdots &amp; \vdots \\
I_x(\mathbf{p}_{25}) &amp; I_y(\mathbf{p}_{25})
\end{bmatrix} \begin{bmatrix} u \\ v \end{bmatrix} = -\begin{bmatrix}
I_t(\mathbf{p}_1) \\
I_t(\mathbf{p}_2) \\
\vdots \\
I_t(\mathbf{p}_{25})
\end{bmatrix} \quad \underset{25 \times 2 \quad 2 \times 1 \quad 25 \times 1}{A \quad d = b}\]</span></li>
</ul>


</section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<div class="footer footer-default">
<p><a href="https://csc477.github.io/website_fall24" target="_blank" style="font-size:0.8em; bottom: -5px;">↩︎ Back to Course Website</a></p>
</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="../../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"8\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true,"boardmarkerWidth":2,"chalkWidth":2,"chalkEffect":1},
'smaller': true,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
            const codeEl = trigger.previousElementSibling.cloneNode(true);
            for (const childEl of codeEl.children) {
              if (isCodeAnnotation(childEl)) {
                childEl.remove();
              }
            }
            return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp("https:\/\/csc477\.github\.io\/website_fall24");
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    

</body></html>