<!DOCTYPE html>
<html lang="en"><head>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-html/tabby.min.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-e1a5c8363afafaef2c763b6775fbf3ca.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/quarto-contrib/videojs/video.min.js"></script>
<link href="../../site_libs/quarto-contrib/videojs/video-js.css" rel="stylesheet"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.7.31">

  <meta name="author" content="Florian Shkurti">
  <title>CSC477 - Fall 2024 – CSC477 Introduction to Mobile Robotics</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/theme/quarto-f563837468303362081e247dddd440d0.css">
  <link rel="stylesheet" href="../style.css">
  <link href="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
<meta property="og:title" content="CSC477 Introduction to Mobile Robotics – CSC477 - Fall 2024">
<meta property="og:description" content="Week #7: Least Squares Estimation and GraphSLAM">
<meta property="og:site_name" content="CSC477 - Fall 2024">
<meta name="twitter:title" content="CSC477 Introduction to Mobile Robotics – CSC477 - Fall 2024">
<meta name="twitter:description" content="Week #7: Least Squares Estimation and GraphSLAM">
<meta name="twitter:card" content="summary">
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">CSC477 Introduction to Mobile Robotics</h1>
  <p class="subtitle">Week #7: Least Squares Estimation and GraphSLAM</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Florian Shkurti 
</div>
</div>
</div>

</section>
<section id="todays-agenda" class="slide level2">
<h2>Today’s agenda</h2>
<div class="columns">
<div class="column" style="width:70%;">
<ul>
<li><p>Least Squares Estimation</p></li>
<li><p>Maximum Likelihood Estimation (MLE)</p></li>
<li><p>Maximum a Posteriori Estimation (MAP)</p></li>
<li><p>Bayesian Estimation</p></li>
<li><p>GraphSLAM</p></li>
</ul>
</div><div class="column" style="width:30%;">
<p><img data-src="img/Lecture7_1.png"></p>
</div></div>
</section>
<section id="estimating-parameters-of-probability-models" class="slide level2">
<h2>Estimating parameters of probability models</h2>
<ul>
<li><p>In the occupancy grid mapping problem we wanted to compute <span class="math inline">\(p(\mathbf{m}|\mathbf{z}_{1:t}, \mathbf{x}_{1:t})\)</span> over all possible maps.</p></li>
<li><p>We can see this problem as a specific instance within a category of problems where we are given data (observations) and we want to “explain” or fit the data using a parametric function.</p></li>
</ul>
<div class="fragment">
<ul>
<li>There are typically three ways to work with this type of problems:
<ol type="1">
<li>Maximum Likelihood parameter estimation (MLE)
<ul>
<li><span class="red">Least Squares</span></li>
</ul></li>
<li>Maximum A Posteriori (MAP) parameter estimation</li>
<li>Bayesian parameter distribution estimation</li>
</ol></li>
</ul>
</div>
</section>
<section id="least-squares-parameter-estimation" class="slide level2">
<h2>Least Squares Parameter Estimation</h2>
<div class="columns medium-font">
<div class="column">
<p><img data-src="img/Lecture7_4.png"></p>
<div class="fragment">
<p>Example: we think that the 2D data was generated by a line <span class="math inline">\(z = \theta_{0} + \theta_{1}x\)</span> whose parameters we do not know, and was corrupted by noise.</p>
</div>
</div><div class="column">
<p>We are given data points <span class="math inline">\(\mathbf{(x_1, z_1), \dots, (x_N, z_N)}\)</span></p>
<p>We <strong>think</strong> that the data was generated by a parametric function <span class="math inline">\(\mathbf{z = h(\theta, x)}\)</span></p>
</div></div>
</section>
<section id="least-squares-parameter-estimation-1" class="slide level2">
<h2>Least Squares Parameter Estimation</h2>
<div class="columns medium-font">
<div class="column">
<p><img data-src="img/Lecture7_4.png"></p>
<p>Example: we think that the 2D data was generated by a line <span class="math inline">\(z = \theta_{0} + \theta_{1}x\)</span> whose parameters we do not know</p>
</div><div class="column">
<p>We are given data points <span class="math inline">\(\mathbf{(x_1, z_1), \dots, (x_N, z_N)}\)</span></p>
<p>We <strong>think</strong> that the data was generated by a parametric function <span class="math inline">\(\mathbf{z = h(\theta, x)}\)</span></p>
<p>This parametric model will have a fitting error: <span class="math display">\[e(\theta)=\sum_{i=1}^{N}||\mathbf{z}_{i}-\mathbf{h}(\theta,\mathbf{x}_{i})||^{2}\]</span></p>
<p>The least-squares estimator is: <span class="math display">\[\theta_{LS} = \underset{\theta}{\operatorname*{argmin}} e(\theta)\]</span></p>
</div></div>
</section>
<section id="linear-least-squares-parameter-estimation" class="slide level2">
<h2><span class="medium-font"><span class="red">Linear</span> Least Squares Parameter Estimation</span></h2>
<div class="columns medium-font">
<div class="column">
<p><img data-src="img/Lecture7_4.png"></p>
<p>Example: we think that the 2D data was generated by a line <span class="math inline">\(z = \theta_{0} + \theta_{1}x\)</span> whose parameters we do not know</p>
</div><div class="column">
<p>We are given data points <span class="math inline">\(\mathbf{(x_1, z_1), \dots, (x_N, z_N)}\)</span></p>
<p>We <strong>think</strong> that the data was generated by a <span class="red">linear</span> parametric function <span class="math inline">\(\mathbf{z = h(\theta, x) = H_x\theta}\)</span> where <span class="math inline">\(\mathbf{H_x}\)</span> is a matrix whose elements depend on <span class="math inline">\(\mathbf{x}\)</span></p>
<p>This parametric model will have a fitting error: <span class="math display">\[e(\theta) = \sum_{i=1}^{N} ||\mathbf{z}_i - \mathbf{H}_{x_i}\theta||^2\]</span></p>
<p>The least-squares estimator is: <span class="math display">\[\theta_{LS} = \underset{\theta}{\operatorname*{argmin}} e(\theta)\]</span></p>
</div></div>
</section>
<section id="linear-least-squares-parameter-estimation-1" class="slide level2">
<h2><span class="medium-font">Linear Least Squares Parameter Estimation</span></h2>
<div class="columns medium-font">
<div class="column">
<p><img data-src="img/Lecture7_4.png"></p>
<p>Example: we think that the 2D data was generated by a line <span class="math inline">\(z = \theta_{0} + \theta_{1}x\)</span> whose parameters we do not know</p>
</div><div class="column">
<p>We are given data points <span class="math inline">\(\mathbf{(x_1, z_1), \dots, (x_N, z_N)}\)</span></p>
<p>We <strong>think</strong> that the data was generated by a linear parametric function <span class="math inline">\(\mathbf{z = h(\theta, x) = \mathbf{H}_x\theta}\)</span></p>
<p>This parametric model will have a fitting error:</p>
<p><span class="math display">\[\begin{align}
e(\theta) &amp;= \sum_{i=1}^{N} ||\mathbf{z}_i - \mathbf{H}_{x_i} \theta||^2 \\
&amp;= \sum_{i=1}^{N} \mathbf{z}_i^T \mathbf{z}_i - 2\theta^T \mathbf{H}_{x_i}^T \mathbf{z}_i + \theta^T \mathbf{H}_{x_i}^T \mathbf{H}_{x_i} \theta
\end{align}\]</span></p>
<div class="fragment">
<p>The least-squares estimator minimizes the error:</p>
<p><span class="medium-font"><span class="math display">\[\frac{\partial e(\theta)}{\partial \theta} = 0 \Leftrightarrow -2\sum_{i=1}^{N} \mathbf{H}_{x_i}^T \mathbf{z}_i + 2\mathbf{H}_{x_i}^T \mathbf{H}_{x_i} \theta = 0 \Leftrightarrow \left[\sum_{i=1}^{N} \mathbf{H}_{x_i}^T \mathbf{H}_{x_i}\right] \theta = \sum_{i=1}^{N} \mathbf{H}_{x_i}^T \mathbf{z}_i\]</span></span></p>
</div>
<div class="fragment medium-font">
<p><span class="math display">\[\theta_{LS} = \operatorname*{argmin}_{\theta} e(\theta) \Leftrightarrow \left[\sum_{i=1}^{N} \mathbf{H}_{x_i}^T \mathbf{H}_{x_i}\right] \theta_{LS} = \sum_{i=1}^{N} \mathbf{H}_{x_i}^T \mathbf{z}_i\]</span></p>
</div>
</div></div>
</section>
<section id="example-1-linear-least-squares" class="slide level2">
<h2>Example #1: Linear Least Squares</h2>
<div class="columns medium-font">
<div class="column">
<p><img data-src="img/Lecture7_4.png"></p>
<p>Example: we think that the 2D data was generated by a line <span class="math inline">\(z = \theta_{0} + \theta_{1}x\)</span> whose parameters we do not know</p>
</div><div class="column">
<p>We are given data points <span class="math inline">\((x_1, z_1), \dots, (x_N, z_N)\)</span></p>
<p>We <strong>think</strong> that the data was generated by a linear parametric function <span class="math inline">\(z = h(\mathbf{\theta}, x) = \begin{bmatrix} 1 &amp; x \end{bmatrix}\mathbf{\theta} = \theta_{0} + \theta_{1}x\)</span></p>
<p>This parametric model will have a fitting error: <span class="math display">\[e(\theta_{0},\theta_{1})=\sum_{i=1}^{N}(z_{i}-\theta_{0}-\theta_{1}x_{i})^{2}\]</span></p>
<p>The least-squares estimator minimizes the error:</p>
<p><span class="math display">\[\theta_{LS} = \operatorname*{argmin}_{\theta_0, \theta_1} e(\theta_0, \theta_1) \Leftrightarrow \left[\sum_{i=1}^{N} \begin{bmatrix} 1 \\ x_i \end{bmatrix} \begin{bmatrix} 1 &amp; x_i \end{bmatrix}\right] \theta_{LS} = \sum_{i=1}^{N} \begin{bmatrix} 1 \\ x_i \end{bmatrix} z_i\]</span></p>
<p>Which is a linear system of 2 equations. If we have at least two data points we can solve for <span class="math inline">\(\theta_{LS}\)</span> to define the line.</p>
</div></div>
</section>
<section id="example-2-linear-least-squares" class="slide level2">
<h2>Example #2: Linear Least Squares</h2>
<div class="columns medium-font">
<div class="column">
<p><img data-src="img/Lecture7_65.gif"></p>
<p>Example: we think that the 2D data was generated by a quadratic <span class="math inline">\(z = \theta_{0} + \theta_{1}x + \theta_2x^2\)</span> whose parameters we do not know.</p>
</div><div class="column">
<p>We are given data points <span class="math inline">\((x_1, z_1), \dots, (x_N, z_N)\)</span></p>
<p>We <strong>think</strong> that the data was generated by a linear parametric function <span class="math inline">\(z = h(\mathbf{\theta},x) = \begin{bmatrix} 1 &amp; x &amp; x^{2} \end{bmatrix}\mathbf{\theta}=\theta_{0}+\theta_{1}x+\theta_{2}x^{2}\)</span></p>
<p>This parametric model will have a fitting error:</p>
<p><span class="math display">\[e(\theta_0, \theta_1, \theta_2) = \sum_{i=1}^{N} (z_i - \theta_0 - \theta_1 x_i - \theta_2 x_i^2)^2\]</span></p>
<p>The least-squares estimator minimizes the error: <span class="medium-font"><span class="math display">\[\theta_{LS} = \operatorname*{argmin}_{\theta_0, \theta_1, \theta_2} e(\theta_0, \theta_1, \theta_2) \Leftrightarrow \left[\sum_{i=1}^{N} \begin{bmatrix} 1 \\ x_i \\ x_i^2 \end{bmatrix} \begin{bmatrix} 1 &amp; x_i &amp; x_i^2 \end{bmatrix}\right] \theta_{LS} = \sum_{i=1}^{N} \begin{bmatrix} 1 \\ x_i \\ x_i^2 \end{bmatrix} z_i\]</span></span></p>
<p>Which is a linear system of 3 equations. If we have at least three data points we can solve for <span class="math inline">\(\theta_{LS}\)</span> to define the quadratic.</p>
</div></div>
</section>
<section id="todays-agenda-1" class="slide level2">
<h2>Today’s agenda</h2>
<div class="columns">
<div class="column" style="width:70%;">
<div class="grey">
<ul>
<li>Least Squares Estimation</li>
</ul>
</div>
<ul>
<li><p>Maximum Likelihood Estimation (MLE)</p></li>
<li><p>Maximum a Posteriori Estimation (MAP)</p></li>
<li><p>Bayesian Estimation</p></li>
<li><p>GraphSLAM</p></li>
</ul>
</div><div class="column" style="width:30%;">
<p><img data-src="img/Lecture7_1.png"></p>
</div></div>
</section>
<section id="estimating-parameters-of-probability-models-1" class="slide level2">
<h2>Estimating parameters of probability models</h2>
<ul>
<li>In the occupancy grid mapping problem we wanted to compute <span class="math inline">\(p(\mathbf{m}|\mathbf{z}_{1:t}, \mathbf{x}_{1:t})\)</span> over all possible maps.</li>
<li>We can see this problem as a specific instance within a category of problems where we are given data (observations) and we want to “explain” or fit the data using a parametric function.</li>
<li>There are typically three ways to work with this type of problems:
<ol type="1">
<li><span style="color:#ff0000">Maximum Likelihood parameter estimation (MLE)</span>
<ul>
<li>Least Squares</li>
</ul></li>
<li>Maximum A Posteriori (MAP) parameter estimation</li>
<li>Bayesian parameter distribution estimation</li>
</ol></li>
</ul>
</section>
<section id="maximum-likelihood-parameter-estimation" class="slide level2">
<h2><span class="medium-font">Maximum Likelihood Parameter Estimation</span></h2>
<div class="columns">
<div class="column" style="width:40%;">
<p><img data-src="img/Lecture7_76.png"></p>
</div><div class="column medium-font" style="width:60%;">
<p>We are given data points <span class="math inline">\(\mathbf{d}_{1:N} = \mathbf{d}_1, \ldots, \mathbf{d}_N\)</span></p>
<p>We <strong>think</strong> the data has been generated from a probability distribution <span class="math inline">\(p(\mathbf{d}_{1:N}|\mathbf{\theta})\)</span></p>
<p>We want to find the parameter of the model that maximizes the likelihood function of the data <span class="math display">\[L(\mathbf{\theta}) = p(\mathbf{d}_{1:N}|\mathbf{\theta})\]</span></p>
<p>which is a function of theta, <strong>not</strong> a probability distribution. <span class="math display">\[\theta_{MLE} = \underset{\theta}{\operatorname*{argmax}} p(\mathbf{d}_{1:N} | \theta)\]</span></p>
</div></div>
</section>
<section id="maximum-likelihood-parameter-estimation-1" class="slide level2">
<h2><span class="medium-font">Maximum Likelihood Parameter Estimation</span></h2>
<p><img data-src="img/data-points.png" class="absolute" style="top: 50px; right: 150px; width: 100px; "></p>
<div class="columns">
<div class="column" style="width:40%;">
<p><img data-src="img/Lecture7_76.png"></p>
</div><div class="column small-font" style="width:60%;">
<p><span class="math display">\[\theta_{MLE} = \underset{\theta}{\operatorname{argmax}} p(\mathbf{d}_{1:N}|\theta)\]</span></p>
<p>Find the parameters of the model that maximize the likelihood function of the data <span class="math display">\[L(\mathbf{\theta}) = p(\mathbf{d}_{1:N}|\mathbf{\theta})\]</span></p>
<p>which is a function of theta, <strong>not</strong> a probability distribution.</p>
</div></div>
<p><img data-src="img/blue-arrow.png" class="absolute" style="left: 240px; bottom: 290px; height: 60px; "></p>
<div class="semi-tiny-font">
<p>Example: assume we know that 1D <span style="color:#002060">data points</span> were generated <u>independently</u> from a Gaussian distribution <span class="math inline">\(\mathcal{N}(\mu, \sigma^2)\)</span> , but we don’t know the mean and variance. The likelihood function of the data is</p>
<div class="fragment">
<p><span class="math display">\[L(\mu, \sigma) = p(\mathbf{d}_{1:N}|\mu, \sigma) = \prod_{i=1}^{N} p(d_i|\mu, \sigma) = \prod_{i=1}^{N} \frac{1}{\sqrt{2\pi\sigma}} \exp(-0.5(d_i - \mu)^2/\sigma^2)\]</span></p>
</div>
<div class="fragment">
<p>And the maximum-likelihood parameter estimates are</p>
<p><span class="math display">\[(\mu, \sigma)_{MLE} = \operatorname*{argmax}_{\mu,\sigma} p(\mathbf{d}_{1:N}|\mu, \sigma) = \operatorname*{argmax}_{\mu,\sigma} \log p(\mathbf{d}_{1:N}|\mu, \sigma) = \operatorname*{argmax}_{\mu,\sigma} \sum_{i=1}^{N} \log p(d_i|\mu, \sigma)\]</span></p>
</div>
</div>
</section>
<section id="maximum-likelihood-parameter-estimation-2" class="slide level2">
<h2><span class="medium-font">Maximum Likelihood Parameter Estimation</span></h2>
<p><img data-src="img/data-points.png" class="absolute" style="top: 50px; right: 150px; width: 100px; "></p>
<div class="columns">
<div class="column" style="width:40%;">
<p><img data-src="img/Lecture7_76.png"></p>
</div><div class="column small-font" style="width:60%;">
<p><span class="math display">\[\theta_{MLE} = \underset{\theta}{\operatorname{argmax}} p(\mathbf{d}_{1:N}|\theta)\]</span></p>
<p>Find the parameters of the model that maximize the likelihood function of the data <span class="math display">\[L(\mathbf{\theta}) = p(\mathbf{d}_{1:N}|\mathbf{\theta})\]</span></p>
<p>which is a function of theta, <strong>not</strong> a probability distribution.</p>
</div></div>
<p><img data-src="img/blue-arrow.png" class="absolute" style="left: 240px; bottom: 290px; height: 60px; "></p>
<div class="semi-tiny-font">
<p>Example: assume we know that 1D <span style="color:#002060">data points</span> were generated <u>independently</u> from a Gaussian distribution <span class="math inline">\(\mathcal{N}(\mu, \sigma^2)\)</span> , but we don’t know the mean and variance. The likelihood function of the data is</p>
<p><span class="math display">\[L(\mu, \sigma) = p(\mathbf{d}_{1:N}|\mu, \sigma) = \prod_{i=1}^{N} p(d_i|\mu, \sigma) = \prod_{i=1}^{N} \frac{1}{\sqrt{2\pi\sigma}} \exp(-0.5(d_i - \mu)^2/\sigma^2)\]</span></p>
<p>And the maximum-likelihood parameter estimates are</p>
<p><span class="math display">\[(\mu, \sigma)_{MLE} = \operatorname*{argmax}_{\mu,\sigma} \sum_{i=1}^{N} \log p(d_i|\mu, \sigma) = \operatorname*{argmax}_{\mu,\sigma} \left[ -N\log(\sqrt{2\pi}\sigma) - \frac{1}{2\sigma^2}\sum_{i=1}^{N}(d_i - \mu)^2 \right]\]</span></p>
</div>
</section>
<section id="maximum-likelihood-parameter-estimation-3" class="slide level2">
<h2><span class="medium-font">Maximum Likelihood Parameter Estimation</span></h2>
<p><img data-src="img/data-points.png" class="absolute" style="top: 50px; right: 150px; width: 100px; "></p>
<div class="columns">
<div class="column" style="width:40%;">
<p><img data-src="img/Lecture7_76.png"></p>
</div><div class="column small-font" style="width:60%;">
<p><span class="math display">\[\theta_{MLE} = \underset{\theta}{\operatorname{argmax}} p(\mathbf{d}_{1:N}|\theta)\]</span></p>
<p>Find the parameters of the model that maximize the likelihood function of the data <span class="math display">\[L(\mathbf{\theta}) = p(\mathbf{d}_{1:N}|\mathbf{\theta})\]</span></p>
<p>which is a function of theta, <strong>not</strong> a probability distribution.</p>
</div></div>
<p><img data-src="img/blue-arrow.png" class="absolute" style="left: 240px; bottom: 290px; height: 60px; "></p>
<div class="semi-tiny-font">
<p>Example: assume we know that 1D <span style="color:#002060">data points</span> were generated <u>independently</u> from a Gaussian distribution <span class="math inline">\(\mathcal{N}(\mu, \sigma^2)\)</span> , but we don’t know the mean and variance. The likelihood function of the data is</p>
<p><span class="math display">\[L(\mu, \sigma) = p(\mathbf{d}_{1:N}|\mu, \sigma) = \prod_{i=1}^{N} p(d_i|\mu, \sigma) = \prod_{i=1}^{N} \frac{1}{\sqrt{2\pi\sigma}} \exp(-0.5(d_i - \mu)^2/\sigma^2)\]</span></p>
<p>And the maximum-likelihood parameter estimates are</p>
<div class="columns">
<div class="column medium-font" style="width:55%;">
<p><span class="math display">\[(\mu, \sigma)_{MLE} = \operatorname*{argmax}_{\mu,\sigma} \sum_{i=1}^{N} \log p(d_i|\mu, \sigma) = \operatorname*{argmax}_{\mu,\sigma} \left[ -N\log(\sqrt{2\pi}\sigma) - \frac{1}{2\sigma^2}\sum_{i=1}^{N}(d_i - \mu)^2 \right]\]</span></p>
</div><div class="column medium-font center-align" style="width:15%;">
<p>Set partial derivatives w.r.t. <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> to zero <img data-src="img/blue-thick-arrow.png"></p>
</div><div class="column medium-font" style="width:30%;">
<p><span class="math display">\[\mu_{MLE}=\sum_{i=1}^{N}d_{i}/N\]</span></p>
<p><span class="math display">\[\sigma_{MLE}^{2}=\frac{1}{N}\sum_{i=1}^{N}(d_{i}-\mu_{MLE})^{2}\]</span></p>
</div></div>
</div>
</section>
<section id="least-squares-as-maximum-likelihood" class="slide level2">
<h2><span class="medium-font">Least Squares as Maximum Likelihood</span></h2>
<div class="columns">
<div class="column" style="width:40%;">
<p><img data-src="img/Lecture7_76.png"></p>
</div><div class="column small-font" style="width:60%;">
<p><span class="math display">\[\theta_{MLE} = \underset{\theta}{\operatorname{argmax}} p(\mathbf{d}_{1:N}|\theta)\]</span></p>
<p>Find the parameters of the model that maximize the likelihood function of the data <span class="math display">\[L(\mathbf{\theta}) = p(\mathbf{d}_{1:N}|\mathbf{\theta})\]</span></p>
<p>which is a function of theta, <strong>not</strong> a probability distribution.</p>
</div></div>
<p><img data-src="img/blue-arrow.png" class="absolute" style="left: 240px; bottom: 290px; height: 60px; "></p>
<div class="semi-tiny-font">
<p>Example: assume we know that 1D <span style="color:#002060">data points</span> were generated <u>independently</u> from a Gaussian distribution <span class="math inline">\(\mathcal{N}(\mu, \sigma^2)\)</span> , but we don’t know the mean and variance. The likelihood function of the data is</p>
<p><span class="math display">\[L(\mu, \sigma) = p(\mathbf{d}_{1:N}|\mu, \sigma) = \prod_{i=1}^{N} p(d_i|\mu, \sigma) = \prod_{i=1}^{N} \frac{1}{\sqrt{2\pi\sigma}} \exp(-0.5(d_i - \mu)^2/\sigma^2)\]</span></p>
<p>And the maximum-likelihood parameter estimates are</p>
<div class="quarto-layout-panel" data-layout="[70, -30]">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 70.0%;justify-content: flex-start;">
<p><span class="math display">\[(\mu, \sigma)_{MLE} = \operatorname*{argmax}_{\mu,\sigma} \sum_{i=1}^{N} \log p(d_i|\mu, \sigma) = \operatorname*{argmax}_{\mu,\sigma} \left[ -N\log(\sqrt{2\pi}\sigma) - \frac{1}{2\sigma^2}\sum_{i=1}^{N}(d_i - \mu)^2 \right]\]</span></p>
</div>
<div class="quarto-figure-spacer quarto-layout-cell" style="flex-basis: 30.0%;justify-content: flex-start;">
<p>&nbsp;</p>
</div>
</div>
</div>
</div>
<p><img data-src="img/least-sqaures.png" class="absolute" style="bottom: 25px; right: 20px; width: 35%; "></p>
</section>
<section id="estimating-parameters-of-probability-models-2" class="slide level2">
<h2>Estimating parameters of probability models</h2>
<ul>
<li>In the occupancy grid mapping problem we wanted to compute <span class="math inline">\(p(\mathbf{m}|\mathbf{z}_{1:t}, \mathbf{x}_{1:t})\)</span> over all possible maps.</li>
<li>We can see this problem as a specific instance within a category of problems where we are given data (observations) and we want to “explain” or fit the data using a parametric function.</li>
<li>There are typically three ways to work with this type of problems:
<ol type="1">
<li>Maximum Likelihood parameter estimation (MLE)
<ul>
<li>Least Squares</li>
</ul></li>
<li><span style="color:#ff0000">Maximum A Posteriori (MAP) parameter estimation</span></li>
<li>Bayesian parameter distribution estimation</li>
</ol></li>
</ul>
</section>
<section id="maximum-a-posteriori-parameter-estimation" class="slide level2">
<h2>Maximum A Posteriori Parameter Estimation</h2>
<div class="columns">
<div class="column small-font" style="width:17%;">
<p>Likelihood<br>
<span class="math inline">\(p(d|\theta)\)</span></p>
<p><br><br><br></p>
<p>Prior<br>
<span class="math inline">\(p(\theta)\)</span></p>
<p><br><br><br></p>
<p>Posterior<br>
<span class="math inline">\(p(\theta|d) \propto p(d|\theta)p(\theta)\)</span></p>
</div><div class="column" style="width:40%;">
<p><img data-src="img/Lecture7_118.png"></p>
</div><div class="column medium-font" style="width:43%;">
<p><span class="math display">\[\begin{align}
\theta_{MAP} &amp;= \operatorname*{argmax}_{\theta} p(\theta|\mathbf{d}_{1:N}) \\
&amp;= \operatorname*{argmax}_{\theta} \left[ \frac{p(\mathbf{d}_{1:N}|\theta)p(\theta)}{p(\mathbf{d}_{1:N})} \right] \\
&amp;= \operatorname*{argmax}_{\theta} \left[ p(\mathbf{d}_{1:N}|\theta)p(\theta) \right]
\end{align}\]</span></p>
<div class="quarto-figure quarto-figure-right">
<figure>
<p><img data-src="img/max-posteriori.png" class="quarto-figure quarto-figure-right" width="200"></p>
</figure>
</div>
</div></div>
</section>
<section id="estimating-parameters-of-probability-models-3" class="slide level2">
<h2>Estimating parameters of probability models</h2>
<ul>
<li><p>In the occupancy grid mapping problem we wanted to compute <span class="math inline">\(p(\mathbf{m}|\mathbf{z}_{1:t}, \mathbf{x}_{1:t})\)</span> over all possible maps.</p></li>
<li><p>We can see this problem as a specific instance within a category of problems where we are given data (observations) and we want to “explain” or fit the data using a parametric function.</p></li>
<li><p>There are typically three ways to work with this type of problems:</p>
<ol type="1">
<li>Maximum Likelihood parameter estimation (MLE)
<ul>
<li>Least Squares</li>
</ul></li>
<li>Maximum A Posteriori (MAP) parameter estimation</li>
<li><span class="red">Bayesian parameter distribution estimation</span></li>
</ol></li>
</ul>
</section>
<section id="bayesian-parameter-estimation" class="slide level2">
<h2>Bayesian parameter estimation</h2>
<ul>
<li><p>Both MLE and MAP estimators give you a single <strong>point estimate</strong> .</p></li>
<li><p>But there might be many parameters that are compatible with the data.</p></li>
<li><p>Instead of point estimates, compute a <strong>distribution of estimates</strong> that explain the data</p></li>
</ul>
<p><br></p>
<ul>
<li>Bayesian parameter estimation:</li>
</ul>
<p><span class="math display">\[p(\theta|\mathbf{d}_{1:N}) = \frac{p(\mathbf{d}_{1:N}|\theta)p(\theta)}{p(\mathbf{d}_{1:N})}\]</span></p>
<p><img data-src="img/arrow.png" class="absolute" style="bottom: 170px; right: 230px; width: 150px; "></p>
<p><span class="red-annotation absolute" style="bottom: 40px; right: 0px; ">The probability of the<br>
data is usually hard to<br>
compute. But it does not<br>
depend on the parameter<br>
theta, so it is treated as a<br>
normalizing factor, and we<br>
can still compute how the<br>
posterior varies with theta.</span></p>
</section>
<section id="bayesian-parameter-estimation-1" class="slide level2">
<h2>Bayesian parameter estimation</h2>
<ul>
<li><p>Both MLE and MAP estimators give you a single <strong>point estimate</strong> .</p></li>
<li><p>But there might be many parameters that are compatible with the data.</p></li>
<li><p>Instead of point estimates, compute a <strong>distribution of estimates</strong> that explain the data</p></li>
<li><p>Bayesian parameter estimation:</p></li>
</ul>
<p><span class="math display">\[p(\theta|\mathbf{d}_{1:N}) = \frac{p(\mathbf{d}_{1:N}|\theta)p(\theta)}{p(\mathbf{d}_{1:N})}\]</span></p>
<ul>
<li>This is what we used in occupancy grid mapping, when we approximated</li>
</ul>
<p><span class="math display">\[p(\mathbf{m}|\mathbf{z}_{1:t}, \mathbf{x}_{1:t})\]</span></p>
</section>
<section id="todays-agenda-2" class="slide level2">
<h2>Today’s agenda</h2>
<div class="columns">
<div class="column" style="width:70%;">
<div class="grey">
<ul>
<li><p>Least Squares Estimation</p></li>
<li><p>Maximum Likelihood Estimation (MLE)</p></li>
<li><p>Maximum a Posteriori Estimation (MAP)</p></li>
<li><p>Bayesian Estimation</p></li>
</ul>
</div>
<ul>
<li>GraphSLAM</li>
</ul>
</div><div class="column" style="width:30%;">
<p><img data-src="img/Lecture7_1.png"></p>
</div></div>
</section>
<section id="goal" class="slide level2">
<h2>Goal</h2>
<ul>
<li><p>Enable a robot to simultaneously build a map of its environment and estimate where it is in that map.</p></li>
<li><p>This is called SLAM (Simultaneous Localization And Mapping)</p></li>
<li><p>Today we are going to look at the batch version, i.e.&nbsp;collect all measurements and controls, and later form an estimate of the states and the map.</p></li>
<li><p>We are going to solve SLAM using least squares</p></li>
</ul>
</section>
<section id="examples-of-slam-systems" class="slide level2">
<h2>Examples of SLAM systems</h2>
<div class="columns">
<div class="column">
<p><img data-src="img/Lecture7_128.png"> <img data-src="img/Lecture7_129.png"></p>
</div><div class="column">
<video id="video_shortcode_videojs_video1" width="600" height="400" class="video-js vjs-default-skin " controls="" preload="auto" data-setup="{}" title=""><source src="vid/Ellion-loop-closure.mp4"></video>
<p><span class="small-font">MORESLAM system, McGill, 2016</span></p>
</div></div>
</section>
<section id="section" class="slide level2">
<h2></h2>
<video id="video_shortcode_videojs_video2" width="800" height="500" class="video-js vjs-default-skin " controls="" preload="auto" data-setup="{}" title=""><source src="vid/stereo_imu_orbslam_mcgill_campus.mp4"></video>
<p><span class="small-font">MORESLAM system, McGill, 2016</span></p>
</section>
<section id="examples-of-slam-systems-1" class="slide level2">
<h2>Examples of SLAM systems</h2>
<iframe data-external="1" src="https://www.youtube.com/embed/iD47JWVqTCk" width="800" height="500" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
</section>
<section id="examples-of-slam-systems-2" class="slide level2">
<h2>Examples of SLAM systems</h2>
<iframe data-external="1" src="https://www.youtube.com/embed/08GTGfNneCI?si=acoB-ikrtOruOxcz" width="800" height="500" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
<p><span class="small-font">Source Code: <a href="https://github.com/erik-nelson/blam" class="uri">https://github.com/erik-nelson/blam</a></span></p>
</section>
<section id="examples-of-slam-systems-3" class="slide level2">
<h2>Examples of SLAM systems</h2>
<p>Google Cartographer: 2D and 3D laser SLAM</p>
<iframe data-external="1" src="https://www.youtube.com/embed/DM0dpHLhtX0" width="800" height="450" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
<p><span class="small-font">Code: <a href="https://github.com/googlecartographer/cartographer" class="uri">https://github.com/googlecartographer/cartographer</a></span></p>
</section>
<section id="slam-possible-problem-definitions" class="slide level2">
<h2>SLAM: possible problem definitions</h2>
<ul>
<li>Smoothing/Batch/Full SLAM</li>
</ul>
<p><span class="math display">\[p(\mathbf{x}_{1:T}, \mathbf{m} \mid \mathbf{z}_{0:T}, \mathbf{u}_{0:T-1}, \mathbf{x}_{0})\]</span></p>
<p><img data-src="img/slam-annotations.png" class="absolute" style="left: 200px; bottom: 42%; width: 670px; "></p>
<p><br><br><br></p>
<div class="fragment">
<ul>
<li>Filtering SLAM</li>
</ul>
<p><span class="math display">\[p(\mathbf{x}_t, \mathbf{m}_t \mid \mathbf{z}_{0:t}, \mathbf{u}_{0:t-1}, \mathbf{x}_0)\]</span></p>
</div>
<div class="fragment">
<p><img data-src="img/big-circle.png" class="absolute" style="bottom: 320px; right: 0px; "></p>
</div>
</section>
<section id="background-multivariate-gaussian-distribution" class="slide level2">
<h2><span class="medium-font">Background: Multivariate Gaussian Distribution</span></h2>
<div class="columns">
<div class="column small-font" style="width:45%;">
<p><span class="math inline">\(p(\mathbf{x}) = \frac{1}{(2\pi)^{D/2}\det(\mathbf{\Sigma})^{1/2}}\exp\left(-0.5(\mathbf{x}-\mathbf{\mu})^T\mathbf{\Sigma}^{-1}(\mathbf{x}-\mathbf{\mu})\right)\)</span></p>
<p><br></p>
<div class="fragment" data-fragment-index="1">
<p><span class="math inline">\(p(\mathbf{x}) = \frac{1}{(2\pi)^{D/2}\det(\mathbf{\Sigma})^{1/2}} \exp\left(-0.5\|\mathbf{x} - \mathbf{\mu}\|_{\mathbf{\Sigma}}^2\right)\)</span></p>
<p><br></p>
<p>Shortcut notation: <span class="math inline">\(\|x\|_{\Sigma}^{2}=x^{T}\Sigma^{-1}x\)</span></p>
</div>
</div><div class="column" style="width:55%;">
<p><img data-src="img/Lecture7_138.jpg"></p>
</div></div>
<p><span class="tiny-font absolute" style="left: 0px; bottom: 10px; ">From “Computer Vision: Models, Learning, and Inference” Simon Prince</span></p>
<p><span class="red semi-tiny-font absolute fragment" data-fragment-index="1" style="bottom: 40px; right: 40px; ">Note: The shapes of these covariances are important, you should<br>
know them well. In particular, when are x1 and x2 correlated?</span></p>
</section>
<section id="background-multivariate-gaussian-distribution-1" class="slide level2">
<h2><span class="medium-font">Background: Multivariate Gaussian Distribution</span></h2>
<div class="columns">
<div class="column small-font" style="width:45%;">
<p><span class="math inline">\(p(\mathbf{x}) = \frac{1}{(2\pi)^{D/2}\det(\mathbf{\Sigma})^{1/2}}\exp\left(-0.5(\mathbf{x}-\mathbf{\mu})^T\mathbf{\Sigma}^{-1}(\mathbf{x}-\mathbf{\mu})\right)\)</span></p>
<p><br></p>
<p><span class="math inline">\(p(\mathbf{x}) = \frac{1}{(2\pi)^{D/2}\det(\mathbf{\Sigma})^{1/2}} \exp\left(-0.5\|\mathbf{x} - \mathbf{\mu}\|_{\mathbf{\Sigma}}^2\right)\)</span></p>
<p><br></p>
<p>Shortcut notation: <span class="math inline">\(\|x\|_{\Sigma}^{2}=x^{T}\Sigma^{-1}x\)</span></p>
</div><div class="column" style="width:55%;">
<p><img data-src="img/Lecture7_138.jpg"></p>
</div></div>
<p><span class="tiny-font absolute" style="left: 0px; bottom: 10px; ">From “Computer Vision: Models, Learning, and Inference” Simon Prince</span></p>
<p><span class="red semi-tiny-font absolute" style="bottom: 40px; right: 40px; ">x1 and x2 are correlated when the shape of the ellipse is rotated,<br>
i.e.&nbsp;when there are nonzero off-diagonal terms in the covariance matrix.<br>
In this example, (e) and (f)</span></p>
</section>
<section id="confidence-regions" class="slide level2">
<h2>Confidence regions</h2>
<ul>
<li>To quantify confidence and uncertainty define a confidence region R about a point x (e.g.&nbsp;the mode) such that at a confidence level c ≤ 1</li>
</ul>
<p><span class="math display">\[p(x \in R) = c\]</span></p>
<ul>
<li><p>we can then say (for example) there is a 99% probability that the true value is in R</p></li>
<li><p>e.g.&nbsp;for a univariate normal distribution <span class="math inline">\(N(\mu, \sigma^2)\)</span></p></li>
</ul>
<div class="columns">
<div class="column">
<p><span class="math inline">\(p(|x - \mu| &lt; \sigma) \approx 0.67\)</span></p>
<p><span class="math inline">\(p(|x - \mu| &lt; 2\sigma) \approx 0.95\)</span></p>
<p><span class="math inline">\(p(|x - \mu| &lt; 3\sigma) \approx 0.997\)</span></p>
</div><div class="column">
<p><img data-src="img/graph.png"></p>
</div></div>
</section>
<section id="expectation" class="slide level2">
<h2>Expectation</h2>
<ul>
<li>Expected value of a random variable X:</li>
</ul>
<p><span class="math display">\[\mathbb{E}_{x\sim p(X)}[X]=\int_{x}xp(X=x)dx\]</span></p>
<ul>
<li>E is linear: <span class="math inline">\(\quad \mathbb{E}_{x \sim p(X)}[X + c] = \mathbb{E}_{x \sim p(X)}[X] + c\)</span></li>
</ul>
<p><span class="math inline">\(\qquad\qquad\quad \mathbb{E}_{x \sim p(X)}[AX + b] = A\mathbb{E}_{x \sim p(X)}[X] + b\)</span></p>
<ul>
<li>If X,Y are independent then [Note: inverse does not hold]</li>
</ul>
<p><span class="math display">\[\mathbb{E}_{x,y \sim p(X,Y)}[XY] = \mathbb{E}_{x \sim p(X)}[X] \mathbb{E}_{x \sim p(Y)}[Y]\]</span></p>
</section>
<section id="covariance-matrix" class="slide level2">
<h2>Covariance Matrix</h2>
<ul>
<li>Measures linear dependence between random variables X, Y. Does <strong>not</strong> measure independence.</li>
</ul>
<p><span class="math display">\[\text{Cov}[X, Y] = E[XY] - E[X]E[Y]\]</span></p>
<ul>
<li>Variance of X</li>
</ul>
<p><span class="math display">\[\begin{align}
&amp; \text{Var}[X] = \text{Cov}[X] = \text{Cov}[X, X] = E[X^2] - E[X]^2 \\
&amp; \text{Cov}[AX + b] = A\text{Cov}[X]A^T \\
&amp; \text{Cov}[X + Y] = \text{Cov}[X] + \text{Cov}[Y] - 2\text{Cov}[X, Y]
\end{align}\]</span></p>
</section>
<section id="covariance-matrix-1" class="slide level2">
<h2>Covariance Matrix</h2>
<ul>
<li>Measures linear dependence between random variables X, Y. Does <strong>not</strong> measure independence.</li>
</ul>
<p><span class="math display">\[\text{Cov}[X, Y] = E[XY] - E[X]E[Y]\]</span></p>
<ul>
<li><p>Entry (i,j) of the covariance matrix measures whether changes in variable <span class="math inline">\(X_i\)</span> co-occur with changes in variable <span class="math inline">\(Y_j\)</span></p></li>
<li><p>It does not measure whether one causes the other.</p></li>
</ul>
</section>
<section id="correlation-does-not-imply-causation" class="slide level2">
<h2>Correlation does not imply causation</h2>

<img data-src="img/Lecture7_160.png" class="quarto-figure quarto-figure-center r-stretch"></section>
<section id="correlation-does-not-imply-causation-1" class="slide level2">
<h2>Correlation does not imply causation</h2>

<img data-src="img/Lecture7_161.png" class="quarto-figure quarto-figure-center r-stretch"></section>
<section id="background-multivariate-gaussian-distribution-2" class="slide level2">
<h2><span class="medium-font">Background: Multivariate Gaussian Distribution</span></h2>
<div class="columns">
<div class="column small-font" style="width:45%;">
<p><span class="math inline">\(p(\mathbf{x}) = \frac{1}{(2\pi)^{D/2}\det(\mathbf{\Sigma})^{1/2}}\exp\left(-0.5(\mathbf{x}-\mathbf{\mu})^T\mathbf{\Sigma}^{-1}(\mathbf{x}-\mathbf{\mu})\right)\)</span></p>
<p><br></p>
<p><span class="math inline">\(p(\mathbf{x}) = \frac{1}{(2\pi)^{D/2}\det(\mathbf{\Sigma})^{1/2}} \exp\left(-0.5\|\mathbf{x} - \mathbf{\mu}\|_{\mathbf{\Sigma}}^2\right)\)</span></p>
<p><br></p>
<p>For multivariate Gaussians:</p>
<p><span class="math display">\[\begin{align}
E[\mathbf{x}] = \mu \\
\text{Cov}[\mathbf{x}] = \Sigma
\end{align}\]</span></p>
</div><div class="column" style="width:55%;">
<p><img data-src="img/Lecture7_138.jpg"></p>
</div></div>
<p><span class="tiny-font absolute" style="left: 0px; bottom: 10px; ">From “Computer Vision: Models, Learning, and Inference” Simon Prince</span></p>
</section>
<section id="background-multivariate-gaussian-distribution-3" class="slide level2">
<h2><span class="medium-font">Background: Multivariate Gaussian Distribution</span></h2>
<div class="columns">
<div class="column small-font" style="width:45%;">
<p><span class="math inline">\(p(\mathbf{x}) = \frac{1}{(2\pi)^{D/2}\det(\mathbf{\Sigma})^{1/2}}\exp\left(-0.5(\mathbf{x}-\mathbf{\mu})^T\mathbf{\Sigma}^{-1}(\mathbf{x}-\mathbf{\mu})\right)\)</span></p>
<p><br></p>
<p><span class="math inline">\(p(\mathbf{x}) = \frac{1}{(2\pi)^{D/2}\det(\mathbf{\Sigma})^{1/2}} \exp\left(-0.5\|\mathbf{x} - \mathbf{\mu}\|_{\mathbf{\Sigma}}^2\right)\)</span></p>
<p><br></p>
<p>Since we have 2D examples here:</p>
<p><span class="math display">\[\begin{align}
\text{Cov}[\mathbf{x}] = \boldsymbol{\Sigma} &amp;= \begin{bmatrix}
\text{Cov}[x_1, x_1] &amp; \text{Cov}[x_1, x_2] \\
\text{Cov}[x_2, x_1] &amp; \text{Cov}[x_2, x_2]
\end{bmatrix} \\
&amp;= \begin{bmatrix}
\text{Var}[x_1] &amp; \text{Cov}[x_1, x_2] \\
\text{Cov}[x_2, x_1] &amp; \text{Var}[x_2]
\end{bmatrix}
\end{align}\]</span></p>
</div><div class="column" style="width:55%;">
<p><img data-src="img/Lecture7_138.jpg"></p>
</div></div>
</section>
<section id="slam-graph-representation" class="slide level2">
<h2>SLAM: graph representation</h2>
<div class="columns">
<div class="column" style="width:70%;">
<p><img data-src="img/slam-graph.png"></p>
</div><div class="column" style="width:30%;">
<p>Map <span class="math inline">\(\mathbf{m} = \{\mathbf{m}_0, \mathbf{m}_1\}\)</span> consists of landmarks that are easily identifiable and cannot be mistaken for one another.</p>
<p><span class="red-annotation absolute" style="right: 0px; ">i.e.&nbsp;we are avoiding<br>
the data association<br>
problem here.</span></p>
</div></div>
</section>
<section id="slam-graph-representation-1" class="slide level2">
<h2>SLAM: graph representation</h2>
<div class="columns">
<div class="column" style="width:70%;">
<p><img data-src="img/slam-graph2.png"></p>
</div><div class="column" style="width:30%;">
<p>Map <span class="math inline">\(\mathbf{m} = \{\mathbf{m}_0, \mathbf{m}_1\}\)</span> consists of landmarks that are easily identifiable and cannot be mistaken for one another.</p>
</div></div>
</section>
<section id="slam-graph-representation-2" class="slide level2">
<h2>SLAM: graph representation</h2>
<div class="columns">
<div class="column" style="width:70%;">
<p><img data-src="img/slam-graph2.png"></p>
</div><div class="column" style="width:30%;">
<p>Notice that the graph is mostly sparse as long as not many states observe the same landmark.</p>
<p><br></p>
<p>That implies that there are many symbolic dependencies between random variables in <span class="math inline">\(p(\mathbf{x}_{1:T}, \mathbf{m} \mid \mathbf{z}_{0:T}, \mathbf{u}_{0:T-1}, \mathbf{x}_{0})\)</span> that are not necessary and can be dropped.</p>
</div></div>
</section>
<section id="graphslam-slam-as-a-maximum-a-posteriori-estimate" class="slide level2">
<h2><span class="medium-font">GraphSLAM: SLAM as a Maximum A Posteriori Estimate</span></h2>
<p>Instead of computing the posterior <span class="math inline">\(p(\mathbf{x}_{1:T}, \mathbf{m} \mid \mathbf{z}_{0:T}, \mathbf{u}_{0:T-1}, \mathbf{x}_{0})\)</span> we are going to compute its max</p>
<p><span class="math display">\[\mathbf{x}^*_{1:T}, \mathbf{m}^* = \underset{\mathbf{x}_{1:T},\mathbf{m}}{\operatorname{argmax}} p(\mathbf{x}_{1:T}, \mathbf{m} \mid \mathbf{z}_{0:T}, \mathbf{u}_{0:T-1}, \mathbf{x}_0)\]</span></p>
<p><span class="absolute red-annotation" style="top: 210px; right: 0px; ">See least<br>
squares lecture</span></p>
</section>
<section id="graphslam-slam-as-a-maximum-a-posteriori-estimate-1" class="slide level2">
<h2><span class="medium-font">GraphSLAM: SLAM as a Maximum A Posteriori Estimate</span></h2>
<p>Instead of computing the posterior <span class="math inline">\(p(\mathbf{x}_{1:T}, \mathbf{m} \mid \mathbf{z}_{0:T}, \mathbf{u}_{0:T-1}, \mathbf{x}_{0})\)</span> we are going to compute its max</p>
<p><span class="math display">\[\begin{align}
\mathbf{x}_{1:T}^*, \mathbf{m}^* &amp;= \operatorname*{argmax}_{\mathbf{x}_{1:T}, \mathbf{m}} p(\mathbf{x}_{1:T}, \mathbf{m} | \mathbf{z}_{0:T}, \mathbf{u}_{0:T-1}, \mathbf{x}_0) \\
&amp;= \operatorname*{argmax}_{\mathbf{x}_{1:T}, \mathbf{m}} \left[ \frac{p(\mathbf{x}_{1:T}, \mathbf{m}, \mathbf{z}_{0:T}, \mathbf{u}_{0:T-1}|\mathbf{x}_0)}{p(\mathbf{z}_{0:T}, \mathbf{u}_{0:T-1}|\mathbf{x}_0)} \right]
\end{align}\]</span></p>
<p><span class="red-annotation absolute" style="bottom: 40%; right: 0px; ">by definition<br>
of conditional<br>
distribution</span></p>
</section>
<section id="graphslam-slam-as-a-maximum-a-posteriori-estimate-2" class="slide level2">
<h2><span class="medium-font">GraphSLAM: SLAM as a Maximum A Posteriori Estimate</span></h2>
<p>Instead of computing the posterior <span class="math inline">\(p(\mathbf{x}_{1:T}, \mathbf{m} \mid \mathbf{z}_{0:T}, \mathbf{u}_{0:T-1}, \mathbf{x}_{0})\)</span> we are going to compute its max</p>
<p><span class="math display">\[\begin{align}
\mathbf{x}_{1:T}^*, \mathbf{m}^* &amp;= \operatorname*{argmax}_{\mathbf{x}_{1:T}, \mathbf{m}} p(\mathbf{x}_{1:T}, \mathbf{m} | \mathbf{z}_{0:T}, \mathbf{u}_{0:T-1}, \mathbf{x}_0) \\
&amp;= \operatorname*{argmax}_{\mathbf{x}_{1:T}, \mathbf{m}} \left[ \frac{p(\mathbf{x}_{1:T}, \mathbf{m}, \mathbf{z}_{0:T}, \mathbf{u}_{0:T-1}|\mathbf{x}_0)}{p(\mathbf{z}_{0:T}, \mathbf{u}_{0:T-1}|\mathbf{x}_0)} \right] \\
&amp;= \operatorname*{argmax}_{\mathbf{x}_{1:T}, \mathbf{m}} p(\mathbf{x}_{1:T}, \mathbf{m}, \mathbf{z}_{0:T}, \mathbf{u}_{0:T-1}|\mathbf{x}_0)
\end{align}\]</span></p>
<p><span class="red-annotation absolute" style="bottom: 230px; right: 0px; ">denominator does<br>
not depend on<br>
optimization<br>
variables</span></p>
</section>
<section id="graphslam-slam-as-a-maximum-a-posteriori-estimate-3" class="slide level2">
<h2><span class="medium-font">GraphSLAM: SLAM as a Maximum A Posteriori Estimate</span></h2>
<p>Instead of computing the posterior <span class="math inline">\(p(\mathbf{x}_{1:T}, \mathbf{m} \mid \mathbf{z}_{0:T}, \mathbf{u}_{0:T-1}, \mathbf{x}_{0})\)</span> we are going to compute its max</p>
<p><span class="math display">\[\begin{align}
\mathbf{x}_{1:T}^*, \mathbf{m}^* &amp;= \operatorname*{argmax}_{\mathbf{x}_{1:T}, \mathbf{m}} p(\mathbf{x}_{1:T}, \mathbf{m} | \mathbf{z}_{0:T}, \mathbf{u}_{0:T-1}, \mathbf{x}_0) \\
&amp;= \operatorname*{argmax}_{\mathbf{x}_{1:T}, \mathbf{m}} \left[ \frac{p(\mathbf{x}_{1:T}, \mathbf{m}, \mathbf{z}_{0:T}, \mathbf{u}_{0:T-1}|\mathbf{x}_0)}{p(\mathbf{z}_{0:T}, \mathbf{u}_{0:T-1}|\mathbf{x}_0)} \right] \\
&amp;= \operatorname*{argmax}_{\mathbf{x}_{1:T}, \mathbf{m}} p(\mathbf{x}_{1:T}, \mathbf{m}, \mathbf{z}_{0:T}, \mathbf{u}_{0:T-1}|\mathbf{x}_0) \\
&amp;= \operatorname*{argmax}_{\mathbf{x}_{1:T}, \mathbf{m}} \left[ \prod_{t=1}^{T} p(\mathbf{x}_t|\mathbf{x}_{t-1}, \mathbf{u}_{t-1}) \prod_{t=0}^{T} \prod_{\mathbf{z}_t^{(k)} \in \mathbf{z}_t} p(\mathbf{z}_t^{(k)}|\mathbf{x}_t, \mathbf{m}_k) \right]
\end{align}\]</span></p>
<p><img data-src="img/graphSLAM-annot1.png" class="absolute" style="left: -40px; bottom: 100px; width: 300px; " data-bottom="100"></p>
<p><img data-src="img/graphSLAM-annot2.png" class="absolute" style="bottom: -10px; right: 200px; width: 200px; "></p>
<p><img data-src="img/graphSLAM-annot3.png" class="absolute" style="bottom: 150px; right: 25px; width: 250px; "></p>
</section>
<section id="graphslam-slam-as-a-maximum-a-posteriori-estimate-4" class="slide level2">
<h2><span class="medium-font">GraphSLAM: SLAM as a Maximum A Posteriori Estimate</span></h2>
<p>Instead of computing the posterior <span class="math inline">\(p(\mathbf{x}_{1:T}, \mathbf{m} \mid \mathbf{z}_{0:T}, \mathbf{u}_{0:T-1}, \mathbf{x}_{0})\)</span> we are going to compute its max</p>
<p><span class="math display">\[\begin{align}
\mathbf{x}_{1:T}^*, \mathbf{m}^* &amp;= \operatorname*{argmax}_{\mathbf{x}_{1:T}, \mathbf{m}} p(\mathbf{x}_{1:T}, \mathbf{m} | \mathbf{z}_{0:T}, \mathbf{u}_{0:T-1}, \mathbf{x}_0) \\
&amp;= \operatorname*{argmax}_{\mathbf{x}_{1:T}, \mathbf{m}} \left[ \frac{p(\mathbf{x}_{1:T}, \mathbf{m}, \mathbf{z}_{0:T}, \mathbf{u}_{0:T-1}|\mathbf{x}_0)}{p(\mathbf{z}_{0:T}, \mathbf{u}_{0:T-1}|\mathbf{x}_0)} \right] \\
&amp;= \operatorname*{argmax}_{\mathbf{x}_{1:T}, \mathbf{m}} p(\mathbf{x}_{1:T}, \mathbf{m}, \mathbf{z}_{0:T}, \mathbf{u}_{0:T-1}|\mathbf{x}_0) \\
&amp;= \operatorname*{argmax}_{\mathbf{x}_{1:T}, \mathbf{m}} \left[ \prod_{t=1}^{T} p(\mathbf{x}_t|\mathbf{x}_{t-1}, \mathbf{u}_{t-1}) \prod_{t=0}^{T} \prod_{z_t^{(k)} \in \mathbf{z}_t} p(z_t^{(k)}|\mathbf{x}_t, \mathbf{m}_k) \right]
\end{align}\]</span></p>
<p><img data-src="img/graphSLAM-annotation2.png" class="absolute" style="bottom: -20px; right: 0px; width: 69%; "></p>
</section>
<section id="graphslam-slam-as-a-maximum-a-posteriori-estimate-5" class="slide level2">
<h2><span class="medium-font">GraphSLAM: SLAM as a Maximum A Posteriori Estimate</span></h2>
<div class="medium-font">
<p>Instead of computing the posterior <span class="math inline">\(p(\mathbf{x}_{1:T}, \mathbf{m} \mid \mathbf{z}_{0:T}, \mathbf{u}_{0:T-1}, \mathbf{x}_{0})\)</span> we are going to compute its max</p>
<p><span class="math display">\[\begin{align}
\mathbf{x}_{1:T}^*, \mathbf{m}^* &amp;= \operatorname*{argmax}_{\mathbf{x}_{1:T}, \mathbf{m}} p(\mathbf{x}_{1:T}, \mathbf{m} | \mathbf{z}_{0:T}, \mathbf{u}_{0:T-1}, \mathbf{x}_0) \\
&amp;= \operatorname*{argmax}_{\mathbf{x}_{1:T}, \mathbf{m}} \left[ \frac{p(\mathbf{x}_{1:T}, \mathbf{m}, \mathbf{z}_{0:T}, \mathbf{u}_{0:T-1}|\mathbf{x}_0)}{p(\mathbf{z}_{0:T}, \mathbf{u}_{0:T-1}|\mathbf{x}_0)} \right] \\
&amp;= \operatorname*{argmax}_{\mathbf{x}_{1:T}, \mathbf{m}} p(\mathbf{x}_{1:T}, \mathbf{m}, \mathbf{z}_{0:T}, \mathbf{u}_{0:T-1}|\mathbf{x}_0) \\
&amp;= \operatorname*{argmax}_{\mathbf{x}_{1:T}, \mathbf{m}} \left[ \prod_{t=1}^{T} p(\mathbf{x}_t|\mathbf{x}_{t-1}, \mathbf{u}_{t-1}) \prod_{t=0}^{T} \prod_{\mathbf{z}_t^{(k)} \in \mathbf{z}_t} p(\mathbf{z}_t^{(k)}|\mathbf{x}_t, \mathbf{m}_k) \right] \\
&amp;= \operatorname*{argmax}_{\mathbf{x}_{1:T}, \mathbf{m}} \left[ \sum_{t=1}^{T} \log p(\mathbf{x}_t|\mathbf{x}_{t-1}, \mathbf{u}_{t-1}) + \sum_{t=0}^{T} \sum_{\mathbf{z}_t^{(k)} \in \mathbf{z}_t} \log p(\mathbf{z}_t^{(k)}|\mathbf{x}_t, \mathbf{m}_k) \right]
\end{align}\]</span></p>
</div>
</section>
<section id="graphslam-slam-as-a-maximum-a-posteriori-estimate-6" class="slide level2">
<h2><span class="medium-font">GraphSLAM: SLAM as a Maximum A Posteriori Estimate</span></h2>
<div class="medium-font">
<p>Instead of computing the posterior <span class="math inline">\(p(\mathbf{x}_{1:T}, \mathbf{m} \mid \mathbf{z}_{0:T}, \mathbf{u}_{0:T-1}, \mathbf{x}_{0})\)</span> we are going to compute its max</p>
<p><span class="math display">\[\mathbf{x}_{1:T}^{*},\mathbf{m}^{*} = \underset{\mathbf{x}_{1:T},\mathbf{m}}{\operatorname{argmax}}\left[\sum_{t=1}^{T}\log p(\mathbf{x}_{t}|\mathbf{x}_{t-1},\mathbf{u}_{t-1})+\sum_{t=0}^{T}\sum_{\mathbf{z}_{t}^{(k)}\in \mathbf{z}_{t}}\log p(\mathbf{z}_{t}^{(k)}|\mathbf{x}_{t},\mathbf{m}_{k})\right]\]</span></p>
<div class="columns">
<div class="column" style="width:23%;">
<p><span class="red">Main GraphSLAM assumptions:</span></p>
</div><div class="column" style="width:43%;">
<p><span class="red">1. Uncertainty in the dynamics model is Gaussian</span></p>
<p><span class="math inline">\(\mathbf{x}_{t}=f(\mathbf{x}_{t-1}, \mathbf{u}_{t-1})+\mathbf{w}_{t}\)</span><br>
<span class="math inline">\(\mathbf{w}_{t} \sim \mathcal{N}(0, \mathbf{R}_{t})\)</span><br>
so</p>
<p><span class="medium-font"><span class="math inline">\(\mathbf{x}_t|\mathbf{x}_{t-1}, \mathbf{u}_{t-1} \sim \mathcal{N}(\mathbf{f}(\mathbf{x}_{t-1}, \mathbf{u}_{t-1}), \mathbf{R}_t)\)</span></span></p>
</div><div class="column" style="width:33%;">
<p><span class="red">2. Uncertainty in the sensor model is Gaussian</span></p>
<p><span class="math inline">\(\mathbf{z}_t^{(k)} = \mathbf{h}(\mathbf{x}_t, \mathbf{m}_k) + \mathbf{v}_t\)</span></p>
<p><span class="math inline">\(\mathbf{v}_t \sim \mathcal{N}(\mathbf{0}, \mathbf{Q}_t)\)</span></p>
<p>so</p>
<p><span class="math inline">\(\mathbf{z}_t^{(k)} | \mathbf{x}_t, \mathbf{m}_k \sim \mathcal{N}(\mathbf{h}(\mathbf{x}_t, \mathbf{m}_k), \mathbf{Q}_t)\)</span></p>
</div></div>
</div>
</section>
<section id="slam-noiseerrors" class="slide level2">
<h2>SLAM: noise/errors</h2>
<div class="columns">
<div class="column">
<p><img data-src="img/slam-noise.png"></p>
</div><div class="column">
<p><span class="math inline">\(\mathbf{x}_1|\mathbf{x}_0, \mathbf{u}_0 \sim \mathcal{N}(\mathbf{f}(\mathbf{x}_0, \mathbf{u}_0), \mathbf{R}_0)\)</span></p>
<p>Expected to end up at <span class="math inline">\(\mathbf{x}_1 = \mathbf{f}(\mathbf{x}_0, \mathbf{u}_0)\)</span> from <span class="math inline">\(\mathbf{x}_0\)</span></p>
</div></div>
</section>
<section id="slam-noiseerrors-1" class="slide level2">
<h2>SLAM: noise/errors</h2>
<div class="columns">
<div class="column">
<p><img data-src="img/slam-noise-2.png"></p>
</div><div class="column">
<p><span class="math inline">\(\mathbf{x}_1|\mathbf{x}_0, \mathbf{u}_0 \sim \mathcal{N}(\mathbf{f}(\mathbf{x}_0, \mathbf{u}_0), \mathbf{R}_0)\)</span></p>
<p>Expected to end up at <span class="math inline">\(\mathbf{x}_1 = \mathbf{f}(\mathbf{x}_0, \mathbf{u}_0)\)</span> from <span class="math inline">\(\mathbf{x}_0\)</span> but we might end up around it, within the ellipse defined by the covariance matrix <span class="math inline">\(\mathbf{R}_0\)</span></p>
</div></div>
</section>
<section id="slam-noiseerrors-2" class="slide level2">
<h2>SLAM: noise/errors</h2>

<img data-src="img/slam-noise-3.png" class="r-stretch"><div class="columns">
<div class="column" style="width:70%;">

</div><div class="column medium-font" style="width:30%;">
<p><span class="math inline">\(\mathbf{z}_1^{(0)}|\mathbf{x}_1, \mathbf{m}_0 \sim \mathcal{N}(\mathbf{h}(\mathbf{x}_1, \mathbf{m}_0), \mathbf{Q}_1)\)</span></p>
<p>Expected to get measurement <span class="math inline">\(\mathbf{h}(\mathbf{x}_1, \mathbf{m}_0)\)</span> at state <span class="math inline">\(\mathbf{x}_1\)</span> but it might be somewhere within the ellipse defined by the covariance matrix <span class="math inline">\(\mathbf{Q}_1\)</span></p>
</div></div>
</section>
<section id="graphslam-slam-as-a-least-squares-problem" class="slide level2">
<h2><span class="medium-font">GraphSLAM: SLAM as a least squares problem</span></h2>
<div class="medium-font">
<p>Instead of computing the posterior <span class="math inline">\(p(\mathbf{x}_{1:T}, \mathbf{m} \mid \mathbf{z}_{0:T}, \mathbf{u}_{0:T-1}, \mathbf{x}_{0})\)</span> we are going to compute its max</p>
<p><span class="math display">\[\begin{align}
\mathbf{x}_{1:T}^{*}, \mathbf{m}^{*} &amp;= \underset{\mathbf{x}_{1:T}, \mathbf{m}}{\operatorname{argmax}} \left[ \sum_{t=1}^{T} \log p(\mathbf{x}_{t}|\mathbf{x}_{t-1}, \mathbf{u}_{t-1}) + \sum_{t=0}^{T} \sum_{\mathbf{z}_{t}^{(k)} \in \mathbf{z}_{t}} \log p(\mathbf{z}_{t}^{(k)}|\mathbf{x}_{t}, \mathbf{m}_{k}) \right] \\
&amp;= \operatorname*{argmax}_{\mathbf{x}_{1:T},\mathbf{m}} \left[ - \sum_{t=1}^{T} \|\mathbf{x}_t - \mathbf{f}(\mathbf{x}_{t-1}, \mathbf{u}_{t-1})\|_{\mathbf{{R}}_t}^2 - \sum_{t=0}^{T} \sum_{z_t^{(k)} \in \mathbf{z}_t} \|\mathbf{z}_t^{(k)} - \mathbf{h}(\mathbf{x}_t, \mathbf{m}_k)\|_{\mathbf{{Q}}_t}^2 \right]
\end{align}\]</span></p>
<div class="small-font absolute" style="left: 0px; bottom: 40%; ">
<p><span class="red">Notation:</span><br>
<span class="math inline">\(\mathbf{x}^T \mathbf{Q}^{-1} \mathbf{x} = ||\mathbf{x}||_{\mathbf{Q}}^2\)</span></p>
</div>
<div class="small-font absolute" style="bottom: 0px; right: 0px; ">
<p><span class="math inline">\(\mathbf{x}_t | \mathbf{x}_{t-1}, \mathbf{u}_{t-1} \sim \mathcal{N}(\mathbf{f}(\mathbf{x}_{t-1}, \mathbf{u}_{t-1}), \mathbf{R}_t) \qquad\qquad\qquad \mathbf{z}_{t}^{(k)}|\mathbf{x}_{t}, \mathbf{m}_{k} \sim \mathcal{N}(\mathbf{h}(\mathbf{x}_{t}, \mathbf{m}_{k}), \mathbf{Q}_{t})\)</span></p>
</div>
</div>
</section>
<section id="graphslam-slam-as-a-least-squares-problem-1" class="slide level2">
<h2><span class="medium-font">GraphSLAM: SLAM as a least squares problem</span></h2>
<div class="medium-font">
<p>Instead of computing the posterior <span class="math inline">\(p(\mathbf{x}_{1:T}, \mathbf{m} \mid \mathbf{z}_{0:T}, \mathbf{u}_{0:T-1}, \mathbf{x}_{0})\)</span> we are going to compute its max</p>
<p><span class="math display">\[\begin{align}
\mathbf{x}_{1:T}^{*}, \mathbf{m}^{*} &amp;= \underset{\mathbf{x}_{1:T}, \mathbf{m}}{\operatorname{argmax}} \left[ \sum_{t=1}^{T} \log p(\mathbf{x}_{t}|\mathbf{x}_{t-1}, \mathbf{u}_{t-1}) + \sum_{t=0}^{T} \sum_{\mathbf{z}_{t}^{(k)} \in \mathbf{z}_{t}} \log p(\mathbf{z}_{t}^{(k)}|\mathbf{x}_{t}, \mathbf{m}_{k}) \right] \\
&amp;= \operatorname*{argmax}_{\mathbf{x}_{1:T},\mathbf{m}} \left[ - \sum_{t=1}^{T} \|\mathbf{x}_t - \mathbf{f}(\mathbf{x}_{t-1}, \mathbf{u}_{t-1})\|_{\mathbf{{R}}_t}^2 - \sum_{t=0}^{T} \sum_{z_t^{(k)} \in \mathbf{z}_t} \|\mathbf{z}_t^{(k)} - \mathbf{h}(\mathbf{x}_t, \mathbf{m}_k)\|_{\mathbf{{Q}}_t}^2 \right] \\
&amp;= \underset{\mathbf{x}_{1:T}, \mathbf{m}}{\operatorname{argmin}} \left[ \sum_{t=1}^{T} \|\mathbf{x}_t - \mathbf{f}(\mathbf{x}_{t-1}, \mathbf{u}_{t-1})\|_{\mathbf{R}_t}^2 + \sum_{t=0}^{T} \sum_{\mathbf{z}_t^{(k)} \in \mathbf{z}_t} \|\mathbf{z}_t^{(k)} - \mathbf{h}(\mathbf{x}_t, \mathbf{m}_k)\|_{\mathbf{Q}_t}^2 \right]
\end{align}\]</span></p>
<div class="small-font absolute" style="left: 0px; bottom: 40%; ">
<p><span class="red">Notation:</span><br>
<span class="math inline">\(\mathbf{x}^T \mathbf{Q}^{-1} \mathbf{x} = ||\mathbf{x}||_{\mathbf{Q}}^2\)</span></p>
</div>
<div class="small-font absolute" style="bottom: 0px; right: 0px; ">
<p><span class="math inline">\(\mathbf{x}_t | \mathbf{x}_{t-1}, \mathbf{u}_{t-1} \sim \mathcal{N}(\mathbf{f}(\mathbf{x}_{t-1}, \mathbf{u}_{t-1}), \mathbf{R}_t) \qquad\qquad\qquad \mathbf{z}_{t}^{(k)}|\mathbf{x}_{t}, \mathbf{m}_{k} \sim \mathcal{N}(\mathbf{h}(\mathbf{x}_{t}, \mathbf{m}_{k}), \mathbf{Q}_{t})\)</span></p>
</div>
</div>
</section>
<section id="graphslam-example" class="slide level2">
<h2>GraphSLAM: example</h2>
<div class="columns">
<div class="column" style="width:70%;">
<p><img data-src="img/glaphSLAM-diagram.png"></p>
</div><div class="column medium-font" style="width:30%;">
<p>Need to minimize the sum of the following quadratic terms:</p>
<p><span class="math display">\[\begin{align}
&amp; ||\mathbf{x}_1 - \mathbf{f}(\mathbf{x}_0, \mathbf{u}_0)||_{\mathbf{R}_1}^2 \\
&amp; ||\mathbf{x}_2 - \mathbf{f}(\mathbf{x}_1, \mathbf{u}_1)||_{\mathbf{R}_2}^2 \\
&amp; ||\mathbf{x}_3 - \mathbf{f}(\mathbf{x}_2, \mathbf{u}_2)||_{\mathbf{R}_3}^2 \\
&amp; ||\mathbf{z}_0^{(0)} - \mathbf{h}(\mathbf{x}_0, \mathbf{m}_0)||_{\mathbf{Q}_0}^2 \\
&amp; ||\mathbf{z}_1^{(0)} - \mathbf{h}(\mathbf{x}_1, \mathbf{m}_0)||_{\mathbf{Q}_1}^2 \\
&amp; ||\mathbf{z}_1^{(1)} - \mathbf{h}(\mathbf{x}_1, \mathbf{m}_1)||_{\mathbf{Q}_1}^2 \\
&amp; ||\mathbf{z}_2^{(1)} - \mathbf{h}(\mathbf{x}_2, \mathbf{m}_1)||_{\mathbf{Q}_2}^2
\end{align}\]</span></p>
<p>with respect to variables: <span class="math inline">\(\mathbf{x_1 \quad x_2 \quad x_3 \quad m_0 \quad m_1}\)</span></p>
<p>initial state <span class="math inline">\(\mathbf{x}_0\)</span> is given</p>
</div></div>
<div class="small-font absolute fragment" style="left: 0px; bottom: 100px; ">
<p>Covariance matrices <span class="math inline">\(\mathbf{R_t, Q_t}\)</span><br>
are problem-dependent, but<br>
they usually do not change<br>
with time.</p>
</div>
</section>
<section id="examples-of-dynamics-and-sensor-models" class="slide level2">
<h2>Examples of dynamics and sensor models</h2>
<div class="columns">
<div class="column" style="width:40%;">
<p><span class="math inline">\(\mathbf{x}_t = \mathbf{f}(\mathbf{x}_{t-1}, \mathbf{u}_{t-1}) + \mathbf{w}_t\)</span></p>
<p><span class="math inline">\(\mathbf{z}_{t}^{(k)} = \mathbf{h}(\mathbf{x}_{t}, \mathbf{m}_{k}) + \mathbf{v}_{t}\)</span></p>
</div><div class="column medium-font" style="width:60%;">
<p>Can be any of the dynamical systems we saw in Lecture 2.</p>
<p><span class="math inline">\(\mathbf{z}_t^{(k)}\)</span> can be any of the sensors we saw in Lecture 4:</p>
<ul>
<li><p>Laser scan <span class="math inline">\(\{(r_{i},\theta_{i})\}_{i=1:K}\)</span> where <span class="math inline">\(\mathbf{m}_k\)</span> is an occupancy grid</p></li>
<li><p>Range and bearing <span class="math inline">\((r, \theta)\)</span> to the landmark <span class="math inline">\(\mathbf{m}_k = (x_k, y_k, z_k)\)</span></p></li>
<li><p>Bearing measurements from images</p></li>
<li><p>Altitude/Depth</p></li>
<li><p>Gyroscope</p></li>
<li><p>Accelerometer</p></li>
</ul>
</div></div>
</section>
<section id="appendix-1" class="slide level2">
<h2>Appendix 1</h2>
<div class="small-font">
<p>Claim:</p>
<div class="absolute" style="top: 80px; ">
<p><span class="math display">\[p(\mathbf{x}_{1:T}, \mathbf{m}, \mathbf{z}_{0:T}, \mathbf{u}_{0:T-1}, \mathbf{x}_{0}) = p(\mathbf{x}_{0}) \prod_{t=1}^{T} p(\mathbf{x}_{t}|\mathbf{x}_{t-1}, \mathbf{u}_{t-1}) \prod_{t=0}^{T} \prod_{\mathbf{z}_{t}^{(k)}\in \mathbf{z}_{t}} p(\mathbf{z}_{t}^{(k)}|\mathbf{x}_{t}, \mathbf{m}_{k})\]</span></p>
</div>
<p><br></p>
</div>
<div class="semi-tiny-font">
<p>Proof:</p>
<p><span class="math display">\[\begin{align}
p(\mathbf{x}_{1:T}, \mathbf{m}, \mathbf{z}_{0:T}, \mathbf{u}_{0:T-1}, \mathbf{x}_0) &amp;= p(\mathbf{z}_T | \mathbf{x}_{1:T}, \mathbf{m}, \mathbf{z}_{0:T-1}, \mathbf{u}_{0:T-1}, \mathbf{x}_0) \, p(\mathbf{x}_{1:T}, \mathbf{m}, \mathbf{z}_{0:T-1}, \mathbf{u}_{0:T-1}, \mathbf{x}_0) \\
&amp;= p(\mathbf{z}_T | \mathbf{x}_T, \mathbf{m}) \, p(\mathbf{x}_{1:T}, \mathbf{m}, \mathbf{z}_{0:T-1}, \mathbf{u}_{0:T-1}, \mathbf{x}_0) \\
&amp;= p(\mathbf{z}_T | \mathbf{x}_T, \mathbf{m}) \, p(\mathbf{z}_{T-1} | \mathbf{x}_{1:T}, \mathbf{m}, \mathbf{z}_{0:T-2}, \mathbf{u}_{0:T-1}, \mathbf{x}_0) \, p(\mathbf{x}_{1:T}, \mathbf{m}, \mathbf{z}_{0:T-2}, \mathbf{u}_{0:T-1}, \mathbf{x}_0) \\
&amp;= p(\mathbf{z}_T | \mathbf{x}_T, \mathbf{m}) \, p(\mathbf{z}_{T-1} | \mathbf{x}_{T-1}, \mathbf{m}) \, p(\mathbf{x}_{1:T}, \mathbf{m}, \mathbf{z}_{0:T-2}, \mathbf{u}_{0:T-1}, \mathbf{x}_0) \\
&amp;\quad \dots \\
&amp;= \prod_{t=0}^{T} p(\mathbf{z}_t | \mathbf{x}_t, \mathbf{m}) \, p(\mathbf{x}_{1:T}, \mathbf{m}, \mathbf{u}_{0:T-1}, \mathbf{x}_0) \\
&amp;= \prod_{t=0}^{T} p(\mathbf{z}_t | \mathbf{x}_t, \mathbf{m}) \, p(\mathbf{x}_T | \mathbf{x}_{1:T-1}, \mathbf{m}, \mathbf{u}_{0:T-1}, \mathbf{x}_0) \, p(\mathbf{x}_{1:T-1}, \mathbf{m}, \mathbf{u}_{0:T-1}, \mathbf{x}_0) \\
&amp;= \prod_{t=0}^{T} p(\mathbf{z}_t | \mathbf{x}_t, \mathbf{m}) \, p(\mathbf{x}_T | \mathbf{x}_{T-1}, \mathbf{u}_{T-1}) \, p(\mathbf{x}_{1:T-1}, \mathbf{m}, \mathbf{u}_{0:T-1}, \mathbf{x}_0) \\
&amp;= \prod_{t=0}^{T} p(\mathbf{z}_t | \mathbf{x}_t, \mathbf{m}) \, p(\mathbf{x}_T | \mathbf{x}_{T-1}, \mathbf{u}_{T-1}) \, p(\mathbf{x}_{T-1} | \mathbf{x}_{T-2}, \mathbf{m}, \mathbf{u}_{0:T-1}, \mathbf{x}_0) \, p(\mathbf{x}_{1:T-2}, \mathbf{m}, \mathbf{u}_{0:T-1}, \mathbf{x}_0) \\
&amp;\quad \dots \\
&amp;= \left[ \prod_{t=0}^{T} p(\mathbf{z}_t | \mathbf{x}_t, \mathbf{m}) \right] p(\mathbf{x}_0) \prod_{t=1}^{T} p(\mathbf{x}_t | \mathbf{x}_{t-1}, \mathbf{u}_{t-1})
\end{align}\]</span></p>
</div>
</section>
<section id="appendix-1-1" class="slide level2">
<h2>Appendix 1</h2>
<div class="columns small-font">
<div class="column">
<p>Claim: <span class="math display">\[p(\mathbf{z}_{t}|\mathbf{x}_{t},\mathbf{m}) = \prod_{\mathbf{z}_{t}^{(k)}\in\mathbf{z}_{t}}p(\mathbf{z}_{t}^{(k)}|\mathbf{x}_{t},\mathbf{m}_{k})\]</span></p>
</div><div class="column">
<p>where <span class="math inline">\(\mathbf{z}_t = \left\{ \mathbf{z}_t^{(k)} \text{ iff landmark } \mathbf{m}_k \text{ was observed} \right\}\)</span></p>
<p><span class="math inline">\(\mathbf{m} = \{\text{landmarks } \mathbf{m}_k\}\)</span></p>
</div></div>
<div class="small-font">
<p>Proof:</p>
<p>Suppose without loss of generality that <span class="math inline">\(\mathbf{z}_t = \left\{ \mathbf{z}_t^{(k)}, k = 1...K \right\}\)</span> and <span class="math inline">\(\mathbf{m} = \{\mathbf{m}_{k}, k = 1...K\}\)</span></p>
<p>i.e.&nbsp;that all landmarks were observed from the state at time t. Then:</p>
<p><span class="math display">\[\begin{align}
p(\mathbf{z}_t^{(1)}, \ldots, \mathbf{z}_t^{(K)}|\mathbf{x}_t, \mathbf{m}) &amp;= p(\mathbf{z}_t^{(1)}|\mathbf{z}_t^{(2)}, \ldots, \mathbf{z}_t^{(K)}, \mathbf{x}_t, \mathbf{m}) \, p(\mathbf{z}_t^{(2)}, \ldots, \mathbf{z}_t^{(K)}|\mathbf{x}_t, \mathbf{m}) \\
&amp;= p(\mathbf{z}_t^{(1)}|\mathbf{x}_t, \mathbf{m}_1) \, p(\mathbf{z}_t^{(2)}, \ldots, \mathbf{z}_t^{(K)}|\mathbf{x}_t, \mathbf{m}) \\
&amp;= p(\mathbf{z}_t^{(1)}|\mathbf{x}_t, \mathbf{m}_1) \, p(\mathbf{z}_t^{(2)}|\mathbf{z}_t^{(3)}, \ldots, \mathbf{z}_t^{(K)}, \mathbf{x}_t, \mathbf{m}) \, p(\mathbf{z}_t^{(3)}, \ldots, \mathbf{z}_t^{(K)}|\mathbf{x}_t, \mathbf{m}) \\
&amp;= p(\mathbf{z}_t^{(1)}|\mathbf{x}_t, \mathbf{m}_1) \, p(\mathbf{z}_t^{(2)}|\mathbf{x}_t, \mathbf{m}_2) \, p(\mathbf{z}_t^{(3)}, \ldots, \mathbf{z}_t^{(K)}|\mathbf{x}_t, \mathbf{m}) \\
&amp;\quad \vdots \\
&amp;= \prod_{k=1}^{K} p(\mathbf{z}_t^{(k)}|\mathbf{x}_t, \mathbf{m}_k)
\end{align}\]</span></p>
</div>


</section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<div class="footer footer-default">
<p><a href="https://csc477.github.io/website_fall24" target="_blank" style="font-size:0.8em; bottom: -5px;">↩︎ Back to Course Website</a></p>
</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="../../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"8\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true,"boardmarkerWidth":2,"chalkWidth":2,"chalkEffect":1},
'smaller': true,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
            const codeEl = trigger.previousElementSibling.cloneNode(true);
            for (const childEl of codeEl.children) {
              if (isCodeAnnotation(childEl)) {
                childEl.remove();
              }
            }
            return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp("https:\/\/csc477\.github\.io\/website_fall24");
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    <script>videojs(video_shortcode_videojs_video1);</script>
    <script>videojs(video_shortcode_videojs_video2);</script>
    

</body></html>